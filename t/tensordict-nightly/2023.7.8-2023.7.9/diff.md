# Comparing `tmp/tensordict_nightly-2023.7.8-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict_nightly-2023.7.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,30 @@
-Zip file size: 200053 bytes, number of entries: 28
--rw-rw-rw-  2.0 fat     1206 b- defN 23-Jul-08 13:49 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 23-Jul-08 13:49 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat   114176 b- defN 23-Jul-08 13:51 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    31293 b- defN 23-Jul-08 13:49 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    34686 b- defN 23-Jul-08 13:49 tensordict/persistent.py
--rw-rw-rw-  2.0 fat    33679 b- defN 23-Jul-08 13:49 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat   317133 b- defN 23-Jul-08 13:49 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    39964 b- defN 23-Jul-08 13:49 tensordict/utils.py
--rw-rw-rw-  2.0 fat       86 b- defN 23-Jul-08 13:51 tensordict/version.py
--rw-rw-rw-  2.0 fat     1366 b- defN 23-Jul-08 13:49 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    51384 b- defN 23-Jul-08 13:49 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat    25098 b- defN 23-Jul-08 13:49 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    23469 b- defN 23-Jul-08 13:49 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19940 b- defN 23-Jul-08 13:49 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    10896 b- defN 23-Jul-08 13:49 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      512 b- defN 23-Jul-08 13:49 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     7272 b- defN 23-Jul-08 13:49 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 23-Jul-08 13:49 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 23-Jul-08 13:49 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 23-Jul-08 13:49 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 23-Jul-08 13:49 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7875 b- defN 23-Jul-08 13:49 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      764 b- defN 23-Jul-08 13:49 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 23-Jul-08 13:51 tensordict_nightly-2023.7.8.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    15788 b- defN 23-Jul-08 13:51 tensordict_nightly-2023.7.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Jul-08 13:51 tensordict_nightly-2023.7.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 23-Jul-08 13:51 tensordict_nightly-2023.7.8.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2438 b- defN 23-Jul-08 13:51 tensordict_nightly-2023.7.8.dist-info/RECORD
-28 files, 757431 bytes uncompressed, 196123 bytes compressed:  74.1%
+Zip file size: 200368 bytes, number of entries: 28
+-rw-rw-rw-  2.0 fat     1206 b- defN 23-Jul-09 13:49 tensordict/__init__.py
+-rw-rw-rw-  2.0 fat     6156 b- defN 23-Jul-09 13:49 tensordict/_contextlib.py
+-rw-rw-rw-  2.0 fat   114176 b- defN 23-Jul-09 13:51 tensordict/_tensordict.pyd
+-rw-rw-rw-  2.0 fat    31293 b- defN 23-Jul-09 13:49 tensordict/memmap.py
+-rw-rw-rw-  2.0 fat    34802 b- defN 23-Jul-09 13:49 tensordict/persistent.py
+-rw-rw-rw-  2.0 fat    33679 b- defN 23-Jul-09 13:49 tensordict/tensorclass.py
+-rw-rw-rw-  2.0 fat   317569 b- defN 23-Jul-09 13:49 tensordict/tensordict.py
+-rw-rw-rw-  2.0 fat    39964 b- defN 23-Jul-09 13:49 tensordict/utils.py
+-rw-rw-rw-  2.0 fat       86 b- defN 23-Jul-09 13:51 tensordict/version.py
+-rw-rw-rw-  2.0 fat     1366 b- defN 23-Jul-09 13:49 tensordict/nn/__init__.py
+-rw-rw-rw-  2.0 fat    51384 b- defN 23-Jul-09 13:49 tensordict/nn/common.py
+-rw-rw-rw-  2.0 fat    25098 b- defN 23-Jul-09 13:49 tensordict/nn/functional_modules.py
+-rw-rw-rw-  2.0 fat    23469 b- defN 23-Jul-09 13:49 tensordict/nn/probabilistic.py
+-rw-rw-rw-  2.0 fat    19940 b- defN 23-Jul-09 13:49 tensordict/nn/sequence.py
+-rw-rw-rw-  2.0 fat    10896 b- defN 23-Jul-09 13:49 tensordict/nn/utils.py
+-rw-rw-rw-  2.0 fat      512 b- defN 23-Jul-09 13:49 tensordict/nn/distributions/__init__.py
+-rw-rw-rw-  2.0 fat     7272 b- defN 23-Jul-09 13:49 tensordict/nn/distributions/continuous.py
+-rw-rw-rw-  2.0 fat     2667 b- defN 23-Jul-09 13:49 tensordict/nn/distributions/discrete.py
+-rw-rw-rw-  2.0 fat     6694 b- defN 23-Jul-09 13:49 tensordict/nn/distributions/truncated_normal.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 23-Jul-09 13:49 tensordict/nn/distributions/utils.py
+-rw-rw-rw-  2.0 fat      393 b- defN 23-Jul-09 13:49 tensordict/prototype/__init__.py
+-rw-rw-rw-  2.0 fat     7875 b- defN 23-Jul-09 13:49 tensordict/prototype/fx.py
+-rw-rw-rw-  2.0 fat      764 b- defN 23-Jul-09 13:49 tensordict/prototype/tensorclass.py
+-rw-rw-rw-  2.0 fat     1119 b- defN 23-Jul-09 13:51 tensordict_nightly-2023.7.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    15788 b- defN 23-Jul-09 13:51 tensordict_nightly-2023.7.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jul-09 13:51 tensordict_nightly-2023.7.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 23-Jul-09 13:51 tensordict_nightly-2023.7.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2438 b- defN 23-Jul-09 13:51 tensordict_nightly-2023.7.9.dist-info/RECORD
+28 files, 757983 bytes uncompressed, 196438 bytes compressed:  74.1%
```

## zipnote {}

```diff
@@ -63,23 +63,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2023.7.8.dist-info/LICENSE
+Filename: tensordict_nightly-2023.7.9.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2023.7.8.dist-info/METADATA
+Filename: tensordict_nightly-2023.7.9.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2023.7.8.dist-info/WHEEL
+Filename: tensordict_nightly-2023.7.9.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2023.7.8.dist-info/top_level.txt
+Filename: tensordict_nightly-2023.7.9.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2023.7.8.dist-info/RECORD
+Filename: tensordict_nightly-2023.7.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/persistent.py

```diff
@@ -439,14 +439,18 @@
         _batch_size = self._batch_size
         try:
             self._batch_size = torch.Size(value)
             self._check_batch_size(self._batch_size)
         except ValueError:
             self._batch_size = _batch_size
 
+    _erase_names = TensorDict._erase_names
+    names = TensorDict.names
+    _has_names = TensorDict._has_names
+
     def _rename_subtds(self, names):
         if names is None:
             names = [None] * self.ndim
         for item in self._nested_tensordicts.values():
             if is_tensor_collection(item):
                 td_names = list(names) + [None] * (item.ndim - self.ndim)
                 item.rename_(*td_names)
```

## tensordict/tensordict.py

```diff
@@ -412,63 +412,36 @@
         """
         raise NotImplementedError
 
     def _erase_cache(self):
         self._cache = None
 
     @property
+    @abc.abstractmethod
     def names(self):
-        names = self._td_dim_names
-        if names is None:
-            return [None for _ in range(self.batch_dims)]
-        return names
+        raise NotImplementedError
 
+    @abc.abstractmethod
     def _erase_names(self):
-        self._td_dim_names = None
-
-    @names.setter
-    def names(self, value):
-        # we don't run checks on types for efficiency purposes
-        if value is None:
-            self._erase_names()
-            return
-        num_none = sum(v is None for v in value)
-        if num_none:
-            num_none -= 1
-        if len(set(value)) != len(value) - num_none:
-            raise ValueError(f"Some dimension names are non-unique: {value}.")
-        if len(value) != self.batch_dims:
-            raise ValueError(
-                "the length of the dimension names must equate the tensordict batch_dims attribute. "
-                f"Got {value} for batch_dims {self.batch_dims}."
-            )
-        self._rename_subtds(value)
-        self._td_dim_names = list(value)
+        raise NotImplementedError
 
     @abc.abstractmethod
     def _rename_subtds(self, value):
         # renames all the sub-tensordicts dimension according to value.
         # If value has less dimensions than the TD, the rest is just assumed to be None
         raise NotImplementedError
 
-    @property
-    def _last_op_queue(self):
-        last_op_queue = self.__last_op_queue
-        if last_op_queue is None:
-            last_op_queue = self.__last_op_queue = collections.deque()
-        return last_op_queue
-
     def _check_dim_name(self, name):
         if name is None:
             return False
-        if self._td_dim_names is not None and name in self._td_dim_names:
+        if self._has_names() and name in self.names:
             return True
         for key in self.keys():
             if _is_tensor_collection(self.entry_class(key)):
-                if self.get(key)._check_dim_name(name):
+                if self._get_str(key, NO_DEFAULT)._check_dim_name(name):
                     return True
         else:
             return False
 
     def refine_names(self, *names):
         """Refines the dimension names of self according to names.
 
@@ -528,18 +501,20 @@
             )
         elif not rename_map and not names:
             raise ValueError(
                 "Neither a name map nor a name list was passed. "
                 "Only one is accepted."
             )
         elif rename_map:
-            for i, name in enumerate(clone.names):
+            cnames = list(clone.names)
+            for i, name in enumerate(cnames):
                 new_name = rename_map.pop(name, NO_DEFAULT)
                 if new_name is not NO_DEFAULT:
-                    clone._td_dim_names[i] = new_name
+                    cnames[i] = new_name
+            clone.names = cnames
             if rename_map:
                 raise ValueError(
                     f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
                 )
         else:
             clone.names = names
         return clone
@@ -553,28 +528,39 @@
             )
         elif not rename_map and not names and self.batch_dims:
             raise ValueError(
                 "Neither a name map nor a name list was passed. "
                 "Only one is accepted."
             )
         elif rename_map:
-            _td_dim_names = copy(self.names)
-            for i, name in enumerate(_td_dim_names):
+            cnames = list(self.names)
+            for i, name in enumerate(cnames):
                 new_name = rename_map.pop(name, NO_DEFAULT)
                 if new_name is not NO_DEFAULT:
-                    _td_dim_names[i] = new_name
+                    cnames[i] = new_name
             if rename_map:
                 raise ValueError(
                     f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
                 )
-            self.names = _td_dim_names
+            self.names = cnames
         else:
             self.names = names
         return self
 
+    @abc.abstractmethod
+    def _has_names(self):
+        raise NotImplementedError
+
+    @property
+    def _last_op_queue(self):
+        last_op_queue = self.__last_op_queue
+        if last_op_queue is None:
+            last_op_queue = self.__last_op_queue = collections.deque()
+        return last_op_queue
+
     def size(self, dim: int | None = None) -> torch.Size | int:
         """Returns the size of the dimension indicated by :obj:`dim`.
 
         If dim is not specified, returns the batch_size (or shape) of the TensorDict.
 
         """
         if dim is None:
@@ -602,21 +588,20 @@
                 tensordict = self.get(key)
                 if len(tensordict.batch_size) < len(new_batch_size):
                     # document as edge case
                     tensordict.batch_size = new_batch_size
                     self._set_str(key, tensordict, inplace=True, validated=True)
         self._check_new_batch_size(new_batch_size)
         self._change_batch_size(new_batch_size)
-        if self._td_dim_names is not None:
-            if len(self._td_dim_names) < len(new_batch_size):
-                self.names = self._td_dim_names + [None] * (
-                    len(new_batch_size) - len(self._td_dim_names)
-                )
+        if self._has_names():
+            names = self.names
+            if len(names) < len(new_batch_size):
+                self.names = names + [None] * (len(new_batch_size) - len(names))
             else:
-                self.names = self._td_dim_names[: self.batch_dims]
+                self.names = names[: self.batch_dims]
 
     @property
     def batch_dims(self) -> int:
         """Length of the tensordict batch size.
 
         Returns:
             int describing the number of dimensions of the tensordict.
@@ -1451,15 +1436,15 @@
                 **constructor_kwargs,
             )
         else:
             out = TensorDict(
                 {},
                 batch_size=self.batch_size,
                 device=self.device if not device else device,
-                names=self._td_dim_names,
+                names=self.names if self._has_names() else None,
                 _run_checks=False,
                 **constructor_kwargs,
             )
 
         is_locked = out.is_locked
         if not inplace and is_locked:
             out.unlock_()
@@ -1725,14 +1710,15 @@
     ) -> CompatibleType | dict[str, CompatibleType]:
         cls = value.__class__
         is_tc = _is_tensor_collection(cls)
         if is_tc or issubclass(cls, _ACCEPTED_CLASSES):
             pass
         elif issubclass(cls, dict):
             value = self._convert_to_tensordict(value)
+            is_tc = True
         else:
             try:
                 value = self._convert_to_tensor(value)
             except ValueError as err:
                 raise ValueError(
                     f"TensorDict conversion only supports tensorclasses, tensordicts,"
                     f" numeric scalars and tensors. Got {type(value)}"
@@ -1748,28 +1734,20 @@
                     f"batch dimension mismatch, got self.batch_size"
                     f"={self.batch_size} and value.shape[:self.batch_dims]"
                     f"={_shape(value)[: self.batch_dims]} with value {value}"
                 )
         device = self.device
         if device is not None and value.device != device:
             value = value.to(device, non_blocking=True)
-        if (
-            self._td_dim_names is not None
-            and is_tc
-            and check_shape
-            and value.names[: self.ndim] != self.names
-        ):
-            value = value.clone(False).refine_names(*self.names)
-        elif (
-            self._td_dim_names is None
-            and check_shape
-            and is_tc
-            and value._td_dim_names is not None
-        ):
-            self.names = value.names[: self.batch_dims]
+        if is_tc and check_shape:
+            has_names = self._has_names()
+            if has_names and value.names[: self.ndim] != self.names:
+                value = value.clone(False).refine_names(*self.names)
+            elif not has_names and value._has_names():
+                self.names = value.names[: self.batch_dims]
 
         return value
 
     @abc.abstractmethod
     def pin_memory(self) -> TensorDictBase:
         """Calls :obj:`pin_memory` on the stored tensors."""
         raise NotImplementedError(f"{self.__class__.__name__}")
@@ -1964,18 +1942,18 @@
                 list(self.batch_size)[:start_dim]
                 + [nelt]
                 + list(self.batch_size[end_dim + 1 :])
             )
         else:
             batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
         out = self.apply(flatten, batch_size=batch_size)
-        if self._td_dim_names is not None:
+        if self._has_names():
             names = [
                 name
-                for i, name in enumerate(self._td_dim_names)
+                for i, name in enumerate(self.names)
                 if (i < start_dim or i > end_dim)
             ]
             names.insert(start_dim, None)
             out.names = names
         return out
 
     def unflatten(self, dim, unflattened_size):
@@ -2010,16 +1988,16 @@
                 list(self.batch_size)[:dim]
                 + list(unflattened_size)
                 + list(self.batch_size[dim + 1 :])
             )
         else:
             batch_size = list(unflattened_size) + list(self.batch_size[1:])
         out = self.apply(unflatten, batch_size=batch_size)
-        if self._td_dim_names is not None:
-            names = copy(self._td_dim_names)
+        if self._has_names():
+            names = copy(self.names)
             for _ in range(len(unflattened_size) - 1):
                 names.insert(dim, None)
             out.names = names
         return out
 
     def __enter__(self):
         self._last_op_queue.append(self._last_op)
@@ -2254,15 +2232,18 @@
             )
         if not self.keys():
             raise Exception(
                 "memmap_like() must be called when the TensorDict is (partially) "
                 "populated. Set a tensor first."
             )
         tensordict = TensorDict(
-            {}, self.batch_size, device=self.device, names=self._td_dim_names
+            {},
+            self.batch_size,
+            device=self.device,
+            names=self.names if self._has_names() else None,
         )
         for key, value in self.items():
             if _is_tensor_collection(value.__class__):
                 if prefix is not None:
                     # ensure subdirectory exists
                     os.makedirs(prefix / key, exist_ok=True)
                     tensordict[key] = value.memmap_like(
@@ -2362,16 +2343,16 @@
         from .persistent import PersistentTensorDict
 
         out = PersistentTensorDict.from_dict(
             self,
             filename=filename,
             **kwargs,
         )
-        if self._td_dim_names is not None:
-            out.names = self._td_dim_names
+        if self._has_names():
+            out.names = self.names
         return out
 
     def to_tensordict(self):
         """Returns a regular TensorDict instance from the TensorDictBase.
 
         Returns:
             a new TensorDict object containing the same values.
@@ -2382,15 +2363,15 @@
                 key: value.clone()
                 if not _is_tensor_collection(value.__class__)
                 else value.to_tensordict()
                 for key, value in self.items()
             },
             device=self.device,
             batch_size=self.batch_size,
-            names=self._td_dim_names,
+            names=self.names if self._has_names() else None,
         )
 
     def zero_(self) -> TensorDictBase:
         """Zeros all tensors in the tensordict in-place."""
         for key in self.keys():
             self.fill_(key, 0)
         return self
@@ -2400,17 +2381,17 @@
 
         Resulting tensordicts will share the storage of the initial tensordict.
 
         """
         if dim < 0:
             dim = self.batch_dims + dim
         batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
-        names = self._td_dim_names
-        if names is not None:
-            names = copy(names)
+        names = None
+        if self._has_names():
+            names = copy(self.names)
             names = [name for i, name in enumerate(names) if i != dim]
         out = []
         unbind_self_dict = {key: tensor.unbind(dim) for key, tensor in self.items()}
         for _idx in range(self.batch_size[dim]):
             td = TensorDict(
                 {key: tensor[_idx] for key, tensor in unbind_self_dict.items()},
                 batch_size=batch_size,
@@ -2820,17 +2801,17 @@
                 "split(): argument 'split_size' must be int or list of ints"
             )
         dictionaries = [{} for _ in range(len(batch_sizes))]
         for key, item in self.items():
             split_tensors = torch.split(item, split_size, dim)
             for idx, split_tensor in enumerate(split_tensors):
                 dictionaries[idx][key] = split_tensor
-        names = self._td_dim_names
-        if names is not None:
-            names = copy(names)
+        names = None
+        if self._has_names():
+            names = copy(self.names)
         return [
             TensorDict(
                 dictionaries[i],
                 batch_sizes[i],
                 device=self.device,
                 names=names,
                 _run_checks=False,
@@ -3063,17 +3044,17 @@
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
 
-            names = self._td_dim_names
-            if names is not None:
-                names = copy(names)
+            names = None
+            if self._has_names():
+                names = copy(self.names)
                 names = [name for i, name in enumerate(names) if i != dim]
 
             return TensorDict(
                 source={key: value.all(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
                 device=self.device,
                 names=names,
@@ -3096,17 +3077,17 @@
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
 
-            names = self._td_dim_names
-            if names is not None:
-                names = copy(names)
+            names = None
+            if self._has_names():
+                names = copy(self.names)
                 names = [name for i, name in enumerate(names) if i != dim]
 
             return TensorDict(
                 source={key: value.any(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
                 device=self.device,
                 names=names,
@@ -3217,15 +3198,20 @@
             if key in self.keys():
                 tensordict.update(self[key])
             for old_key, new_key in list_of_keys:
                 value = self.get(old_key)
                 tensordict[new_key] = value
                 if inplace:
                     del self[old_key]
-            out.set(key, tensordict.unflatten_keys(separator=separator))
+            out._set_str(
+                key,
+                tensordict.unflatten_keys(separator=separator),
+                inplace=False,
+                validated=True,
+            )
         return out
 
     def __len__(self) -> int:
         """Returns the length of first dimension, if there is, otherwise 0."""
         return self.shape[0] if self.batch_dims else 0
 
     def __contains__(self, key: NestedKey) -> bool:
@@ -3238,36 +3224,37 @@
         raise NotImplementedError(
             "TensorDict does not support membership checks with the `in` keyword. If "
             "you want to check if a particular key is in your TensorDict, please use "
             "`key in tensordict.keys()` instead."
         )
 
     def _get_names_idx(self, idx):
-        if self._td_dim_names is None:
+        if not self._has_names():
             names = None
         else:
 
             def is_boolean(idx):
                 if isinstance(idx, tuple) and len(idx) == 1:
                     return is_boolean(idx[0])
                 if hasattr(idx, "dtype") and idx.dtype is torch.bool:
                     return idx.ndim
                 return None
 
             num_boolean_dim = is_boolean(idx)
+            names = self.names
             if num_boolean_dim:
-                names = [None] + self._td_dim_names[num_boolean_dim:]
+                names = [None] + names[num_boolean_dim:]
             else:
 
-                def is_int(subidx):
-                    if isinstance(subidx, Number):
-                        return True
-                    if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
-                        return True
-                    return False
+                # def is_int(subidx):
+                #     if isinstance(subidx, Number):
+                #         return True
+                #     if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
+                #         return True
+                #     return False
 
                 if not isinstance(idx, tuple):
                     idx = (idx,)
                 if len(idx) < self.ndim:
                     idx = (*idx, Ellipsis)
                 idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
                 # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
@@ -3277,18 +3264,15 @@
                     if _idx is None:
                         idx_to_take.append(None)
                     elif _is_number(_idx):
                         count += 1
                     else:
                         idx_to_take.append(count)
                         count += 1
-                names = [
-                    self._td_dim_names[i] if i is not None else None
-                    for i in idx_to_take
-                ]
+                names = [names[i] if i is not None else None for i in idx_to_take]
         return names
 
     def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
         names = self._get_names_idx(idx)
         return TensorDict(
             source={key: _get_item(item, idx) for key, item in self.items()},
             batch_size=_getitem_batch_size(self.batch_size, idx),
@@ -3811,15 +3795,14 @@
             self._tensordict = _StringOnlyDict()
             if not isinstance(source, (TensorDictBase, dict)):
                 raise ValueError(
                     "A TensorDict source is expected to be a TensorDictBase "
                     f"sub-type or a dictionary, found type(source)={type(source)}."
                 )
             self._batch_size = self._parse_batch_size(source, batch_size)
-
             self.names = names
 
             if source is not None:
                 for key, value in source.items():
                     self.set(key, value)
 
     @classmethod
@@ -3933,14 +3916,46 @@
     @batch_dims.setter
     def batch_dims(self, value: int) -> None:
         raise RuntimeError(
             f"Setting batch dims on {self.__class__.__name__} instances is "
             f"not allowed."
         )
 
+    def _has_names(self):
+        return self._td_dim_names is not None
+
+    def _erase_names(self):
+        self._td_dim_names = None
+
+    @property
+    def names(self):
+        names = self._td_dim_names
+        if names is None:
+            return [None for _ in range(self.batch_dims)]
+        return names
+
+    @names.setter
+    def names(self, value):
+        # we don't run checks on types for efficiency purposes
+        if value is None:
+            self._erase_names()
+            return
+        num_none = sum(v is None for v in value)
+        if num_none:
+            num_none -= 1
+        if len(set(value)) != len(value) - num_none:
+            raise ValueError(f"Some dimension names are non-unique: {value}.")
+        if len(value) != self.batch_dims:
+            raise ValueError(
+                "the length of the dimension names must equate the tensordict batch_dims attribute. "
+                f"Got {value} for batch_dims {self.batch_dims}."
+            )
+        self._rename_subtds(value)
+        self._td_dim_names = list(value)
+
     def _rename_subtds(self, names):
         if names is None:
             for item in self._tensordict.values():
                 if _is_tensor_collection(item.__class__):
                     item._erase_names()
             return
         for item in self._tensordict.values():
@@ -4490,14 +4505,15 @@
                         *val, strict=strict, inplace=inplace
                     )
 
         out = TensorDict(
             device=self.device,
             batch_size=self.batch_size,
             source=source,
+            # names=self.names if self._has_names() else None,
             names=self._td_dim_names,
             _run_checks=False,
             _is_memmap=self._is_memmap,
             _is_shared=self._is_shared,
         )
         if inplace:
             self._tensordict = out._tensordict
@@ -4737,15 +4753,15 @@
         target_shape[dim] = index_expand.shape[dim]
         index_expand = index_expand.expand(target_shape)
         out = torch.gather(tensor, dim, index_expand, out=dest)
         return out
 
     if out is None:
 
-        names = input._td_dim_names
+        names = input.names if input._has_names() else None
 
         return TensorDict(
             {key: _gather_tensor(value) for key, value in input.items()},
             batch_size=index.shape,
             names=names,
         )
     TensorDict(
@@ -4866,24 +4882,28 @@
     keys = _check_keys(list_of_tensordicts, strict=True)
     if out is None:
         out = {}
         for key in keys:
             with _ErrorInteceptor(
                 key, "Attempted to concatenate tensors on different devices at key"
             ):
-                out[key] = torch.cat([td.get(key) for td in list_of_tensordicts], dim)
+                out[key] = torch.cat(
+                    [td._get_str(key, NO_DEFAULT) for td in list_of_tensordicts], dim
+                )
         if device is None:
             device = list_of_tensordicts[0].device
             for td in list_of_tensordicts[1:]:
                 if device == td.device:
                     continue
                 else:
                     device = None
                     break
-        names = list_of_tensordicts[0]._td_dim_names
+        names = None
+        if list_of_tensordicts[0]._has_names():
+            names = list_of_tensordicts[0].names
         return TensorDict(
             out, device=device, batch_size=batch_size, _run_checks=False, names=names
         )
     else:
         if out.batch_size != batch_size:
             raise RuntimeError(
                 "out.batch_size and cat batch size must match, "
@@ -5304,24 +5324,28 @@
     @property
     def names(self):
         names = self._source._get_names_idx(self.idx)
         if names is None:
             return [None] * self.batch_dims
         return names
 
-    @property
-    def _td_dim_names(self):
-        return self.names
-
     @names.setter
     def names(self, value):
         raise RuntimeError(
             "Names of a subtensordict cannot be modified. Instantiate the tensordict first."
         )
 
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            "Cannot erase names of a SubTensorDict. Erase source TensorDict's names instead."
+        )
+
     def _rename_subtds(self, names):
         for key in self.keys():
             if _is_tensor_collection(self.entry_class(key)):
                 raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
 
     @property
     def device(self) -> None | torch.device:
@@ -5469,16 +5493,16 @@
     def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             out = dest(
                 source=self.clone(),
             )
-            if self._td_dim_names is not None:
-                out.names = self._td_dim_names
+            if self._has_names():
+                out.names = self.names
             return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             # try:
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
@@ -5844,15 +5868,15 @@
         torch.Size([3, 10, 4])
         >>> print(td_stack[:, 0] is tds[0])
         True
 
     """
 
     def __new__(cls, *args: Any, **kwargs: Any) -> LazyStackedTensorDict:
-        cls._td_dim_names = None
+        cls._td_dim_name = None
         return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
 
     def __init__(
         self,
         *tensordicts: TensorDictBase,
         stack_dim: int = 0,
         hook_out: callable | None = None,
@@ -5927,46 +5951,58 @@
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         return self._batch_size_setter(new_size)
 
     @property
+    @cache  # noqa
     def names(self):
-        if self._td_dim_names is None:
-            names = copy(self.tensordicts[0].names)
-            names.insert(self.stack_dim, None)
-            self._td_dim_names = names
-        return self._td_dim_names
+        names = list(self.tensordicts[0].names)
+        for td in self.tensordicts[1:]:
+            if names != td.names:
+                raise ValueError(
+                    f"Not all dim names match, got {names} and {td.names}."
+                )
+        names.insert(self.stack_dim, self._td_dim_name)
+        return names
 
     @names.setter
     @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
     def names(self, value):
         if value is None:
             for td in self.tensordicts:
                 td.names = None
-            self._td_dim_names = None
+            self._td_dim_name = None
         else:
             names_c = list(value)
-            self._td_dim_names = copy(names_c)
             name = names_c[self.stack_dim]
-            names_c = list(names_c)
+            self._td_dim_name = name
             del names_c[self.stack_dim]
             for td in self.tensordicts:
                 if td._check_dim_name(name):
+                    # TODO: should reset names here
                     raise ValueError(f"The dimension name {name} is already taken.")
                 td.rename_(*names_c)
 
     def _rename_subtds(self, names):
         # remove the name of the stack dim
         names = list(names)
         del names[self.stack_dim]
         for td in self.tensordicts:
             td.names = names
 
+    def _has_names(self):
+        return all(td._has_names() for td in self.tensordicts)
+
+    def _erase_names(self):
+        self._td_dim_name = None
+        for td in self.tensordicts:
+            td._erase_names()
+
     def get_item_shape(self, key):
         """Gets the shape of an item in the lazy stack.
 
         Heterogeneous dimensions are returned as -1.
 
         This implementation is inefficient as it will attempt to stack the items
         to compute their shape, and should only be used for printing.
@@ -6204,23 +6240,33 @@
     def _get_str(
         self,
         key: NestedKey,
         default: str | CompatibleType = NO_DEFAULT,
     ) -> CompatibleType:
 
         # we can handle the case where the key is a tuple of length 1
-        if key not in self.keys():
-            return self._default_get(key, default)
-        tensors = [td.get(key, default=None) for td in self.tensordicts]
+        tensors = []
+        for td in self.tensordicts:
+            tensors.append(td._get_str(key, default=default))
+            if (
+                tensors[-1] is default
+                and not isinstance(
+                    default, (MemmapTensor, KeyedJaggedTensor, torch.Tensor)
+                )
+                and not is_tensor_collection(default)
+            ):
+                # then we consider this default as non-stackable and return prematurly
+                return default
         try:
             out = torch.stack(tensors, self.stack_dim)
             if _is_tensor_collection(out.__class__):
-                if self._td_dim_names is not None:
-                    out.refine_names(*self.names, *out.names[self.ndim :])
-                if isinstance(out, TensorDictBase):
+                if self._td_dim_name is not None:
+                    out._td_dim_name = self._td_dim_name
+                if isinstance(out, LazyStackedTensorDict):
+                    # then it's a LazyStackedTD
                     out.hook_out = self.hook_out
                     out.hook_in = self.hook_in
                 else:
                     # then it's a tensorclass
                     out._tensordict.hook_out = self.hook_out
                     out._tensordict.hook_in = self.hook_in
             elif self.hook_out is not None:
@@ -6307,20 +6353,14 @@
             return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
 
         out.hook_out = hook_out
         out.hook_in = hook_in
         out._batch_size = torch.Size(
             [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
         )
-        if out._td_dim_names is not None:
-            out._td_dim_names = [
-                name for i, name in enumerate(out._td_dim_names) if i != out.stack_dim
-            ]
-        else:
-            out._td_dim_names = [None] * out.ndim
         return out
 
     @cache  # noqa: B019
     def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
         if self.hook_out is not None:
             # this is the hacked version. We just need to remove the hook_out and
             # reset a proper batch size
@@ -6435,31 +6475,32 @@
                 stack_dim=self.stack_dim,
             )
         else:
             out = LazyStackedTensorDict(
                 *[td.clone(recurse=False) for td in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-        if self._td_dim_names is not None:
-            out.names = self.names
+        if self._td_dim_name is not None:
+            out._td_dim_name = self._td_dim_name
         return out
 
     def pin_memory(self) -> TensorDictBase:
         for td in self.tensordicts:
             td.pin_memory()
         return self
 
     def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             kwargs.update({"batch_size": self.batch_size})
             out = dest(source=self, **kwargs)
-            if self._td_dim_names is not None:
-                out.names = self._td_dim_names
+            # if self._td_dim_name is not None:
+            # TODO: define a _has_names util to quickly check and avoid this
+            out.names = self.names
             return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
             return td
@@ -6685,54 +6726,42 @@
         elif isinstance(index, (Tensor, list)) and self.stack_dim != 0:
             tds = [tensordict[index] for tensordict in self.tensordicts]
             dim_drop = self.tensordicts[0].ndim - tds[0].ndim
             out = LazyStackedTensorDict(
                 *tds,
                 stack_dim=self.stack_dim - dim_drop,
             )
-            if self._td_dim_names is not None:
-                out.names = [
-                    name if i != out.stack_dim else self.names[self.stack_dim]
-                    for i, name in enumerate(out.names)
-                ]
+            if self._td_dim_name is not None:
+                out._td_dim_name = self._td_dim_name
             return out
         elif isinstance(index, slice) and self.stack_dim == 0:
             out = LazyStackedTensorDict(
                 *self.tensordicts[index], stack_dim=self.stack_dim
             )
-            if self._td_dim_names is not None:
-                out.names = [
-                    name if i != out.stack_dim else self.names[self.stack_dim]
-                    for i, name in enumerate(out.names)
-                ]
+            if self._td_dim_name is not None:
+                out._td_dim_name = self._td_dim_name
             return out
         elif isinstance(index, slice) and self.stack_dim != 0:
             out = LazyStackedTensorDict(
                 *[tensordict[index] for tensordict in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-            if self._td_dim_names is not None:
-                out.names = [
-                    name if i != out.stack_dim else self.names[self.stack_dim]
-                    for i, name in enumerate(out.names)
-                ]
+            if self._td_dim_name is not None:
+                out._td_dim_name = self._td_dim_name
             return out
         elif isinstance(index, (slice, Number)):
             new_stack_dim = (
                 self.stack_dim - 1 if isinstance(index, Number) else self.stack_dim
             )
             out = LazyStackedTensorDict(
                 *[td[index] for td in self.tensordicts],
                 stack_dim=new_stack_dim,
             )
-            if self._td_dim_names is not None:
-                out.names = [
-                    name if i != out.stack_dim else self.names[self.stack_dim]
-                    for i, name in enumerate(out.names)
-                ]
+            if self._td_dim_name is not None:
+                out._td_dim_name = self._td_dim_name
             return out
         elif isinstance(index, tuple):
             for i, item in enumerate(index):
                 if item is None:
                     truncated = tuple(
                         item if j != i else slice(None) for j, item in enumerate(index)
                     )
@@ -6772,19 +6801,16 @@
                 count += 1
                 if item is not None:
                     index_to_stack += [item]
             new_stack_dim = self.stack_dim - sum(
                 int(_is_number(_item)) - int(_item is None) for _item in index_to_stack
             )
             out = torch.stack(list(tensordicts), dim=new_stack_dim)
-            if self._td_dim_names is not None:
-                out.names = [
-                    name if i != out.stack_dim else self.names[self.stack_dim]
-                    for i, name in enumerate(out.names)
-                ]
+            if self._td_dim_name is not None:
+                out._td_dim_name = self._td_dim_name
             return out
         else:
             raise NotImplementedError(
                 f"selecting StackedTensorDicts with type "
                 f"{index.__class__.__name__} is not supported yet"
             )
 
@@ -7411,14 +7437,23 @@
             )(**self.custom_op_kwargs).shape
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         self._batch_size_setter(new_size)
 
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            f"Cannot erase names of a {type(self)}. "
+            f"Erase source TensorDict's names instead."
+        )
+
     def _rename_subtds(self, names):
         for key in self.keys():
             if _is_tensor_collection(self.entry_class(key)):
                 raise RuntimeError(
                     "Cannot rename dimensions of a lazy TensorDict with "
                     "nested collections. Convert the instance to a regular "
                     "tensordict by using the `to_tensordict()` method first."
@@ -8120,37 +8155,30 @@
 
 def _check_keys(
     list_of_tensordicts: Sequence[TensorDictBase],
     strict: bool = False,
     include_nested: bool = False,
     leaves_only: bool = False,
 ) -> set[str]:
-    keys: set[str] = set()
-    for td in list_of_tensordicts:
-        if not len(keys):
-            keys = set(td.keys(include_nested=include_nested, leaves_only=leaves_only))
-        else:
-            if not strict:
-                keys = keys.intersection(
-                    set(td.keys(include_nested=include_nested, leaves_only=leaves_only))
+    if not len(list_of_tensordicts):
+        return set()
+    keys: set[str] = set(
+        list_of_tensordicts[0].keys(
+            include_nested=include_nested, leaves_only=leaves_only
+        )
+    )
+    for td in list_of_tensordicts[1:]:
+        k = td.keys(include_nested=include_nested, leaves_only=leaves_only)
+        if not strict:
+            keys = keys.intersection(k)
+        else:
+            if set(k) != keys:
+                raise KeyError(
+                    f"got keys {keys} and {set(td.keys())} which are " f"incompatible"
                 )
-            else:
-                if len(
-                    set(
-                        td.keys(include_nested=include_nested, leaves_only=leaves_only)
-                    ).difference(keys)
-                ) or len(
-                    set(td.keys(include_nested=include_nested, leaves_only=leaves_only))
-                ) != len(
-                    keys
-                ):
-                    raise KeyError(
-                        f"got keys {keys} and {set(td.keys())} which are "
-                        f"incompatible"
-                    )
     return keys
 
 
 def _expand_to_match_shape(
     parent_batch_size: torch.Size,
     tensor: Tensor,
     self_batch_dims: int,
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.07.08'
-git_version = '938b166aaefa913ec7edd1ca28fa4ee334116926'
+__version__ = '2023.07.09'
+git_version = 'a73040fa1cbd5f5a8639653618e0dd570dfbd6e3'
```

## Comparing `tensordict_nightly-2023.7.8.dist-info/LICENSE` & `tensordict_nightly-2023.7.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2023.7.8.dist-info/METADATA` & `tensordict_nightly-2023.7.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2023.7.8
+Version: 2023.7.9
 Summary: UNKNOWN
 Home-page: https://github.com/pytorch-labs/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.7
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.7.8 Summary:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.7.9 Summary:
 UNKNOWN Home-page: https://github.com/pytorch-labs/tensordict Author:
 tensordict contributors Author-email: vmoens@fb.com License: BSD Platform:
 UNKNOWN Classifier: Programming Language :: Python :: 3.7 Classifier:
 Programming Language :: Python :: 3.8 Classifier: Programming Language ::
 Python :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
 Development Status :: 4 - Beta Description-Content-Type: text/markdown License-
 File: LICENSE Requires-Dist: torch Requires-Dist: numpy Requires-Dist:
```

## Comparing `tensordict_nightly-2023.7.8.dist-info/RECORD` & `tensordict_nightly-2023.7.9.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 tensordict/__init__.py,sha256=ytaPVEcwv6XL4tl1qrUFmX_0fPaaz5asm3G__DwcQU0,1206
 tensordict/_contextlib.py,sha256=yao_SZSgKUJt6dXHtAc5ZFJzAmm_NQQFYgR1rjDg0k8,6156
-tensordict/_tensordict.pyd,sha256=6pvdxrYF-eaAjuOpkRYgvhRBAqWwDdCf1bJ1ehyqrA0,114176
+tensordict/_tensordict.pyd,sha256=xL1geCda_I4H36LKeLMvM4CywZJSBGv8k224yMxmkIU,114176
 tensordict/memmap.py,sha256=t_1vr_qFN25rOLbzg0y783mMd7y96SbLW06IoZRR4ck,31293
-tensordict/persistent.py,sha256=-e-CSf_7Sq5Qf87WmEWeDSG8viNEYA2B1TP1g3JXEb0,34686
+tensordict/persistent.py,sha256=Shs64Y5xLKfaDElMCUwPy1y_bAy388yyYhRp9ybuYyI,34802
 tensordict/tensorclass.py,sha256=Q3pI92U8i5Acp6-hPXB_4_6yyC_iLJV-iFV4Tg9KJVw,33679
-tensordict/tensordict.py,sha256=LROf95uTfeJzkn4svhkSbAveKF6f6Ymlqv_Kfvvlmiw,317133
+tensordict/tensordict.py,sha256=_XM6uggkgEhQhXX__oB9cWL2a2SzozaGKnM_h5b23wc,317569
 tensordict/utils.py,sha256=TkU8f-seBk4_FfUDOAjTjhyPkbQJ4Q5uP5oANrE3pVw,39964
-tensordict/version.py,sha256=wXFZXmsrV4AuJbTBpXuD7NC4vaja5b8E1ZqLNfdzCoQ,86
+tensordict/version.py,sha256=pQ8sfQPvafHA6W57F-y6MnT4JVW3Sg3TlG0opZP5mUY,86
 tensordict/nn/__init__.py,sha256=IQE4Xm3ydZ_D6eXtY_AKcJxfXJkNPrZcqEMIR6fA9YU,1366
 tensordict/nn/common.py,sha256=eycYJW_KdHqU-2wnTolls3jPadY60e27rBVdelkOa9M,51384
 tensordict/nn/functional_modules.py,sha256=o-2GPB41eCK8u8IdtFsQM-yYqvvyLoWnJooUqw1SVRg,25098
 tensordict/nn/probabilistic.py,sha256=FdBzqPFZyhIy0G0MTPWYZpO19PoMRU7EmVO_IhqFo_A,23469
 tensordict/nn/sequence.py,sha256=VHLQWAAhoUHqsQJvOql8kVM62KtE7KKuPR-6mNOBnBE,19940
 tensordict/nn/utils.py,sha256=5uS9WJC-Bdwuq2_SDnrGzBj3gpboB2IS_fxbAe_isX0,10896
 tensordict/nn/distributions/__init__.py,sha256=vDaCGSo1Zo8HHUBxjeS5E6ov-sTFN766V9jGmInVSQE,512
 tensordict/nn/distributions/continuous.py,sha256=lFpgk2QN9a8WLCIdT2rUAWwLlXmnRQrwepppb0YNKeM,7272
 tensordict/nn/distributions/discrete.py,sha256=VrcrSPr9YyBDKagOphYTAgpiQqlwfpf4-8t3TsrJyPQ,2667
 tensordict/nn/distributions/truncated_normal.py,sha256=f--2ISj15TTUlLcUzMh1e50yADuh-vKMFbrzLqwUv7I,6694
 tensordict/nn/distributions/utils.py,sha256=3vEDATr12hUk9OYYKf4dzPmNMmLzKHQuZPsdWcPVfi4,1266
 tensordict/prototype/__init__.py,sha256=XBECOVFLLCiUsvNwwByWkz-U87nt7oJXPp2iIIA5Ysk,393
 tensordict/prototype/fx.py,sha256=q2TbyO7WAc4B292Y9EBUTKsVtCftnR9ewghcRLkB8GM,7875
 tensordict/prototype/tensorclass.py,sha256=4yIxnvcebszIUNXHmKTq9P85AyD8_aUhoJxr-p2m7K4,764
-tensordict_nightly-2023.7.8.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-tensordict_nightly-2023.7.8.dist-info/METADATA,sha256=TdRTkqkB6mkFmiufrUAEb4x1-qCBsVw6ow4HuXhuRu8,15788
-tensordict_nightly-2023.7.8.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-tensordict_nightly-2023.7.8.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2023.7.8.dist-info/RECORD,,
+tensordict_nightly-2023.7.9.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
+tensordict_nightly-2023.7.9.dist-info/METADATA,sha256=RG53GDeRYkwpCRw6CSE8KU5pICgUFXZTHVCJ7k8XaOA,15788
+tensordict_nightly-2023.7.9.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+tensordict_nightly-2023.7.9.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2023.7.9.dist-info/RECORD,,
```

