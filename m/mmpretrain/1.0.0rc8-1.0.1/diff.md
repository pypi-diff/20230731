# Comparing `tmp/mmpretrain-1.0.0rc8.tar.gz` & `tmp/mmpretrain-1.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/mmpretrain-1.0.0rc8.tar", last modified: Tue May 23 03:25:07 2023, max compression
+gzip compressed data, was "dist/mmpretrain-1.0.1.tar", last modified: Mon Jul 31 09:10:48 2023, max compression
```

## Comparing `mmpretrain-1.0.0rc8.tar` & `mmpretrain-1.0.1.tar`

### file list

```diff
@@ -1,1415 +1,1530 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/
--rw-r--r--   0 runner    (1001) docker     (123)      170 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    20818 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    17105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)     1114 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cifar100_bs16.py
--rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cifar10_bs16.py
--rw-r--r--   0 runner    (1001) docker     (123)     1746 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     2666 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     2892 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_vg_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     2280 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     1241 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_448.py
--rw-r--r--   0 runner    (1001) docker     (123)     1432 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet21k_bs128.py
--rw-r--r--   0 runner    (1001) docker     (123)     1871 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_mbv3.py
--rw-r--r--   0 runner    (1001) docker     (123)     2143 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_medium_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2143 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_small_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2326 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_revvit_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2142 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_medium_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     2142 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_small_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     2137 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_vig_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     1639 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_196.py
--rw-r--r--   0 runner    (1001) docker     (123)     1639 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_336.py
--rw-r--r--   0 runner    (1001) docker     (123)     1638 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_448.py
--rw-r--r--   0 runner    (1001) docker     (123)     1639 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_560.py
--rw-r--r--   0 runner    (1001) docker     (123)     1455 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_pil_bicubic_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_beitv2.py
--rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_davit_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2149 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_levit_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     1941 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a12.py
--rw-r--r--   0 runner    (1001) docker     (123)     1941 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a3.py
--rw-r--r--   0 runner    (1001) docker     (123)      957 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_simmim_192.py
--rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_swin_192.py
--rw-r--r--   0 runner    (1001) docker     (123)     1420 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32.py
--rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_byol.py
--rw-r--r--   0 runner    (1001) docker     (123)     1563 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_mocov2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_bicubic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1456 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_resize.py
--rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_simclr.py
--rw-r--r--   0 runner    (1001) docker     (123)      889 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mae.py
--rw-r--r--   0 runner    (1001) docker     (123)     2333 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mocov3.py
--rw-r--r--   0 runner    (1001) docker     (123)     1420 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64.py
--rw-r--r--   0 runner    (1001) docker     (123)     1671 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_autoaug.py
--rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     2232 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_448.py
--rw-r--r--   0 runner    (1001) docker     (123)     2139 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_convmixer_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_edgenext_256.py
--rw-r--r--   0 runner    (1001) docker     (123)     1461 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_mixer_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     1456 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize.py
--rw-r--r--   0 runner    (1001) docker     (123)     1831 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize_autoaug.py
--rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_256.py
--rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_384.py
--rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_t2t_224.py
--rw-r--r--   0 runner    (1001) docker     (123)     1567 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs8_pil_bicubic_320.py
--rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/inshop_bs32_448.py
--rw-r--r--   0 runner    (1001) docker     (123)     2249 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/nlvr2.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/pipelines/
--rw-r--r--   0 runner    (1001) docker     (123)     2870 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/pipelines/auto_aug.py
--rw-r--r--   0 runner    (1001) docker     (123)     1430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/pipelines/rand_aug.py
--rw-r--r--   0 runner    (1001) docker     (123)     2701 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/refcoco.py
--rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/voc_bs16.py
--rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/default_runtime.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/
--rw-r--r--   0 runner    (1001) docker     (123)      689 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/base-p16.py
--rw-r--r--   0 runner    (1001) docker     (123)      690 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/small-p16.py
--rw-r--r--   0 runner    (1001) docker     (123)      737 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/small-p32.py
--rw-r--r--   0 runner    (1001) docker     (123)      688 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/tiny-p16.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convmixer/
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-1024-20.py
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-1536-20.py
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-768-32.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/
--rw-r--r--   0 runner    (1001) docker     (123)      563 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      564 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-large.py
--rw-r--r--   0 runner    (1001) docker     (123)      563 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      562 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-tiny.py
--rw-r--r--   0 runner    (1001) docker     (123)      565 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-xlarge.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/atto.py
--rw-r--r--   0 runner    (1001) docker     (123)      621 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/base.py
--rw-r--r--   0 runner    (1001) docker     (123)      504 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/femto.py
--rw-r--r--   0 runner    (1001) docker     (123)      621 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/huge.py
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/large.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/nano.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/pico.py
--rw-r--r--   0 runner    (1001) docker     (123)      620 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/davit/
--rw-r--r--   0 runner    (1001) docker     (123)      495 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/davit/davit-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      495 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/davit/davit-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/davit/davit-tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/
--rw-r--r--   0 runner    (1001) docker     (123)      663 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-224.py
--rw-r--r--   0 runner    (1001) docker     (123)      664 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-384.py
--rw-r--r--   0 runner    (1001) docker     (123)      665 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-huge-p14-224.py
--rw-r--r--   0 runner    (1001) docker     (123)      665 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-224.py
--rw-r--r--   0 runner    (1001) docker     (123)      664 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-384.py
--rw-r--r--   0 runner    (1001) docker     (123)      663 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-medium-p16-224.py
--rw-r--r--   0 runner    (1001) docker     (123)      664 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-224.py
--rw-r--r--   0 runner    (1001) docker     (123)      663 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-384.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/densenet/
--rw-r--r--   0 runner    (1001) docker     (123)      316 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/densenet/densenet121.py
--rw-r--r--   0 runner    (1001) docker     (123)      316 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/densenet/densenet161.py
--rw-r--r--   0 runner    (1001) docker     (123)      316 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/densenet/densenet169.py
--rw-r--r--   0 runner    (1001) docker     (123)      316 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/densenet/densenet201.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/
--rw-r--r--   0 runner    (1001) docker     (123)      632 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      633 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      634 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xsmall.py
--rw-r--r--   0 runner    (1001) docker     (123)      635 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xxsmall.py
--rw-r--r--   0 runner    (1001) docker     (123)      598 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientformer-l1.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b0.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b1.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b2.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b3.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b4.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b5.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b6.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b7.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_b8.py
--rw-r--r--   0 runner    (1001) docker     (123)      412 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_em.py
--rw-r--r--   0 runner    (1001) docker     (123)      412 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_es.py
--rw-r--r--   0 runner    (1001) docker     (123)      340 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_l2.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b0.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b1.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b2.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b3.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_l.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_m.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_s.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_xl.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/eva/
--rw-r--r--   0 runner    (1001) docker     (123)      806 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/eva/eva-g.py
--rw-r--r--   0 runner    (1001) docker     (123)      838 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/eva/eva-l.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/
--rw-r--r--   0 runner    (1001) docker     (123)      738 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base-gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      752 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      756 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf384.py
--rw-r--r--   0 runner    (1001) docker     (123)      753 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large.py
--rw-r--r--   0 runner    (1001) docker     (123)      755 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small-gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      752 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      754 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny-gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      751 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w18.py
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w30.py
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w32.py
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w40.py
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w44.py
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w48.py
--rw-r--r--   0 runner    (1001) docker     (123)      419 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w64.py
--rw-r--r--   0 runner    (1001) docker     (123)      273 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/inception_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)      668 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/levit-256-p16.py
--rw-r--r--   0 runner    (1001) docker     (123)      674 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mae_vit-base-p16.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mixmim/
--rw-r--r--   0 runner    (1001) docker     (123)      653 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mixmim/mixmim_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      610 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mlp_mixer_base_patch16.py
--rw-r--r--   0 runner    (1001) docker     (123)      611 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mlp_mixer_large_patch16.py
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v2_1x.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_large_imagenet.py
--rw-r--r--   0 runner    (1001) docker     (123)      533 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_050_imagenet.py
--rw-r--r--   0 runner    (1001) docker     (123)      533 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_075_imagenet.py
--rw-r--r--   0 runner    (1001) docker     (123)      406 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_imagenet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s0.py
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s1.py
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s2.py
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s3.py
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s4.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilevit/
--rw-r--r--   0 runner    (1001) docker     (123)      339 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_s.py
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_xs.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_xxs.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      685 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-large.py
--rw-r--r--   0 runner    (1001) docker     (123)      623 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m36.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m48.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s12.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s24.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s36.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_1.6gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_12gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      345 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_3.2gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      345 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_4.0gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_400mf.py
--rw-r--r--   0 runner    (1001) docker     (123)      345 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_6.4gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      345 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_8.0gf.py
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_800mf.py
--rw-r--r--   0 runner    (1001) docker     (123)      647 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/replknet-31B_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      370 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/replknet-31L_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      369 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/replknet-XL_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      460 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/repmlp-base_224.py
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/repvgg-A0_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      551 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/repvgg-B3_lbs-mixup_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      431 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net101-w26-s4.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net50-w14-s8.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s4.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s6.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s8.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/res2net50-w48-s2.py
--rw-r--r--   0 runner    (1001) docker     (123)      650 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest101.py
--rw-r--r--   0 runner    (1001) docker     (123)      650 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest200.py
--rw-r--r--   0 runner    (1001) docker     (123)      650 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest269.py
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest50.py
--rw-r--r--   0 runner    (1001) docker     (123)      425 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet101.py
--rw-r--r--   0 runner    (1001) docker     (123)      408 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet101_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      425 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet152.py
--rw-r--r--   0 runner    (1001) docker     (123)      408 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet152_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      423 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet18.py
--rw-r--r--   0 runner    (1001) docker     (123)      406 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet18_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      423 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet34.py
--rw-r--r--   0 runner    (1001) docker     (123)      406 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet34_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      425 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet34_gem.py
--rw-r--r--   0 runner    (1001) docker     (123)      424 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50.py
--rw-r--r--   0 runner    (1001) docker     (123)      407 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)      549 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_cutmix.py
--rw-r--r--   0 runner    (1001) docker     (123)      487 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_mixup.py
--rw-r--r--   0 runner    (1001) docker     (123)      538 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cutmix.py
--rw-r--r--   0 runner    (1001) docker     (123)      458 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_label_smooth.py
--rw-r--r--   0 runner    (1001) docker     (123)      484 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_mixup.py
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnetv1c50.py
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnetv1d101.py
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnetv1d152.py
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnetv1d50.py
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnext101_32x4d.py
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnext101_32x8d.py
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnext152_32x4d.py
--rw-r--r--   0 runner    (1001) docker     (123)      471 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnext50_32x4d.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/revvit/
--rw-r--r--   0 runner    (1001) docker     (123)      705 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/revvit/revvit-base.py
--rw-r--r--   0 runner    (1001) docker     (123)      705 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/revvit/revvit-small.py
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/seresnet101.py
--rw-r--r--   0 runner    (1001) docker     (123)      426 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/seresnet50.py
--rw-r--r--   0 runner    (1001) docker     (123)      495 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/seresnext101_32x4d.py
--rw-r--r--   0 runner    (1001) docker     (123)      494 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/seresnext50_32x4d.py
--rw-r--r--   0 runner    (1001) docker     (123)      338 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/shufflenet_v1_1x.py
--rw-r--r--   0 runner    (1001) docker     (123)      347 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/shufflenet_v2_1x.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/
--rw-r--r--   0 runner    (1001) docker     (123)      767 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_224.py
--rw-r--r--   0 runner    (1001) docker     (123)      458 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_384.py
--rw-r--r--   0 runner    (1001) docker     (123)      376 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/large_224.py
--rw-r--r--   0 runner    (1001) docker     (123)      459 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/large_384.py
--rw-r--r--   0 runner    (1001) docker     (123)      775 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/small_224.py
--rw-r--r--   0 runner    (1001) docker     (123)      766 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/tiny_224.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/
--rw-r--r--   0 runner    (1001) docker     (123)      793 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_256.py
--rw-r--r--   0 runner    (1001) docker     (123)      518 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_384.py
--rw-r--r--   0 runner    (1001) docker     (123)      431 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/large_256.py
--rw-r--r--   0 runner    (1001) docker     (123)      431 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/large_384.py
--rw-r--r--   0 runner    (1001) docker     (123)      793 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/small_256.py
--rw-r--r--   0 runner    (1001) docker     (123)      792 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/tiny_256.py
--rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-14.py
--rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-19.py
--rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-24.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/
--rw-r--r--   0 runner    (1001) docker     (123)      694 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-11m.py
--rw-r--r--   0 runner    (1001) docker     (123)      694 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-21m.py
--rw-r--r--   0 runner    (1001) docker     (123)      693 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-5m.py
--rw-r--r--   0 runner    (1001) docker     (123)      809 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tnt_s_patch16_224.py
--rw-r--r--   0 runner    (1001) docker     (123)      906 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/twins_pcpvt_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      893 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/twins_svt_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/
--rw-r--r--   0 runner    (1001) docker     (123)      456 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_large.py
--rw-r--r--   0 runner    (1001) docker     (123)      732 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_small.py
--rw-r--r--   0 runner    (1001) docker     (123)      731 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_tiny.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg11.py
--rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg11bn.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg13.py
--rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg13bn.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg16.py
--rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg16bn.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg19.py
--rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vgg19bn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/
--rw-r--r--   0 runner    (1001) docker     (123)      818 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      819 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_medium.py
--rw-r--r--   0 runner    (1001) docker     (123)      818 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_small.py
--rw-r--r--   0 runner    (1001) docker     (123)      817 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_tiny.py
--rw-r--r--   0 runner    (1001) docker     (123)      848 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      849 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_small.py
--rw-r--r--   0 runner    (1001) docker     (123)      848 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_tiny.py
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-base-p16.py
--rw-r--r--   0 runner    (1001) docker     (123)      592 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-base-p32.py
--rw-r--r--   0 runner    (1001) docker     (123)      593 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-large-p16.py
--rw-r--r--   0 runner    (1001) docker     (123)      593 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-large-p32.py
--rw-r--r--   0 runner    (1001) docker     (123)      498 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/wide-resnet50.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/
--rw-r--r--   0 runner    (1001) docker     (123)      493 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/cifar10_bs128.py
--rw-r--r--   0 runner    (1001) docker     (123)      820 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/cub_bs64.py
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1137 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_revvit.py
--rw-r--r--   0 runner    (1001) docker     (123)     1134 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_swin.py
--rw-r--r--   0 runner    (1001) docker     (123)      573 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_coslr.py
--rw-r--r--   0 runner    (1001) docker     (123)      606 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_linearlr_bn_nowd.py
--rw-r--r--   0 runner    (1001) docker     (123)      624 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048.py
--rw-r--r--   0 runner    (1001) docker     (123)     1133 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_AdamW.py
--rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_adamw_levit.py
--rw-r--r--   0 runner    (1001) docker     (123)      870 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_coslr.py
--rw-r--r--   0 runner    (1001) docker     (123)      800 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_rsb.py
--rw-r--r--   0 runner    (1001) docker     (123)      497 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256.py
--rw-r--r--   0 runner    (1001) docker     (123)      498 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_140e.py
--rw-r--r--   0 runner    (1001) docker     (123)      804 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_200e_coslr_warmup.py
--rw-r--r--   0 runner    (1001) docker     (123)      496 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr.py
--rw-r--r--   0 runner    (1001) docker     (123)      991 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr_coswd_300e.py
--rw-r--r--   0 runner    (1001) docker     (123)      479 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_epochstep.py
--rw-r--r--   0 runner    (1001) docker     (123)      953 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs4096_AdamW.py
--rw-r--r--   0 runner    (1001) docker     (123)      515 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_200e.py
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_90e.py
--rw-r--r--   0 runner    (1001) docker     (123)      384 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_coslr_100e.py
--rw-r--r--   0 runner    (1001) docker     (123)      350 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_coslr_200e.py
--rw-r--r--   0 runner    (1001) docker     (123)      381 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_steplr_100e.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/arcface/
--rw-r--r--   0 runner    (1001) docker     (123)      871 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/arcface/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1857 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/arcface/resnet50-arcface_8xb32_inshop.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/
--rw-r--r--   0 runner    (1001) docker     (123)     1965 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-1000e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1963 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-300e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      368 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/benchmarks/resnet50_8xb32-linear-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1641 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/
--rw-r--r--   0 runner    (1001) docker     (123)     3752 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/beit_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3517 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1144 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/
--rw-r--r--   0 runner    (1001) docker     (123)     3432 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-1600e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3530 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      935 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2602 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/
--rw-r--r--   0 runner    (1001) docker     (123)     1592 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb16_refcoco.py
--rw-r--r--   0 runner    (1001) docker     (123)     1555 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     1528 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_nlvr.py
--rw-r--r--   0 runner    (1001) docker     (123)     2090 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     3401 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/
--rw-r--r--   0 runner    (1001) docker     (123)     2397 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb32_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     2150 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2_8xb32_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     2399 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     1482 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50-c4_ms-1x_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)      911 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50_fpn_ms-1x_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1669 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/byol_resnet50_16xb256-coslr-200e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1573 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3581 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3138 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/cae_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1553 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/
--rw-r--r--   0 runner    (1001) docker     (123)     1869 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_resnet50_zeroshot-cls_cifar100.py
--rw-r--r--   0 runner    (1001) docker     (123)     2030 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-base-p16_zeroshot-cls_cifar100.py
--rw-r--r--   0 runner    (1001) docker     (123)     1999 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-huge-p14_zeroshot-cls_cifar100.py
--rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-large-p14_zeroshot-cls_cifar100.py
--rw-r--r--   0 runner    (1001) docker     (123)     2928 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/
--rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-448px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-448px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      789 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-large-p14_headless.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/conformer-base-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      250 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/conformer-small-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      250 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/conformer-small-p32_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/conformer-tiny-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3086 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/
--rw-r--r--   0 runner    (1001) docker     (123)      988 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-1024-20_10xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      988 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-1536-20_10xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-768-32_10xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/
--rw-r--r--   0 runner    (1001) docker     (123)      668 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      670 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)      667 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      653 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      655 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      668 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      668 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)    15777 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-atto_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      660 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-femto_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1493 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-512px.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      896 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      896 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-pico_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)    18532 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/
--rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspdarknet50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspresnet50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspresnext50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2520 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/csra/
--rw-r--r--   0 runner    (1001) docker     (123)      961 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/csra/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2558 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/csra/resnet101-csra_1xb16_voc07-448px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/
--rw-r--r--   0 runner    (1001) docker     (123)      260 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/davit-base_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/davit-small_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      260 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/davit-tiny_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/
--rw-r--r--   0 runner    (1001) docker     (123)      995 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1177 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      999 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base_16xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1176 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-small-distilled_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1273 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-small_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-tiny-distilled_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-tiny_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     6153 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-huge-p14_64xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb16_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      546 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-medium-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)    11683 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      500 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/benchmarks/resnet50_8xb32-linear-steplr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1010 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/densecl_resnet50_8xb32-coslr-200e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1594 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/densenet121_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/densenet161_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/densenet169_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/densenet201_4xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2856 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/
--rw-r--r--   0 runner    (1001) docker     (123)     2626 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      445 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/vit-base-p14_dinov2-pre_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/vit-giant-p14_dinov2-pre_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      446 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/vit-large-p14_dinov2-pre_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      453 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/vit-small-p14_dinov2-pre_headless.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/
--rw-r--r--   0 runner    (1001) docker     (123)      425 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256-usi_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      613 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      426 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256-usi_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      615 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-xsmall_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      616 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-xxsmall_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/
--rw-r--r--   0 runner    (1001) docker     (123)      217 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/efficientformer-l1_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      115 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/efficientformer-l3_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      115 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/efficientformer-l7_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2687 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-em_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-es_8xb32-01norm_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb32_in1k-475px.py
--rw-r--r--   0 runner    (1001) docker     (123)      770 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb8_in1k-800px.py
--rw-r--r--   0 runner    (1001) docker     (123)    25083 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/
--rw-r--r--   0 runner    (1001) docker     (123)     1635 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b0_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      679 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b1_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      679 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      679 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b3_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      675 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in1k-480px.py
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)      675 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in1k-480px.py
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1012 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in1k-512px.py
--rw-r--r--   0 runner    (1001) docker     (123)      105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)    11840 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p14_8xb16_in1k-336px.py
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p14_8xb16_in1k-560px.py
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p16_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-l-p14_8xb16_in1k-196px.py
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-l-p14_8xb16_in1k-336px.py
--rw-r--r--   0 runner    (1001) docker     (123)      644 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-l-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)     2378 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-mae-style_vit-base-p16_16xb256-coslr-400e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     9809 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/
--rw-r--r--   0 runner    (1001) docker     (123)      513 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-base-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      858 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-base-p14_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      513 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-large-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      859 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-large-p14_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      492 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-small-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      837 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-small-p14_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      492 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      837 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     8019 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/
--rw-r--r--   0 runner    (1001) docker     (123)     2797 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     2716 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     3239 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     1494 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/glip/
--rw-r--r--   0 runner    (1001) docker     (123)      487 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/glip/glip-l_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      434 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/glip/glip-t_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)     1604 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/glip/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/
--rw-r--r--   0 runner    (1001) docker     (123)      396 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-base-gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      393 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-base_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      397 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-small-gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      394 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-small_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      397 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-tiny-gf_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      396 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/hornet-tiny_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     4401 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w18_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w30_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w32_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w40_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w44_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w48_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/hrnet-w64_4xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/inception_v3/
--rw-r--r--   0 runner    (1001) docker     (123)      749 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/inception_v3/inception-v3_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1281 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/inception_v3/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/lenet/
--rw-r--r--   0 runner    (1001) docker     (123)     2535 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/lenet/lenet5_mnist.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/
--rw-r--r--   0 runner    (1001) docker     (123)      105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/levit-128_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/levit-128s_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/levit-192_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/levit-256_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      105 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/deploy/levit-384_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/levit-128_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      347 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/levit-128s_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/levit-192_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      260 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/levit-256_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      377 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/levit-384_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3633 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3194 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1721 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb2048-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_32xb8-coslr-50e_in1k-448px.py
--rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-coslr-50e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-coslr-50e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb2048-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1450 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-1600e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1445 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-400e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1452 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-800e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1692 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-huge-p14_8xb512-amp-coslr-1600e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1584 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-1600e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1579 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-400e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-800e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)    13047 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/benchmarks/vit-base-p16_8xb256-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2854 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/maskfeat_vit-base-p16_8xb256-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2394 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/milan_vit-base-p16_16xb256-amp-coslr-400e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     3226 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb128-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      203 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1828 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2556 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/mixmim_mixmim-base_16xb128-coslr-300e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mlp_mixer/
--rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mlp_mixer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      256 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-base-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      257 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-large-p16_64xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v2/
--rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      200 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v2/mobilenet-v2_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/
--rw-r--r--   0 runner    (1001) docker     (123)     4421 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      817 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-large_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1986 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-050_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-075_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      817 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      376 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb16_cifar10.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s0_deploy_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s1_deploy_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s2_deploy_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s3_deploy_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s4_deploy_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2834 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      513 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s0_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1891 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s1_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s3_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1986 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s4_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/
--rw-r--r--   0 runner    (1001) docker     (123)     2302 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      728 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      729 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-xsmall_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      730 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-xxsmall_8xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      500 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/benchmarks/resnet50_8xb32-linear-steplr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/mocov2_resnet50_8xb32-coslr-200e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      762 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/resnet50_8xb128-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1148 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb128-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb64-coslr-150e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1921 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-large-p16_8xb64-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1156 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-small-p16_8xb128-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     8391 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2305 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-800e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3934 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-base-p16_16xb256-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     4023 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-large-p16_64xb64-amp-coslr-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3942 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-small-p16_16xb256-amp-coslr-300e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/
--rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-base_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-large_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-small_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1197 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-tiny_8xb256_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/
--rw-r--r--   0 runner    (1001) docker     (123)     2932 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      942 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_refcoco.py
--rw-r--r--   0 runner    (1001) docker     (123)     1532 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)      881 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_zeroshot_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)      924 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-large_zeroshot_vqa.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/
--rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      530 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-m36_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      530 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-m48_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s12_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s24_32xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s36_32xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/
--rw-r--r--   0 runner    (1001) docker     (123)     4244 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      166 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-1.6gf_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      520 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-12gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-3.2gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-4.0gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1671 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-400mf_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-6.4gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-8.0gf_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      166 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-800mf_8xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/deploy/
--rw-r--r--   0 runner    (1001) docker     (123)      103 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      103 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/deploy/replknet-31L-deploy_32xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      102 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/deploy/replknet-XL-deploy_32xb64_in1k-320px.py
--rw-r--r--   0 runner    (1001) docker     (123)     4867 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      371 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/replknet-31B_32xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/replknet-31B_32xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      371 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/replknet-31L_32xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      369 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/replknet-XL_32xb64_in1k-320px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/
--rw-r--r--   0 runner    (1001) docker     (123)     1871 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1126 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      879 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       83 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_delopy_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_deploy_8xb64_in1k-256px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/
--rw-r--r--   0 runner    (1001) docker     (123)     5987 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      876 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-A0_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       79 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-A0_deploy_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       77 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-A1_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-A2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B0_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B1_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B1g2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B1g4_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B2_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      108 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B2g4_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B3_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       79 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B3g4_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      675 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-D2se_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/
--rw-r--r--   0 runner    (1001) docker     (123)     2614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      188 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/res2net101-w26-s4_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/res2net50-w14-s8_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/res2net50-w26-s8_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/
--rw-r--r--   0 runner    (1001) docker     (123)     2343 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/_randaug_policies.py
--rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest101_32xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest200_64xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest269_64xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2083 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest50_32xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/
--rw-r--r--   0 runner    (1001) docker     (123)    11688 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      173 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet101_8xb16_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet101_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      173 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet152_8xb16_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet152_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      168 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet18_8xb16_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      164 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet18_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      168 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet34_8xb16_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      164 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet34_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      175 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup-coslr_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      313 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup-lbs_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      307 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb128_coslr-90e_in21k.py
--rw-r--r--   0 runner    (1001) docker     (123)      178 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb16-mixup_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      168 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb16_cifar10.py
--rw-r--r--   0 runner    (1001) docker     (123)      432 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb16_cifar100.py
--rw-r--r--   0 runner    (1001) docker     (123)     1339 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a1-600e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a2-300e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      587 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a3-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      401 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-coslr-preciseBN_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      174 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-coslr_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      175 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-cutmix_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      126 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-fp16-dynamic_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-fp16_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      181 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-lbs_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      174 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32-mixup_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      164 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      684 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb8_cub.py
--rw-r--r--   0 runner    (1001) docker     (123)      222 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1c101_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      222 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1c152_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      182 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1c50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1d101_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1d152_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      182 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnetv1d50_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/
--rw-r--r--   0 runner    (1001) docker     (123)     2559 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/resnext101-32x4d_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/resnext101-32x8d_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/resnext152-32x4d_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      186 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/resnext50-32x4d_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/revvit/
--rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/revvit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      208 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/revvit/revvit-base_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/revvit/revvit-small_8xb256_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-m36-deploy_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-m36-deploy_8xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-m48-deploy_8xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)       83 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-m48-deploy_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       90 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s12-deploy_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s12-deploy_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       90 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s24-deploy_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s24-deploy_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       84 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s36-deploy_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/deploy/riformer-s36-deploy_8xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     5186 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m36_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m36_8xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s36_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s36_8xb64_in1k-384px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/
--rw-r--r--   0 runner    (1001) docker     (123)     2097 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      502 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/vit-base-p16_sam_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      502 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/vit-huge-p16_sam_headless.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/vit-large-p16_sam_headless.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/
--rw-r--r--   0 runner    (1001) docker     (123)     1565 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      182 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/seresnet101_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      190 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/seresnet50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/seresnext101-32x4d_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      188 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/seresnext50-32x4d_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v1/
--rw-r--r--   0 runner    (1001) docker     (123)     1220 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v1/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v1/shufflenet-v1-1x_16xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v2/
--rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v2/shufflenet-v2-1x_16xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1308 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-200e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-800e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_8xb32-coslr-200e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)     1579 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w6_8xb256-coslr-100e_in1k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)     2789 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w7_8xb256-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-large-w14_8xb256-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     4633 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-100e_in1k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1768 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-800e_in1k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_8xb256-amp-coslr-100e_in1k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)     1798 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-large-w12_16xb128-amp-coslr-800e_in1k-192px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2766 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-100e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-200e_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/benchmarks/
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1617 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4124 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/swav_resnet50_8xb32-mcrop-coslr-200e_in1k-224px-96px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/
--rw-r--r--   0 runner    (1001) docker     (123)     8957 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      283 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-large_16xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      283 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-large_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-large_8xb8_cub-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      283 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-small_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-tiny_16xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/
--rw-r--r--   0 runner    (1001) docker     (123)     9205 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      584 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w12_8xb128_in21k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)      271 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w16_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w16_in21k-pre_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w24_in21k-pre_16xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      213 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w8_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      584 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w12_8xb128_in21k-192px.py
--rw-r--r--   0 runner    (1001) docker     (123)      379 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w16_in21k-pre_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      410 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w24_in21k-pre_16xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      272 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-small-w16_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      214 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-small-w8_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      271 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-tiny-w16_16xb64_in1k-256px.py
--rw-r--r--   0 runner    (1001) docker     (123)      213 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-tiny-w8_16xb64_in1k-256px.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/
--rw-r--r--   0 runner    (1001) docker     (123)     1983 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-14_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-19_8xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-24_8xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/
--rw-r--r--   0 runner    (1001) docker     (123)     6452 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)       49 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-11m-distill_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      208 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-11m_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      677 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      691 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-512px.py
--rw-r--r--   0 runner    (1001) docker     (123)       49 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      208 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-5m-distill_8xb256_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      207 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-5m_8xb256_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tnt/
--rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tnt/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1538 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tnt/tnt-s-p16_16xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/
--rw-r--r--   0 runner    (1001) docker     (123)     5169 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-pcpvt-base_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      192 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-pcpvt-large_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-pcpvt-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-svt-base_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      191 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-svt-large_16xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      131 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-svt-small_8xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/
--rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-base_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-large_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-tiny_8xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/
--rw-r--r--   0 runner    (1001) docker     (123)     4063 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      248 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg11_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg11bn_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      248 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg13_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg13bn_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg16_8xb16_voc.py
--rw-r--r--   0 runner    (1001) docker     (123)      248 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg16_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg16bn_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      248 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg19_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg19bn_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/
--rw-r--r--   0 runner    (1001) docker     (123)     5221 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      603 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/pvig-base_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      196 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/pvig-medium_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      195 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/pvig-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      194 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/pvig-tiny_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      186 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/vig-base_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/vig-small_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      186 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/vig-tiny_8xb128_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/
--rw-r--r--   0 runner    (1001) docker     (123)     4067 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_32xb128-mae_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     3402 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_4xb544-ipu_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      399 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      399 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/
--rw-r--r--   0 runner    (1001) docker     (123)     2876 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      224 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/wide-resnet101_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      184 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/wide-resnet50_8xb32_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      185 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/wide-resnet50_timm_8xb32_in1k.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/
--rw-r--r--   0 runner    (1001) docker     (123)    28291 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      815 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      815 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      812 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k-384px.py
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k.py
--rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/model-index.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/
--rw-r--r--   0 runner    (1001) docker     (123)     7189 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/analyze_logs.py
--rw-r--r--   0 runner    (1001) docker     (123)     4037 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/analyze_results.py
--rw-r--r--   0 runner    (1001) docker     (123)     3750 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (123)     1891 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/eval_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/get_flops.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      548 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_c4.sh
--rw-r--r--   0 runner    (1001) docker     (123)      366 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_fpn.sh
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      796 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_c4.sh
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_fpn.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      394 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_train.sh
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      642 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_train.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dataset_converters/
--rw-r--r--   0 runner    (1001) docker     (123)     1524 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dataset_converters/convert_imagenet_subsets.py
--rw-r--r--   0 runner    (1001) docker     (123)      924 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dataset_converters/convert_inaturalist.py
--rw-r--r--   0 runner    (1001) docker     (123)      479 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dist_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      442 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dist_train.sh
--rw-r--r--   0 runner    (1001) docker     (123)     8957 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/kfold-cross-valid.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/misc/
--rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/misc/print_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4771 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/misc/verify_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/
--rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/clip_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1725 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/convnext_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     3194 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/davit_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/deit3_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/edgenext_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     8753 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/efficientnet_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/efficientnetv2_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/eva02_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/eva_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2400 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/glip_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/hornet2mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/levit2mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2748 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mixmim_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1684 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mlpmixer_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     4737 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mobilenetv2_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/ofa.py
--rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/publish_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1935 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/reparameterize_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/replknet_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/repvgg_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     3099 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/revvit_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     4142 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/shufflenetv2_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1703 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/tinyvit_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1838 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/torchvision_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     2180 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/twins2mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1895 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/van2mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     4093 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/vgg_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)     3360 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/vig_to_mmpretrain.py
--rw-r--r--   0 runner    (1001) docker     (123)      566 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/slurm_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      574 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/slurm_train.sh
--rw-r--r--   0 runner    (1001) docker     (123)     6729 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/
--rw-r--r--   0 runner    (1001) docker     (123)     3842 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/mmpretrain2torchserve.py
--rw-r--r--   0 runner    (1001) docker     (123)     2420 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/mmpretrain_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1422 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/test_torchserver.py
--rw-r--r--   0 runner    (1001) docker     (123)     5419 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/train.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)     9156 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/browse_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    10247 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_cam.py
--rw-r--r--   0 runner    (1001) docker     (123)     9020 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)     9745 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_tsne.py
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/
--rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14927 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     4649 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/feature_extractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6257 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/image_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     8646 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/image_classification.py
--rw-r--r--   0 runner    (1001) docker     (123)    10928 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/image_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)    15940 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/model.py
--rw-r--r--   0 runner    (1001) docker     (123)    23773 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/multimodal_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     5737 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/nlvr.py
--rw-r--r--   0 runner    (1001) docker     (123)    10844 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     7048 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/visual_grounding.py
--rw-r--r--   0 runner    (1001) docker     (123)     7214 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/apis/visual_question_answering.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)     1863 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8037 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/base_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)      844 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3772 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/caltech101.py
--rw-r--r--   0 runner    (1001) docker     (123)    58132 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/categories.py
--rw-r--r--   0 runner    (1001) docker     (123)     7876 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     5191 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/cub.py
--rw-r--r--   0 runner    (1001) docker     (123)    10408 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/custom.py
--rw-r--r--   0 runner    (1001) docker     (123)     5461 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/dataset_wrappers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3664 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/dtd.py
--rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/fgvcaircraft.py
--rw-r--r--   0 runner    (1001) docker     (123)    10694 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/flamingo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3500 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/flowers102.py
--rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/food101.py
--rw-r--r--   0 runner    (1001) docker     (123)     3823 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/imagenet.py
--rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/inshop.py
--rw-r--r--   0 runner    (1001) docker     (123)     8147 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/mnist.py
--rw-r--r--   0 runner    (1001) docker     (123)     3227 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/multi_label.py
--rw-r--r--   0 runner    (1001) docker     (123)    11053 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/multi_task.py
--rw-r--r--   0 runner    (1001) docker     (123)     1092 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/nlvr2.py
--rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/oxfordiiitpet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1452 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/places205.py
--rw-r--r--   0 runner    (1001) docker     (123)     2600 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/refcoco.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/
--rw-r--r--   0 runner    (1001) docker     (123)      184 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3845 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/repeat_aug.py
--rw-r--r--   0 runner    (1001) docker     (123)     2207 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/sequential.py
--rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/scienceqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     5418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/stanfordcars.py
--rw-r--r--   0 runner    (1001) docker     (123)     9313 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/sun397.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/
--rw-r--r--   0 runner    (1001) docker     (123)     2027 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    48878 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/auto_augment.py
--rw-r--r--   0 runner    (1001) docker     (123)    11347 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/formatting.py
--rw-r--r--   0 runner    (1001) docker     (123)    63637 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/processing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5280 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/wrappers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7667 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3002 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/vg_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/visual_genome.py
--rw-r--r--   0 runner    (1001) docker     (123)     5418 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/datasets/voc.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/
--rw-r--r--   0 runner    (1001) docker     (123)      178 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      780 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/class_num_check_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     1556 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/densecl_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     8632 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/ema_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/margin_head_hooks.py
--rw-r--r--   0 runner    (1001) docker     (123)     8774 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/precise_bn_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/retriever_hooks.py
--rw-r--r--   0 runner    (1001) docker     (123)     1657 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/simsiam_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/swav_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     6732 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/switch_recipe_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     5026 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/visualization_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     2314 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/warmup_param_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/
--rw-r--r--   0 runner    (1001) docker     (123)      297 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11336 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/adan_t.py
--rw-r--r--   0 runner    (1001) docker     (123)     9660 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/lamb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4758 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/lars.py
--rw-r--r--   0 runner    (1001) docker     (123)     7409 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/layer_decay_optim_wrapper_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/runners/
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/runners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6547 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/engine/runners/retrieval_loop.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/
--rw-r--r--   0 runner    (1001) docker     (123)      135 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/functional/
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/functional/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      782 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4497 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/caption.py
--rw-r--r--   0 runner    (1001) docker     (123)    24925 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/multi_label.py
--rw-r--r--   0 runner    (1001) docker     (123)     4826 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/multi_task.py
--rw-r--r--   0 runner    (1001) docker     (123)     9795 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)     6756 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/scienceqa.py
--rw-r--r--   0 runner    (1001) docker     (123)    31789 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/single_label.py
--rw-r--r--   0 runner    (1001) docker     (123)     2915 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/visual_grounding_eval.py
--rw-r--r--   0 runner    (1001) docker     (123)     4184 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/voc_multi_label.py
--rw-r--r--   0 runner    (1001) docker     (123)    10435 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/vqa.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/
--rw-r--r--   0 runner    (1001) docker     (123)      809 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/alexnet.py
--rw-r--r--   0 runner    (1001) docker     (123)      957 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/base_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)    22489 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/beit.py
--rw-r--r--   0 runner    (1001) docker     (123)    22645 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6192 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/convmixer.py
--rw-r--r--   0 runner    (1001) docker     (123)    13603 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/convnext.py
--rw-r--r--   0 runner    (1001) docker     (123)    25507 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/cspnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    30645 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/davit.py
--rw-r--r--   0 runner    (1001) docker     (123)     4788 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/deit.py
--rw-r--r--   0 runner    (1001) docker     (123)    17185 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/deit3.py
--rw-r--r--   0 runner    (1001) docker     (123)    12016 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/densenet.py
--rw-r--r--   0 runner    (1001) docker     (123)    15475 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/edgenext.py
--rw-r--r--   0 runner    (1001) docker     (123)    22042 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    15726 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    16382 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientnet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    18959 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/hornet.py
--rw-r--r--   0 runner    (1001) docker     (123)    23396 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/hrnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    18806 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/inception_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/lenet.py
--rw-r--r--   0 runner    (1001) docker     (123)    18201 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/levit.py
--rw-r--r--   0 runner    (1001) docker     (123)    19834 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mixmim.py
--rw-r--r--   0 runner    (1001) docker     (123)     9664 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mlp_mixer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9600 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     8780 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilenet_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)    18955 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobileone.py
--rw-r--r--   0 runner    (1001) docker     (123)    17053 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilevit.py
--rw-r--r--   0 runner    (1001) docker     (123)    26094 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mvit.py
--rw-r--r--   0 runner    (1001) docker     (123)    14706 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/poolformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    11898 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/regnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    25324 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/replknet.py
--rw-r--r--   0 runner    (1001) docker     (123)    22849 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/repmlp.py
--rw-r--r--   0 runner    (1001) docker     (123)    22087 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/repvgg.py
--rw-r--r--   0 runner    (1001) docker     (123)    11237 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/res2net.py
--rw-r--r--   0 runner    (1001) docker     (123)    12235 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnest.py
--rw-r--r--   0 runner    (1001) docker     (123)    24500 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3711 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnet_cifar.py
--rw-r--r--   0 runner    (1001) docker     (123)     6260 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnext.py
--rw-r--r--   0 runner    (1001) docker     (123)    24265 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/revvit.py
--rw-r--r--   0 runner    (1001) docker     (123)    14049 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/riformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/seresnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/seresnext.py
--rw-r--r--   0 runner    (1001) docker     (123)    11596 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/shufflenet_v1.py
--rw-r--r--   0 runner    (1001) docker     (123)    10783 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/shufflenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    23638 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    23060 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/swin_transformer_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    16685 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/t2t_vit.py
--rw-r--r--   0 runner    (1001) docker     (123)     4213 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/timm_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)    26505 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/tinyvit.py
--rw-r--r--   0 runner    (1001) docker     (123)    14530 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/tnt.py
--rw-r--r--   0 runner    (1001) docker     (123)    30218 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/twins.py
--rw-r--r--   0 runner    (1001) docker     (123)    15649 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/van.py
--rw-r--r--   0 runner    (1001) docker     (123)     6756 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vgg.py
--rw-r--r--   0 runner    (1001) docker     (123)    31677 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vig.py
--rw-r--r--   0 runner    (1001) docker     (123)    19677 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vision_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    12783 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vit_eva02.py
--rw-r--r--   0 runner    (1001) docker     (123)    27014 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vit_sam.py
--rw-r--r--   0 runner    (1001) docker     (123)    29245 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/backbones/xcit.py
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/
--rw-r--r--   0 runner    (1001) docker     (123)      299 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4332 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/hugging_face.py
--rw-r--r--   0 runner    (1001) docker     (123)    10673 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/image.py
--rw-r--r--   0 runner    (1001) docker     (123)     8104 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/timm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/
--rw-r--r--   0 runner    (1001) docker     (123)     1982 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1685 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/beitv1_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     1861 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/beitv2_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2430 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/cae_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     6046 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/cls_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4579 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/conformer_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     1618 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/contrastive_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3023 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/deit_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/efficientformer_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     8046 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/grounding_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     6310 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/itc_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/itm_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3077 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/latent_heads.py
--rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/levit_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2344 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/linear_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/mae_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    11382 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/margin_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     1003 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/mim_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     1396 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/mixmim_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/mocov3_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     5919 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_cls_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4117 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_csra_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_linear_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     5278 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_task_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     6720 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/seq_gen_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     1234 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/simmim_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4538 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/stacked_head.py
--rw-r--r--   0 runner    (1001) docker     (123)      711 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/swav_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2265 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/vig_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/vision_transformer_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     9757 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/heads/vqa_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/
--rw-r--r--   0 runner    (1001) docker     (123)     1135 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5535 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/asymmetric_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1496 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/cae_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/cosine_similarity_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1337 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/cross_correlation_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7602 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/cross_entropy_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4285 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/focal_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7173 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/label_smooth_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     2393 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/reconstruction_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     6733 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/seesaw_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/swav_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3694 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/losses/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/
--rw-r--r--   0 runner    (1001) docker     (123)      709 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/
--rw-r--r--   0 runner    (1001) docker     (123)      461 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7055 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     9274 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_grounding.py
--rw-r--r--   0 runner    (1001) docker     (123)     7761 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_nlvr.py
--rw-r--r--   0 runner    (1001) docker     (123)    28731 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (123)    10422 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)    52002 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/language_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/
--rw-r--r--   0 runner    (1001) docker     (123)    32118 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/Qformer.py
--rw-r--r--   0 runner    (1001) docker     (123)      331 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8458 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_caption.py
--rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_opt_vqa.py
--rw-r--r--   0 runner    (1001) docker     (123)    21891 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)    48175 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/modeling_opt.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/
--rw-r--r--   0 runner    (1001) docker     (123)      192 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10497 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/bert.py
--rw-r--r--   0 runner    (1001) docker     (123)    16720 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/chinese_clip.py
--rw-r--r--   0 runner    (1001) docker     (123)     7132 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/
--rw-r--r--   0 runner    (1001) docker     (123)      163 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3658 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/adapter.py
--rw-r--r--   0 runner    (1001) docker     (123)    12648 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/flamingo.py
--rw-r--r--   0 runner    (1001) docker     (123)    13810 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/modules.py
--rw-r--r--   0 runner    (1001) docker     (123)     1968 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/
--rw-r--r--   0 runner    (1001) docker     (123)      204 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11897 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/ofa.py
--rw-r--r--   0 runner    (1001) docker     (123)    64644 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/ofa_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/
--rw-r--r--   0 runner    (1001) docker     (123)      958 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5945 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/beitv2_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)    10771 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/cae_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     2637 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/densecl_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     1504 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/gap.py
--rw-r--r--   0 runner    (1001) docker     (123)     1961 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/gem.py
--rw-r--r--   0 runner    (1001) docker     (123)     2984 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/hr_fuse.py
--rw-r--r--   0 runner    (1001) docker     (123)     3100 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/linear_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     6939 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/mae_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     8577 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/milan_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     3990 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/mixmim_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     1723 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/mocov2_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     4346 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/nonlinear_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)      971 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/simmim_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)     3143 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/necks/swav_neck.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/
--rw-r--r--   0 runner    (1001) docker     (123)      181 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5857 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/base.py
--rw-r--r--   0 runner    (1001) docker     (123)    12813 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/image2image.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/
--rw-r--r--   0 runner    (1001) docker     (123)     1175 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/barlowtwins.py
--rw-r--r--   0 runner    (1001) docker     (123)     7191 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/base.py
--rw-r--r--   0 runner    (1001) docker     (123)    14523 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/beit.py
--rw-r--r--   0 runner    (1001) docker     (123)     3339 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/byol.py
--rw-r--r--   0 runner    (1001) docker     (123)    18944 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/cae.py
--rw-r--r--   0 runner    (1001) docker     (123)     7932 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/densecl.py
--rw-r--r--   0 runner    (1001) docker     (123)     1304 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/eva.py
--rw-r--r--   0 runner    (1001) docker     (123)     9290 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mae.py
--rw-r--r--   0 runner    (1001) docker     (123)    13334 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/maskfeat.py
--rw-r--r--   0 runner    (1001) docker     (123)     7684 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/milan.py
--rw-r--r--   0 runner    (1001) docker     (123)    10167 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mixmim.py
--rw-r--r--   0 runner    (1001) docker     (123)     5102 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/moco.py
--rw-r--r--   0 runner    (1001) docker     (123)     8209 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mocov3.py
--rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simclr.py
--rw-r--r--   0 runner    (1001) docker     (123)     7398 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simmim.py
--rw-r--r--   0 runner    (1001) docker     (123)     1334 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simsiam.py
--rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/swav.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/tta/
--rw-r--r--   0 runner    (1001) docker     (123)      124 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/tta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/tta/score_tta.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/
--rw-r--r--   0 runner    (1001) docker     (123)     3291 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    45478 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/attention.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/
--rw-r--r--   0 runner    (1001) docker     (123)      239 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6451 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/cutmix.py
--rw-r--r--   0 runner    (1001) docker     (123)     2345 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/mixup.py
--rw-r--r--   0 runner    (1001) docker     (123)     3910 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/resizemix.py
--rw-r--r--   0 runner    (1001) docker     (123)     2598 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_shuffle.py
--rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      889 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/channel_shuffle.py
--rw-r--r--   0 runner    (1001) docker     (123)    13411 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/clip_generator_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    25992 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/data_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3351 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/ema.py
--rw-r--r--   0 runner    (1001) docker     (123)    15753 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/embed.py
--rw-r--r--   0 runner    (1001) docker     (123)     1518 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3255 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/huggingface.py
--rw-r--r--   0 runner    (1001) docker     (123)     4173 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/inverted_residual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/layer_scale.py
--rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/make_divisible.py
--rw-r--r--   0 runner    (1001) docker     (123)     4944 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/norm.py
--rw-r--r--   0 runner    (1001) docker     (123)     8826 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (123)     1019 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/res_layer_extra_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/se_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3121 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/swiglu_ffn.py
--rw-r--r--   0 runner    (1001) docker     (123)     6189 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     8614 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/models/utils/vector_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6908 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/structures/
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/structures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6011 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/structures/data_sample.py
--rw-r--r--   0 runner    (1001) docker     (123)      212 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/structures/multi_task_data_sample.py
--rw-r--r--   0 runner    (1001) docker     (123)     4930 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/structures/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      419 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/analyze.py
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/collect_env.py
--rw-r--r--   0 runner    (1001) docker     (123)     2375 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/dependency.py
--rw-r--r--   0 runner    (1001) docker     (123)      466 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/progress.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/utils/setup_env.py
--rw-r--r--   0 runner    (1001) docker     (123)      834 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      219 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/visualization/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    33454 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/mmpretrain/visualization/visualizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    20818 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    69771 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)      435 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       11 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/mmpretrain.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/requirements/
--rw-r--r--   0 runner    (1001) docker     (123)      216 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/docs.txt
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/mminstall.txt
--rw-r--r--   0 runner    (1001) docker     (123)       33 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/multimodal.txt
--rw-r--r--   0 runner    (1001) docker     (123)      238 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/optional.txt
--rw-r--r--   0 runner    (1001) docker     (123)      127 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/readthedocs.txt
--rw-r--r--   0 runner    (1001) docker     (123)       66 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/runtime.txt
--rw-r--r--   0 runner    (1001) docker     (123)       28 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/requirements/tests.txt
--rw-r--r--   0 runner    (1001) docker     (123)      591 2023-05-23 03:25:07.000000 mmpretrain-1.0.0rc8/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     7593 2023-05-23 03:25:03.000000 mmpretrain-1.0.0rc8/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/
+-rw-r--r--   0 runner    (1001) docker     (123)      212 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    22059 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    18269 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)     1114 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cifar100_bs16.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cifar10_bs16.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1897 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1937 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_okvqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3072 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2892 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_vg_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2280 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1241 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_448.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2375 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/flickr30k_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3195 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/flickr30k_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2031 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/gqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)      722 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet21k_bs128.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_mbv3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2063 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_medium_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2063 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_small_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2246 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_revvit_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2062 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_medium_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2062 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_small_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2057 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_vig_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1559 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_196.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1559 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_336.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1638 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_448.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1559 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_560.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1375 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_pil_bicubic_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1273 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_beitv2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_davit_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1417 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_itpn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2045 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_levit_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1861 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a12.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1861 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      900 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_simmim_192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2149 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_swin_192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2247 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_byol.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1506 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_mocov2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1500 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_bicubic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1376 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_resize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_simclr.py
+-rw-r--r--   0 runner    (1001) docker     (123)      832 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mae.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2276 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mocov3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_autoaug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_448.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2059 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_convmixer_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1500 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2060 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_edgenext_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2158 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_hivit_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1381 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_mixer_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1376 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1751 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize_autoaug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1397 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_t2t_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1487 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs8_pil_bicubic_320.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1595 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/inshop_bs32_448.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2249 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/nlvr2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/nocaps.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2040 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/ocrvqa.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)     2870 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/pipelines/auto_aug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/pipelines/rand_aug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2701 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/refcoco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/vizwiz.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1768 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/voc_bs16.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1952 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/vsr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/default_runtime.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      689 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/base-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      690 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/small-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      737 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/small-p32.py
+-rw-r--r--   0 runner    (1001) docker     (123)      688 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/tiny-p16.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convmixer/
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-1024-20.py
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-1536-20.py
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convmixer/convmixer-768-32.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/
+-rw-r--r--   0 runner    (1001) docker     (123)      563 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      564 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-large.py
+-rw-r--r--   0 runner    (1001) docker     (123)      563 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      562 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-tiny.py
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-xlarge.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/atto.py
+-rw-r--r--   0 runner    (1001) docker     (123)      621 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      504 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/femto.py
+-rw-r--r--   0 runner    (1001) docker     (123)      621 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/huge.py
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/large.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/nano.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/pico.py
+-rw-r--r--   0 runner    (1001) docker     (123)      620 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/davit/
+-rw-r--r--   0 runner    (1001) docker     (123)      495 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/davit/davit-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      495 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/davit/davit-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/davit/davit-tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/
+-rw-r--r--   0 runner    (1001) docker     (123)      663 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      664 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      665 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-huge-p14-224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      665 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      664 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      663 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-medium-p16-224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      664 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      663 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-384.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/densenet/
+-rw-r--r--   0 runner    (1001) docker     (123)      316 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/densenet/densenet121.py
+-rw-r--r--   0 runner    (1001) docker     (123)      316 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/densenet/densenet161.py
+-rw-r--r--   0 runner    (1001) docker     (123)      316 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/densenet/densenet169.py
+-rw-r--r--   0 runner    (1001) docker     (123)      316 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/densenet/densenet201.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      633 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      634 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xsmall.py
+-rw-r--r--   0 runner    (1001) docker     (123)      635 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xxsmall.py
+-rw-r--r--   0 runner    (1001) docker     (123)      598 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientformer-l1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b0.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b4.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b5.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b6.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b7.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_b8.py
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_em.py
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_es.py
+-rw-r--r--   0 runner    (1001) docker     (123)      340 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_l2.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b0.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_l.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_m.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_s.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_xl.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/eva/
+-rw-r--r--   0 runner    (1001) docker     (123)      806 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/eva/eva-g.py
+-rw-r--r--   0 runner    (1001) docker     (123)      838 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/eva/eva-l.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hivit/
+-rw-r--r--   0 runner    (1001) docker     (123)      816 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hivit/base_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hivit/small_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hivit/tiny_224.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/
+-rw-r--r--   0 runner    (1001) docker     (123)      738 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base-gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      752 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      756 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      753 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large.py
+-rw-r--r--   0 runner    (1001) docker     (123)      755 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small-gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      752 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      754 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny-gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w18.py
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w30.py
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w32.py
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w40.py
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w44.py
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w48.py
+-rw-r--r--   0 runner    (1001) docker     (123)      419 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hrnet/hrnet-w64.py
+-rw-r--r--   0 runner    (1001) docker     (123)      273 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/inception_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      884 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/itpn_hivit-base-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      668 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/levit-256-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      688 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mae_hivit-base-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      674 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mae_vit-base-p16.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mixmim/
+-rw-r--r--   0 runner    (1001) docker     (123)      653 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mixmim/mixmim_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      610 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mlp_mixer_base_patch16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      611 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mlp_mixer_large_patch16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v2_1x.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_large_imagenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      533 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_050_imagenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      533 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_075_imagenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_imagenet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s0.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobileone/mobileone_s4.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilevit/
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_s.py
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_xs.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilevit/mobilevit_xxs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      685 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-large.py
+-rw-r--r--   0 runner    (1001) docker     (123)      623 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m36.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m48.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s12.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s24.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s36.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_1.6gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_12gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_3.2gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_4.0gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_400mf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_6.4gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_8.0gf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/regnet/regnetx_800mf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/replknet-31B_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      370 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/replknet-31L_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      369 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/replknet-XL_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/repmlp-base_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/repvgg-A0_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      551 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/repvgg-B3_lbs-mixup_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      431 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net101-w26-s4.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net50-w14-s8.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s4.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s6.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net50-w26-s8.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/res2net50-w48-s2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      650 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest101.py
+-rw-r--r--   0 runner    (1001) docker     (123)      650 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest200.py
+-rw-r--r--   0 runner    (1001) docker     (123)      650 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest269.py
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet101.py
+-rw-r--r--   0 runner    (1001) docker     (123)      408 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet101_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet152.py
+-rw-r--r--   0 runner    (1001) docker     (123)      408 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet152_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      423 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet18.py
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet18_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      423 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet34.py
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet34_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet34_gem.py
+-rw-r--r--   0 runner    (1001) docker     (123)      424 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      407 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)      549 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (123)      487 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      538 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (123)      458 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_label_smooth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      484 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnetv1c50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnetv1d101.py
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnetv1d152.py
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnetv1d50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnext101_32x4d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnext101_32x8d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnext152_32x4d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      471 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnext50_32x4d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/revvit/
+-rw-r--r--   0 runner    (1001) docker     (123)      705 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/revvit/revvit-base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      705 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/revvit/revvit-small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/seresnet101.py
+-rw-r--r--   0 runner    (1001) docker     (123)      426 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/seresnet50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      495 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/seresnext101_32x4d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      494 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/seresnext50_32x4d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      338 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/shufflenet_v1_1x.py
+-rw-r--r--   0 runner    (1001) docker     (123)      347 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/shufflenet_v2_1x.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      458 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      376 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/large_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      459 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/large_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      775 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/small_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      766 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/tiny_224.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      431 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/large_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      431 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/large_384.py
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/small_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      792 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/tiny_256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-14.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-19.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-24.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/
+-rw-r--r--   0 runner    (1001) docker     (123)      694 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-11m.py
+-rw-r--r--   0 runner    (1001) docker     (123)      694 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-21m.py
+-rw-r--r--   0 runner    (1001) docker     (123)      693 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-5m.py
+-rw-r--r--   0 runner    (1001) docker     (123)      809 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tnt_s_patch16_224.py
+-rw-r--r--   0 runner    (1001) docker     (123)      906 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/twins_pcpvt_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      893 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/twins_svt_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/
+-rw-r--r--   0 runner    (1001) docker     (123)      456 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_large.py
+-rw-r--r--   0 runner    (1001) docker     (123)      732 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      731 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg11.py
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg11bn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg13.py
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg13bn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg16bn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg19.py
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vgg19bn.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/
+-rw-r--r--   0 runner    (1001) docker     (123)      818 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      819 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_medium.py
+-rw-r--r--   0 runner    (1001) docker     (123)      818 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (123)      848 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      849 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      848 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-base-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      592 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-base-p32.py
+-rw-r--r--   0 runner    (1001) docker     (123)      593 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-large-p16.py
+-rw-r--r--   0 runner    (1001) docker     (123)      593 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-large-p32.py
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/wide-resnet50.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/
+-rw-r--r--   0 runner    (1001) docker     (123)      493 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/cifar10_bs128.py
+-rw-r--r--   0 runner    (1001) docker     (123)      820 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/cub_bs64.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_hivit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1137 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_revvit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1134 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_swin.py
+-rw-r--r--   0 runner    (1001) docker     (123)      573 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_coslr.py
+-rw-r--r--   0 runner    (1001) docker     (123)      606 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_linearlr_bn_nowd.py
+-rw-r--r--   0 runner    (1001) docker     (123)      624 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1133 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_AdamW.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_adamw_levit.py
+-rw-r--r--   0 runner    (1001) docker     (123)      870 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_coslr.py
+-rw-r--r--   0 runner    (1001) docker     (123)      800 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_rsb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      497 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_140e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      804 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_200e_coslr_warmup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      496 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr.py
+-rw-r--r--   0 runner    (1001) docker     (123)      991 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr_coswd_300e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      479 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_epochstep.py
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs4096_AdamW.py
+-rw-r--r--   0 runner    (1001) docker     (123)      515 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_200e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_90e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      384 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_coslr_100e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      350 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_coslr_200e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      381 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_sgd_steplr_100e.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/arcface/
+-rw-r--r--   0 runner    (1001) docker     (123)      889 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/arcface/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1857 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/arcface/resnet50-arcface_8xb32_inshop.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/
+-rw-r--r--   0 runner    (1001) docker     (123)     1965 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-1000e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1963 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-300e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      459 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/benchmarks/resnet50_8xb32-linear-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1641 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/
+-rw-r--r--   0 runner    (1001) docker     (123)     3752 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/beit_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3594 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1144 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/
+-rw-r--r--   0 runner    (1001) docker     (123)     3432 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3607 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      935 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2602 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/
+-rw-r--r--   0 runner    (1001) docker     (123)     1592 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb16_refcoco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1555 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1560 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_caption_flickr30k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1528 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_nlvr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_nocaps.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2254 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_ocrvqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2258 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_okvqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2090 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2095 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval_flickr30k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3440 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/
+-rw-r--r--   0 runner    (1001) docker     (123)     2218 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_gqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2397 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb32_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2150 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2_8xb32_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1482 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50-c4_ms-1x_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)      911 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50_fpn_ms-1x_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1669 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/byol_resnet50_16xb256-coslr-200e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1573 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3601 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3138 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/cae_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1553 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/
+-rw-r--r--   0 runner    (1001) docker     (123)     1869 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_resnet50_zeroshot-cls_cifar100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2030 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-base-p16_zeroshot-cls_cifar100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1999 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-huge-p14_zeroshot-cls_cifar100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-large-p14_zeroshot-cls_cifar100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2928 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/
+-rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-448px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-448px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      789 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-large-p14_headless.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/conformer-base-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      250 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/conformer-small-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      250 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/conformer-small-p32_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/conformer-tiny-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3086 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/
+-rw-r--r--   0 runner    (1001) docker     (123)      988 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-1024-20_10xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      988 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-1536-20_10xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-768-32_10xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/
+-rw-r--r--   0 runner    (1001) docker     (123)      668 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      670 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      667 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      653 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      655 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      668 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      668 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15777 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-atto_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      660 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-femto_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1493 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-512px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      896 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      896 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-pico_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18532 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspdarknet50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspresnet50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspresnext50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2520 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/csra/
+-rw-r--r--   0 runner    (1001) docker     (123)      961 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/csra/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2558 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/csra/resnet101-csra_1xb16_voc07-448px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/
+-rw-r--r--   0 runner    (1001) docker     (123)      260 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/davit-base_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/davit-small_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      260 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/davit-tiny_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/
+-rw-r--r--   0 runner    (1001) docker     (123)      995 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1177 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      999 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base_16xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1176 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-small-distilled_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1273 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-small_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-tiny-distilled_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-tiny_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6153 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-huge-p14_64xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb16_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      546 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-medium-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11683 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      591 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/benchmarks/resnet50_8xb32-linear-steplr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1010 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/densecl_resnet50_8xb32-coslr-200e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1594 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/
+-rw-r--r--   0 runner    (1001) docker     (123)      509 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/densenet121_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      509 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/densenet161_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      509 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/densenet169_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      509 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/densenet201_4xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2856 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/
+-rw-r--r--   0 runner    (1001) docker     (123)     2626 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      445 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/vit-base-p14_dinov2-pre_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/vit-giant-p14_dinov2-pre_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      446 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/vit-large-p14_dinov2-pre_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      453 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/vit-small-p14_dinov2-pre_headless.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256-usi_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      613 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      426 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256-usi_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      615 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-xsmall_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      616 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-xxsmall_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/efficientformer-l1_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      115 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/efficientformer-l3_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      115 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/efficientformer-l7_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2687 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-em_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-es_8xb32-01norm_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb32_in1k-475px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      770 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb8_in1k-800px.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25083 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1635 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b0_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b1_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b3_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in1k-480px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in1k-480px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1012 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      676 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in1k-512px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      105 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11840 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3232 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1885 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p14_8xb16_in1k-336px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p14_8xb16_in1k-560px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p16_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-l-p14_8xb16_in1k-196px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-l-p14_8xb16_in1k-336px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      644 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-l-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2378 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-mae-style_vit-base-p16_16xb256-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9809 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/
+-rw-r--r--   0 runner    (1001) docker     (123)      513 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-base-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      858 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-base-p14_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      513 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-large-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      859 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-large-p14_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-small-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      837 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-small-p14_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      837 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8019 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/
+-rw-r--r--   0 runner    (1001) docker     (123)     2797 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2716 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3239 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1494 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/glip/
+-rw-r--r--   0 runner    (1001) docker     (123)      487 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/glip/glip-l_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      434 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/glip/glip-t_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1604 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/glip/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hivit/
+-rw-r--r--   0 runner    (1001) docker     (123)      273 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hivit/hivit-base-p16_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      274 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hivit/hivit-small-p16_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      273 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hivit/hivit-tiny-p16_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hivit/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-base-gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      393 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-base_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      397 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-small-gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      394 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-small_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      397 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-tiny-gf_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/hornet-tiny_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4401 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w18_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w30_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w32_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w40_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w44_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w48_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/hrnet-w64_4xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/inception_v3/
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/inception_v3/inception-v3_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1281 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/inception_v3/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2233 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-clip-b_hivit-base-p16_8xb256-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2234 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-clip-b_hivit-base-p16_8xb256-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1438 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1435 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1435 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1577 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1350 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/lenet/
+-rw-r--r--   0 runner    (1001) docker     (123)     2535 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/lenet/lenet5_mnist.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/
+-rw-r--r--   0 runner    (1001) docker     (123)      105 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/levit-128_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/levit-128s_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      105 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/levit-192_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      105 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/levit-256_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      105 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/deploy/levit-384_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/levit-128_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      347 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/levit-128s_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/levit-192_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      260 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/levit-256_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      377 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/levit-384_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3633 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/llava/
+-rw-r--r--   0 runner    (1001) docker     (123)     2454 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/llava/llava-7b-v1_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)      541 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/llava/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1741 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb2048-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_32xb8-coslr-50e_in1k-448px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3321 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      795 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-ds-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      374 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-fsdp-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      796 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-ds-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      375 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-fsdp-coslr-50e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb2048-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1437 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1434 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1434 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1578 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1450 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1445 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1452 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1692 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-huge-p14_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1584 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1579 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-400e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13047 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3335 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/benchmarks/vit-base-p16_8xb256-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2854 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/maskfeat_vit-base-p16_8xb256-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3232 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1885 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2394 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/milan_vit-base-p16_16xb256-amp-coslr-400e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/minigpt4/
+-rw-r--r--   0 runner    (1001) docker     (123)      871 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/minigpt4/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2492 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/minigpt4/minigpt-4_vicuna-7b_caption.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb128-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1828 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2556 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/mixmim_mixmim-base_16xb128-coslr-300e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mlp_mixer/
+-rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mlp_mixer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      256 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-base-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      257 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-large-p16_64xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      200 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v2/mobilenet-v2_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/
+-rw-r--r--   0 runner    (1001) docker     (123)     4421 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-large_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1986 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-050_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-075_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      376 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb16_cifar10.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s0_deploy_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s1_deploy_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s2_deploy_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s3_deploy_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/deploy/mobileone-s4_deploy_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2834 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      513 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s0_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1891 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s1_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s3_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1986 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s4_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/
+-rw-r--r--   0 runner    (1001) docker     (123)     2302 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      729 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-xsmall_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      730 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-xxsmall_8xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      591 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/benchmarks/resnet50_8xb32-linear-steplr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/mocov2_resnet50_8xb32-coslr-200e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      861 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/resnet50_8xb128-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb128-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb64-coslr-150e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1992 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-large-p16_8xb64-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-small-p16_8xb128-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8391 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2305 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3934 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-base-p16_16xb256-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4023 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-large-p16_64xb64-amp-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3942 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-small-p16_16xb256-amp-coslr-300e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/
+-rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-base_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-large_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-small_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1197 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-tiny_8xb256_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/
+-rw-r--r--   0 runner    (1001) docker     (123)     2932 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_refcoco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1532 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)      881 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_zeroshot_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)      924 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-large_zeroshot_vqa.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/otter/
+-rw-r--r--   0 runner    (1001) docker     (123)     1355 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/otter/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2399 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/otter/otter-9b_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2975 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/otter/otter-9b_vqa.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/
+-rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      530 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-m36_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      530 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-m48_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s12_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s24_32xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s36_32xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     4244 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-1.6gf_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      520 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-12gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-3.2gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-4.0gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1671 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-400mf_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-6.4gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-8.0gf_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-800mf_8xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/deploy/
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/deploy/replknet-31B-deploy_32xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/deploy/replknet-31L-deploy_32xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      102 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/deploy/replknet-XL-deploy_32xb64_in1k-320px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4867 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      371 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/replknet-31B_32xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/replknet-31B_32xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      371 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/replknet-31L_32xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      369 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/replknet-XL_32xb64_in1k-320px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/
+-rw-r--r--   0 runner    (1001) docker     (123)     1871 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1126 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      879 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       83 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_delopy_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_deploy_8xb64_in1k-256px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/
+-rw-r--r--   0 runner    (1001) docker     (123)     5987 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      876 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-A0_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       79 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-A0_deploy_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       77 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-A1_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-A2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B0_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B1_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B1g2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B1g4_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B2_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B2g4_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B3_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       79 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B3g4_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-D2se_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/
+-rw-r--r--   0 runner    (1001) docker     (123)     2614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/res2net101-w26-s4_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/res2net50-w14-s8_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/res2net50-w26-s8_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/
+-rw-r--r--   0 runner    (1001) docker     (123)     2343 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/_randaug_policies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest101_32xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest200_64xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest269_64xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2083 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest50_32xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/
+-rw-r--r--   0 runner    (1001) docker     (123)    11688 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      173 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet101_8xb16_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet101_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      173 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet152_8xb16_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet152_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      168 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet18_8xb16_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      164 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet18_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      168 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet34_8xb16_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      164 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet34_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      175 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup-coslr_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      313 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup-lbs_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_32xb64-warmup_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      307 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb128_coslr-90e_in21k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      178 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb16-mixup_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      168 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb16_cifar10.py
+-rw-r--r--   0 runner    (1001) docker     (123)      432 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb16_cifar100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1339 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a1-600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a2-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      587 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a3-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      401 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-coslr-preciseBN_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      174 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-coslr_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      175 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-cutmix_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      126 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-fp16-dynamic_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-fp16_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      181 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-lbs_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      174 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32-mixup_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      164 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      684 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb8_cub.py
+-rw-r--r--   0 runner    (1001) docker     (123)      222 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1c101_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      222 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1c152_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1c50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1d101_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1d152_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnetv1d50_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/
+-rw-r--r--   0 runner    (1001) docker     (123)     2559 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/resnext101-32x4d_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/resnext101-32x8d_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/resnext152-32x4d_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      186 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/resnext50-32x4d_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/revvit/
+-rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/revvit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      208 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/revvit/revvit-base_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/revvit/revvit-small_8xb256_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-m36-deploy_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-m36-deploy_8xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-m48-deploy_8xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       83 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-m48-deploy_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       90 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s12-deploy_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s12-deploy_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       90 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s24-deploy_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s24-deploy_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s36-deploy_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/deploy/riformer-s36-deploy_8xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5225 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m36_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m36_8xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s36_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s36_8xb64_in1k-384px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/
+-rw-r--r--   0 runner    (1001) docker     (123)     2097 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      502 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/vit-base-p16_sam_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      502 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/vit-huge-p16_sam_headless.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/vit-large-p16_sam_headless.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1565 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/seresnet101_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      190 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/seresnet50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      189 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/seresnext101-32x4d_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/seresnext50-32x4d_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v1/
+-rw-r--r--   0 runner    (1001) docker     (123)     1220 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v1/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v1/shufflenet-v1-1x_16xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v2/shufflenet-v2-1x_16xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1308 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-200e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_8xb32-coslr-200e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1656 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w6_8xb256-coslr-100e_in1k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2866 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w7_8xb256-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-large-w14_8xb256-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4633 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-100e_in1k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1768 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-800e_in1k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_8xb256-amp-coslr-100e_in1k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1798 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-large-w12_16xb128-amp-coslr-800e_in1k-192px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2766 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-100e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-200e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)     3191 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/benchmarks/convnextv2-tiny_8xb256-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2913 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/benchmarks/resnet50_8xb256-coslr-300e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2748 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/spark_sparse-convnext-small_16xb256-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/spark_sparse-convnextv2-tiny_16xb256-amp-coslr-800e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/spark_sparse-resnet50_8xb512-amp-coslr-1600e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2062 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/spark_sparse-resnet50_8xb512-amp-coslr-800e_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1617 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4124 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/swav_resnet50_8xb32-mcrop-coslr-200e_in1k-224px-96px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)     8957 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-large_16xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-large_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-large_8xb8_cub-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-small_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-tiny_16xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)     9205 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      584 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w12_8xb128_in21k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      271 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w16_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w16_in21k-pre_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      413 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w24_in21k-pre_16xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      213 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w8_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      584 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w12_8xb128_in21k-192px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      379 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w16_in21k-pre_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      410 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w24_in21k-pre_16xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      272 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-small-w16_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      214 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-small-w8_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      271 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-tiny-w16_16xb64_in1k-256px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      213 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-tiny-w8_16xb64_in1k-256px.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/
+-rw-r--r--   0 runner    (1001) docker     (123)     1983 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-14_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-19_8xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-24_8xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/
+-rw-r--r--   0 runner    (1001) docker     (123)     6452 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       49 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-11m-distill_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      208 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-11m_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      677 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      691 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-512px.py
+-rw-r--r--   0 runner    (1001) docker     (123)       49 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      208 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)       48 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-5m-distill_8xb256_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      207 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-5m_8xb256_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tnt/
+-rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tnt/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1538 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/tnt/tnt-s-p16_16xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/
+-rw-r--r--   0 runner    (1001) docker     (123)     5169 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-pcpvt-base_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-pcpvt-large_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-pcpvt-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-svt-base_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      191 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-svt-large_16xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      131 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-svt-small_8xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/
+-rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-base_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-large_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-tiny_8xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/
+-rw-r--r--   0 runner    (1001) docker     (123)     4063 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg11_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg11bn_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg13_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg13bn_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg16_8xb16_voc.py
+-rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg16_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg16bn_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg19_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg19bn_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/
+-rw-r--r--   0 runner    (1001) docker     (123)     5221 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      603 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/pvig-base_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/pvig-medium_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      195 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/pvig-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      194 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/pvig-tiny_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      186 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/vig-base_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/vig-small_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      186 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/vig-tiny_8xb128_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)     4067 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_32xb128-mae_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3402 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_4xb544-ipu_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      398 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2238 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_8xb64-lora_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      398 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      399 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      399 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2876 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/wide-resnet101_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/wide-resnet50_8xb32_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      185 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/wide-resnet50_timm_8xb32_in1k.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/
+-rw-r--r--   0 runner    (1001) docker     (123)    28291 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      815 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      815 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      812 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k-384px.py
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k.py
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain/.mim/dataset-index.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2751 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain/.mim/model-index.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/
+-rw-r--r--   0 runner    (1001) docker     (123)     7189 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/analyze_logs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4037 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/analyze_results.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3750 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1891 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/eval_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/get_flops.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9284 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/shape_bias.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9055 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_c4.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      366 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_fpn.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      796 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_c4.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_fpn.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      394 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      642 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_train.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/
+-rw-r--r--   0 runner    (1001) docker     (123)     1764 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/convert_flickr30k_ann.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1524 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/convert_imagenet_subsets.py
+-rw-r--r--   0 runner    (1001) docker     (123)      924 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/convert_inaturalist.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      301 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/odl_cub_preprocess.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      596 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/odl_imagenet1k_preprocess.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      479 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dist_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      442 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/dist_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     8957 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/kfold-cross-valid.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/misc/
+-rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/misc/print_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4875 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/misc/verify_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/
+-rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/clip_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1725 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/convnext_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3194 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/davit_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/deit3_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/edgenext_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8753 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/efficientnet_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/efficientnetv2_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/eva02_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/eva_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2400 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/glip_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/hornet2mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/levit2mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2172 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/llava-delta2mmpre.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2772 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/merge_lora_weight.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2748 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mixmim_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1684 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mlpmixer_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4737 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mobilenetv2_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/ofa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2108 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/otter2mmpre.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/publish_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1935 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/reparameterize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/replknet_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/repvgg_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3099 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/revvit_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4142 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/shufflenetv2_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1703 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/tinyvit_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1838 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/torchvision_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2180 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/twins2mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1895 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/van2mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4093 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/vgg_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3360 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/vig_to_mmpretrain.py
+-rw-r--r--   0 runner    (1001) docker     (123)      566 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/slurm_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      574 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/slurm_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     6982 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/
+-rw-r--r--   0 runner    (1001) docker     (123)     3842 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/mmpretrain2torchserve.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2420 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/mmpretrain_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1422 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/test_torchserver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5672 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/train.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)     9156 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/browse_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10342 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_cam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9020 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9910 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_tsne.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1032 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/apis/
+-rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14927 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4715 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/feature_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6323 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/image_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8712 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/image_classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10988 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/image_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15940 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23773 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/multimodal_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5737 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/nlvr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10844 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7114 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/visual_grounding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7280 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/apis/visual_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)     2220 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8037 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/base_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)      844 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3772 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/caltech101.py
+-rw-r--r--   0 runner    (1001) docker     (123)    58132 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/categories.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7876 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/coco_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/coco_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/coco_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5191 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/cub.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10408 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/custom.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5461 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/dataset_wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3664 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/dtd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/fgvcaircraft.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10694 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/flamingo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2623 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/flickr30k_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3745 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/flickr30k_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3500 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/flowers102.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/food101.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2388 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/gqa_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8071 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/imagenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/inshop.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8650 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/mnist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3227 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/multi_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11053 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/multi_task.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1092 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/nlvr2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1443 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/nocaps.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3072 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/ocr_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/oxfordiiitpet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1452 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/places205.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2600 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/refcoco.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/datasets/samplers/
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3845 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/samplers/repeat_aug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2207 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/samplers/sequential.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3643 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/scienceqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5418 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/stanfordcars.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/sun397.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/textvqa.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/
+-rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    48888 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/auto_augment.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11347 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/formatting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    62820 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/processing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1579 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5280 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/transforms/wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7667 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3002 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/vg_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/visual_genome.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3929 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/vizwiz.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7402 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/voc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1705 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/datasets/vsr.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/engine/
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      780 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/class_num_check_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1556 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/densecl_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8632 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/ema_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2266 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/margin_head_hooks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8774 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/precise_bn_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/retriever_hooks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1657 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/simsiam_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4914 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/swav_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6732 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/switch_recipe_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5026 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/visualization_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2314 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/hooks/warmup_param_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      297 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11336 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/adan_t.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9660 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/lamb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4758 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/lars.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7409 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/optimizers/layer_decay_optim_wrapper_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/engine/runners/
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6547 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/runners/retrieval_loop.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/engine/schedulers/
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/schedulers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/engine/schedulers/weight_decay_scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/evaluation/functional/
+-rw-r--r--   0 runner    (1001) docker     (123)       48 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/functional/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      986 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4497 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2788 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/gqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24925 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/multi_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4826 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/multi_task.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1936 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/nocaps.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18798 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6755 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/scienceqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6827 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/shape_bias_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31789 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/single_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2915 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/visual_grounding_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4184 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/voc_multi_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10435 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/evaluation/metrics/vqa.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/
+-rw-r--r--   0 runner    (1001) docker     (123)      848 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (123)     3159 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/alexnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      957 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/base_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28643 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/beit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22645 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6192 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/convmixer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15099 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25507 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/cspnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30645 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/davit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4788 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/deit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17185 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/deit3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12016 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/densenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15475 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/edgenext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22042 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/efficientformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15726 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/efficientnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16382 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/efficientnet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24628 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/hivit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18959 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/hornet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23396 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/hrnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18806 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/inception_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/lenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18201 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/levit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19834 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mixmim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9664 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mlp_mixer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9600 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mobilenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8780 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mobilenet_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18955 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mobileone.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17053 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mobilevit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26094 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/mvit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14706 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/poolformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11898 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/regnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25324 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/replknet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22849 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/repmlp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22087 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/repvgg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11237 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/res2net.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12235 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/resnest.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26552 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3711 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/resnet_cifar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6260 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/resnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24265 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/revvit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14049 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/riformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/seresnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/seresnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11596 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/shufflenet_v1.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10783 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/shufflenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11733 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/sparse_convnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7444 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/sparse_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23638 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23060 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/swin_transformer_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16685 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/t2t_vit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4213 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/timm_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26505 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/tinyvit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14530 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/tnt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30218 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/twins.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15649 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/van.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6756 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/vgg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31677 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/vig.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20364 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/vision_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12783 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/vit_eva02.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27046 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/vit_sam.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29245 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/backbones/xcit.py
+-rw-r--r--   0 runner    (1001) docker     (123)      676 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/
+-rw-r--r--   0 runner    (1001) docker     (123)      299 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4332 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/hugging_face.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10957 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/image.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8104 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/classifiers/timm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/heads/
+-rw-r--r--   0 runner    (1001) docker     (123)     2110 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1685 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/beitv1_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1861 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/beitv2_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2430 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/cae_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6046 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/cls_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4579 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/conformer_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1618 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/contrastive_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3023 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/deit_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/efficientformer_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8046 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/grounding_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6310 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/itc_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/itm_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1736 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/itpn_clip_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3077 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/latent_heads.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/levit_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2344 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/linear_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/mae_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11382 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/margin_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1003 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/mim_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1396 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/mixmim_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/mocov3_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5919 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_cls_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4117 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_csra_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_linear_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5730 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/multi_task_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6720 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/seq_gen_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1234 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/simmim_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3073 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/spark_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4538 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/stacked_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)      711 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/swav_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2265 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/vig_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/vision_transformer_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9757 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/heads/vqa_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/losses/
+-rw-r--r--   0 runner    (1001) docker     (123)     1135 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5535 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/asymmetric_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1496 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/cae_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/cosine_similarity_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1337 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/cross_correlation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7602 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/cross_entropy_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4285 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/focal_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7173 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/label_smooth_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2393 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/reconstruction_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6733 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/seesaw_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/swav_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3694 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/losses/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/
+-rw-r--r--   0 runner    (1001) docker     (123)      877 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/
+-rw-r--r--   0 runner    (1001) docker     (123)      461 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7055 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9274 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_grounding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7761 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_nlvr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28732 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10422 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52002 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/language_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/
+-rw-r--r--   0 runner    (1001) docker     (123)    32118 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/Qformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      331 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8458 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_caption.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_opt_vqa.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21891 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)    48175 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/modeling_opt.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10497 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/bert.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16720 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/chinese_clip.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7132 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3857 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/adapter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12697 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/flamingo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13810 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/modules.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1968 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/llava/
+-rw-r--r--   0 runner    (1001) docker     (123)      162 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/llava/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10149 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/llava/llava.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10259 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/llava/modules.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/minigpt4/
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/minigpt4/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15458 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/minigpt4/minigpt4.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11897 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/ofa.py
+-rw-r--r--   0 runner    (1001) docker     (123)    64644 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/ofa_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/otter/
+-rw-r--r--   0 runner    (1001) docker     (123)       94 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/otter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5903 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/multimodal/otter/otter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1095 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5945 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/beitv2_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10771 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/cae_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2637 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/densecl_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1504 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/gap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1961 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/gem.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2984 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/hr_fuse.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14481 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/itpn_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3100 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/linear_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6939 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/mae_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8577 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/milan_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3990 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/mixmim_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1723 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/mocov2_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4346 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/nonlinear_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)      971 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/simmim_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5967 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/spark_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3143 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/necks/swav_neck.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/peft/
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/peft/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7471 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/peft/lora.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/retrievers/
+-rw-r--r--   0 runner    (1001) docker     (123)      181 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/retrievers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5857 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/retrievers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12813 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/retrievers/image2image.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/
+-rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/barlowtwins.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7191 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14523 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/beit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3339 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/byol.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18944 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/cae.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7932 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/densecl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1304 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/eva.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13309 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/itpn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16222 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/mae.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13334 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/maskfeat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7684 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/milan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10167 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/mixmim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5102 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/moco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8209 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/mocov3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/simclr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7399 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/simmim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1334 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/simsiam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6079 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/spark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/selfsup/swav.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/tta/
+-rw-r--r--   0 runner    (1001) docker     (123)      124 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/tta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/tta/score_tta.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     3670 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45478 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/attention.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/
+-rw-r--r--   0 runner    (1001) docker     (123)      239 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6451 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2345 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/mixup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3910 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/resizemix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2598 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/batch_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      889 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/channel_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13411 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/clip_generator_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25992 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/data_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3351 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/ema.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15753 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/embed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1518 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3325 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/huggingface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4173 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/inverted_residual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/layer_scale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/make_divisible.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4944 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8826 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1019 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/res_layer_extra_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/se_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5428 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/sparse_modules.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3121 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/swiglu_ffn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6189 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/models/utils/vector_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6908 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/structures/
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/structures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6011 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/structures/data_sample.py
+-rw-r--r--   0 runner    (1001) docker     (123)      212 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/structures/multi_task_data_sample.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4930 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/structures/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      419 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1305 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/analyze.py
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/collect_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2375 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/dependency.py
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/progress.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/utils/setup_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      219 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2061 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/visualization/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33454 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/mmpretrain/visualization/visualizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    22059 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    75582 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/mmpretrain.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      432 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       11 2023-07-31 09:10:47.000000 mmpretrain-1.0.1/mmpretrain.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/requirements/
+-rw-r--r--   0 runner    (1001) docker     (123)      216 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/docs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       42 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/mminstall.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       33 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/multimodal.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      238 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/optional.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      127 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/readthedocs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       66 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/runtime.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       28 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/requirements/tests.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      629 2023-07-31 09:10:48.000000 mmpretrain-1.0.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     7614 2023-07-31 09:10:42.000000 mmpretrain-1.0.1/setup.py
```

### Comparing `mmpretrain-1.0.0rc8/PKG-INFO` & `mmpretrain-1.0.1/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmpretrain
-Version: 1.0.0rc8
+Version: 1.0.1
 Summary: OpenMMLab Model Pretraining Toolbox and Benchmark
 Home-page: https://github.com/open-mmlab/mmpretrain
 Author: MMPretrain Contributors
 Author-email: openmmlab@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
         
@@ -90,36 +90,37 @@
           - Visual Grounding
           - Retrieval (Image-To-Image, Text-To-Image, Image-To-Text)
         
         https://github.com/open-mmlab/mmpretrain/assets/26739999/e4dcd3a2-f895-4d1b-a351-fbc74a04e904
         
         ## What's new
         
+         v1.0.1 was released in 28/07/2023
+        
+        Fix some bugs and enhance the codebase. Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details.
+        
+         v1.0.0 was released in 04/07/2023
+        
+        - Support inference of more **multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc.
+        - Support around **10 multi-modal** datasets!
+        - Add [**iTPN**](./configs/itpn/), [**SparK**](./configs/spark/) self-supervised learning algorithms.
+        - Provide examples of [New Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./configs/mae/benchmarks/). Here are the documentation links of [New Config](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner).
+        
          v1.0.0rc8 was released in 22/05/2023
         
         - Support multiple **multi-modal** algorithms and inferencers. You can explore these features by the [gradio demo](https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)!
         - Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones.
         - Register torchvision transforms into MMPretrain, you can now easily integrate torchvision's data augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/en/latest/api/data_process.html#torchvision-transforms)
         
-         v1.0.0rc7 was released in 07/04/2023
+        Update of previous versions
         
         - Integrated Self-supervised learning algorithms from **MMSelfSup**, such as **MAE**, **BEiT**, etc.
         - Support **RIFormer**, a simple but effective vision backbone by removing token mixer.
-        - Add t-SNE visualization.
         - Refactor dataset pipeline visualization.
-        
-        Update of previous versions
-        
         - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones.
-        - Reproduce the training accuracy of **ConvNeXt** and **RepVGG**.
-        - Support confusion matrix calculation and plot.
-        - Support **multi-task** training and testing.
-        - Support Test-time Augmentation.
-        - Upgrade API to get pre-defined models of MMPreTrain.
-        - Refactor BEiT backbone and support v1/v2 inference.
         
         This release introduced a brand new and flexible training & test engine, but it's still in progress. Welcome
         to try according to [the documentation](https://mmpretrain.readthedocs.io/en/latest/).
         
         And there are some BC-breaking changes. Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/latest/migration.html).
         
         Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and other release history.
@@ -228,14 +229,18 @@
                 <li><a href="configs/revvit">RevViT</a></li>
                 <li><a href="configs/convnext_v2">ConvNeXt V2</a></li>
                 <li><a href="configs/vig">ViG</a></li>
                 <li><a href="configs/xcit">XCiT</a></li>
                 <li><a href="configs/levit">LeViT</a></li>
                 <li><a href="configs/riformer">RIFormer</a></li>
                 <li><a href="configs/glip">GLIP</a></li>
+                <li><a href="configs/sam">ViT SAM</a></li>
+                <li><a href="configs/eva02">EVA02</a></li>
+                <li><a href="configs/dinov2">DINO V2</a></li>
+                <li><a href="configs/hivit">HiViT</a></li>
                 </ul>
               </td>
               <td>
                 <ul>
                 <li><a href="configs/mocov2">MoCo V1 (CVPR'2020)</a></li>
                 <li><a href="configs/simclr">SimCLR (ICML'2020)</a></li>
                 <li><a href="configs/mocov2">MoCo V2 (arXiv'2020)</a></li>
@@ -250,23 +255,28 @@
                 <li><a href="configs/simmim">SimMIM (CVPR'2022)</a></li>
                 <li><a href="configs/maskfeat">MaskFeat (CVPR'2022)</a></li>
                 <li><a href="configs/cae">CAE (arXiv'2022)</a></li>
                 <li><a href="configs/milan">MILAN (arXiv'2022)</a></li>
                 <li><a href="configs/beitv2">BEiT V2 (arXiv'2022)</a></li>
                 <li><a href="configs/eva">EVA (CVPR'2023)</a></li>
                 <li><a href="configs/mixmim">MixMIM (arXiv'2022)</a></li>
+                <li><a href="configs/itpn">iTPN (CVPR'2023)</a></li>
+                <li><a href="configs/spark">SparK (ICLR'2023)</a></li>
                 </ul>
               </td>
               <td>
                 <ul>
                 <li><a href="configs/blip">BLIP (arxiv'2022)</a></li>
                 <li><a href="configs/blip2">BLIP-2 (arxiv'2023)</a></li>
                 <li><a href="configs/ofa">OFA (CoRR'2022)</a></li>
                 <li><a href="configs/flamingo">Flamingo (NeurIPS'2022)</a></li>
                 <li><a href="configs/chinese_clip">Chinese CLIP (arxiv'2022)</a></li>
+                <li><a href="configs/minigpt4">MiniGPT-4 (arxiv'2023)</a></li>
+                <li><a href="configs/llava">LLaVA (arxiv'2023)</a></li>
+                <li><a href="configs/otter">Otter (arxiv'2023)</a></li>
                 </ul>
               </td>
               <td>
               Image Retrieval Task:
                 <ul>
                 <li><a href="configs/arcface">ArcFace (CVPR'2019)</a></li>
                 </ul>
```

#### html2text {}

```diff
@@ -1,11 +1,11 @@
-Metadata-Version: 2.1 Name: mmpretrain Version: 1.0.0rc8 Summary: OpenMMLab
-Model Pretraining Toolbox and Benchmark Home-page: https://github.com/open-
-mmlab/mmpretrain Author: MMPretrain Contributors Author-email:
-openmmlab@gmail.com License: Apache License 2.0 Description:
+Metadata-Version: 2.1 Name: mmpretrain Version: 1.0.1 Summary: OpenMMLab Model
+Pretraining Toolbox and Benchmark Home-page: https://github.com/open-mmlab/
+mmpretrain Author: MMPretrain Contributors Author-email: openmmlab@gmail.com
+License: Apache License 2.0 Description:
                            [resources/mmpt-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
  [![PyPI](https://img.shields.io/pypi/v/mmpretrain)](https://pypi.org/project/
  mmpretrain) [![Docs](https://img.shields.io/badge/docs-latest-blue)](https://
 mmpretrain.readthedocs.io/en/latest/) [![Build Status](https://github.com/open-
@@ -33,50 +33,58 @@
 backbones and pretrained models - Rich training strategies (supervised
 learning, self-supervised learning, multi-modality learning etc.) - Bag of
 training tricks - Large-scale training configs - High efficiency and
 extensibility - Powerful toolkits for model analysis and experiments - Various
 out-of-box inference tasks. - Image Classification - Image Caption - Visual
 Question Answering - Visual Grounding - Retrieval (Image-To-Image, Text-To-
 Image, Image-To-Text) https://github.com/open-mmlab/mmpretrain/assets/26739999/
-e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.0rc8 was released
-in 22/05/2023 - Support multiple **multi-modal** algorithms and inferencers.
-You can explore these features by the [gradio demo](https://github.com/open-
-mmlab/mmpretrain/tree/main/projects/gradio_demo)! - Add EVA-02, Dino-V2, ViT-
-SAM and GLIP backbones. - Register torchvision transforms into MMPretrain, you
-can now easily integrate torchvision's data augmentations in MMPretrain. See
-[the doc](https://mmpretrain.readthedocs.io/en/latest/api/
-data_process.html#torchvision-transforms)  v1.0.0rc7 was released in 07/04/
-2023 - Integrated Self-supervised learning algorithms from **MMSelfSup**, such
-as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
-vision backbone by removing token mixer. - Add t-SNE visualization. - Refactor
-dataset pipeline visualization. Update of previous versions - Support
-**LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**,
-**EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones. - Reproduce
-the training accuracy of **ConvNeXt** and **RepVGG**. - Support confusion
-matrix calculation and plot. - Support **multi-task** training and testing. -
-Support Test-time Augmentation. - Upgrade API to get pre-defined models of
-MMPreTrain. - Refactor BEiT backbone and support v1/v2 inference. This release
-introduced a brand new and flexible training & test engine, but it's still in
-progress. Welcome to try according to [the documentation](https://
-mmpretrain.readthedocs.io/en/latest/). And there are some BC-breaking changes.
-Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/
-latest/migration.html). Please refer to [changelog](https://
-mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and
-other release history. ## Installation Below are quick steps for installation:
-```shell conda create -n open-mmlab python=3.8 pytorch==1.10.1
-torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda activate open-mmlab
-pip install openmim git clone https://github.com/open-mmlab/mmpretrain.git cd
-mmpretrain mim install -e . ``` Please refer to [installation documentation]
-(https://mmpretrain.readthedocs.io/en/latest/get_started.html) for more
-detailed installation and dataset preparation. For multi-modality models
-support, please install the extra dependencies by: ```shell mim install -e ".
-[multimodal]" ``` ## User Guides We provided a series of tutorials about the
-basic usage of MMPreTrain for new users: - [Learn about Configs](https://
-mmpretrain.readthedocs.io/en/latest/user_guides/config.html) - [Prepare
-Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
+e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.1 was released in
+28/07/2023 Fix some bugs and enhance the codebase. Please refer to [changelog]
+(https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more
+details.  v1.0.0 was released in 04/07/2023 - Support inference of more
+**multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-
+4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc. - Support around
+**10 multi-modal** datasets! - Add [**iTPN**](./configs/itpn/), [**SparK**](./
+configs/spark/) self-supervised learning algorithms. - Provide examples of [New
+Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./
+configs/mae/benchmarks/). Here are the documentation links of [New Config]
+(https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-
+pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with
+FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/
+mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner). 
+v1.0.0rc8 was released in 22/05/2023 - Support multiple **multi-modal**
+algorithms and inferencers. You can explore these features by the [gradio demo]
+(https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)! -
+Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones. - Register torchvision
+transforms into MMPretrain, you can now easily integrate torchvision's data
+augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/
+en/latest/api/data_process.html#torchvision-transforms) Update of previous
+versions - Integrated Self-supervised learning algorithms from **MMSelfSup**,
+such as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
+vision backbone by removing token mixer. - Refactor dataset pipeline
+visualization. - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**,
+**EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM**
+backbones. This release introduced a brand new and flexible training & test
+engine, but it's still in progress. Welcome to try according to [the
+documentation](https://mmpretrain.readthedocs.io/en/latest/). And there are
+some BC-breaking changes. Please check [the migration tutorial](https://
+mmpretrain.readthedocs.io/en/latest/migration.html). Please refer to
+[changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html)
+for more details and other release history. ## Installation Below are quick
+steps for installation: ```shell conda create -n open-mmlab python=3.8
+pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda
+activate open-mmlab pip install openmim git clone https://github.com/open-
+mmlab/mmpretrain.git cd mmpretrain mim install -e . ``` Please refer to
+[installation documentation](https://mmpretrain.readthedocs.io/en/latest/
+get_started.html) for more detailed installation and dataset preparation. For
+multi-modality models support, please install the extra dependencies by:
+```shell mim install -e ".[multimodal]" ``` ## User Guides We provided a series
+of tutorials about the basic usage of MMPreTrain for new users: - [Learn about
+Configs](https://mmpretrain.readthedocs.io/en/latest/user_guides/config.html) -
+[Prepare Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 dataset_prepare.html) - [Inference with existing models](https://
 mmpretrain.readthedocs.io/en/latest/user_guides/inference.html) - [Train]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/train.html) - [Test]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/test.html) -
 [Downstream tasks](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 downstream.html) For more information, please refer to [our documentation]
 (https://mmpretrain.readthedocs.io/en/latest/). ## Model zoo Results and models
@@ -91,20 +99,20 @@
     * SE-ResNet             (ICML'2020)          (arxiv'2023)   Training&Test Tips:
     * SE-ResNeXt          * MoCo_V2_           * OFA_               * RandAug
     * RegNet                (arXiv'2020)         (CoRR'2022)        * AutoAug
     * ShuffleNet_V1       * BYOL_              * Flamingo_          * RepeatAugSampler
     * ShuffleNet_V2         (NeurIPS'2020)       (NeurIPS'2022)     * TTA
     * MobileNet_V2        * SwAV_              * Chinese_CLIP_      * ...
     * MobileNet_V3          (NeurIPS'2020)       (arxiv'2022)
-    * Swin-               * DenseCL_
-      Transformer           (CVPR'2021)
-    * Swin-               * SimSiam_
-      Transformer_V2        (CVPR'2021)
-    * RepVGG              * Barlow_Twins_
-    * Vision-               (ICML'2021)
+    * Swin-               * DenseCL_           * MiniGPT-4_
+      Transformer           (CVPR'2021)          (arxiv'2023)
+    * Swin-               * SimSiam_           * LLaVA_
+      Transformer_V2        (CVPR'2021)          (arxiv'2023)
+    * RepVGG              * Barlow_Twins_      * Otter_
+    * Vision-               (ICML'2021)          (arxiv'2023)
       Transformer         * MoCo_V3_
     * Transformer-in-       (ICCV'2021)
       Transformer         * BEiT_
     * Res2Net               (ICLR'2022)
     * MLP-Mixer           * MAE_
     * DeiT                  (CVPR'2022)
     * DeiT-3              * SimMIM_
@@ -117,30 +125,34 @@
     * HRNet                 (arXiv'2022)
     * VAN                 * BEiT_V2_
     * ConvMixer             (arXiv'2022)
     * CSPNet              * EVA_
     * PoolFormer            (CVPR'2023)
     * Inception_V3        * MixMIM_
     * MobileOne             (arXiv'2022)
-    * EfficientFormer
-    * MViT
-    * HorNet
-    * MobileViT
+    * EfficientFormer     * iTPN_
+    * MViT                  (CVPR'2023)
+    * HorNet              * SparK_
+    * MobileViT             (ICLR'2023)
     * DaViT
     * RepLKNet
     * BEiT
     * MixMIM
     * EfficientNet_V2
     * RevViT
     * ConvNeXt_V2
     * ViG
     * XCiT
     * LeViT
     * RIFormer
     * GLIP
+    * ViT_SAM
+    * EVA02
+    * DINO_V2
+    * HiViT
 ## Contributing We appreciate all contributions to improve MMPreTrain. Please
 refer to [CONTRUBUTING](https://mmpretrain.readthedocs.io/en/latest/notes/
 contribution_guide.html) for the contributing guideline. ## Acknowledgement
 MMPreTrain is an open source project that is contributed by researchers and
 engineers from various colleges and companies. We appreciate all the
 contributors who implement their methods or add new features, as well as users
 who give valuable feedbacks. We wish that the toolbox and benchmark could serve
```

### Comparing `mmpretrain-1.0.0rc8/README.md` & `mmpretrain-1.0.1/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -82,36 +82,37 @@
   - Visual Grounding
   - Retrieval (Image-To-Image, Text-To-Image, Image-To-Text)
 
 https://github.com/open-mmlab/mmpretrain/assets/26739999/e4dcd3a2-f895-4d1b-a351-fbc74a04e904
 
 ## What's new
 
+ v1.0.1 was released in 28/07/2023
+
+Fix some bugs and enhance the codebase. Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details.
+
+ v1.0.0 was released in 04/07/2023
+
+- Support inference of more **multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc.
+- Support around **10 multi-modal** datasets!
+- Add [**iTPN**](./configs/itpn/), [**SparK**](./configs/spark/) self-supervised learning algorithms.
+- Provide examples of [New Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./configs/mae/benchmarks/). Here are the documentation links of [New Config](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner).
+
  v1.0.0rc8 was released in 22/05/2023
 
 - Support multiple **multi-modal** algorithms and inferencers. You can explore these features by the [gradio demo](https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)!
 - Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones.
 - Register torchvision transforms into MMPretrain, you can now easily integrate torchvision's data augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/en/latest/api/data_process.html#torchvision-transforms)
 
- v1.0.0rc7 was released in 07/04/2023
+Update of previous versions
 
 - Integrated Self-supervised learning algorithms from **MMSelfSup**, such as **MAE**, **BEiT**, etc.
 - Support **RIFormer**, a simple but effective vision backbone by removing token mixer.
-- Add t-SNE visualization.
 - Refactor dataset pipeline visualization.
-
-Update of previous versions
-
 - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones.
-- Reproduce the training accuracy of **ConvNeXt** and **RepVGG**.
-- Support confusion matrix calculation and plot.
-- Support **multi-task** training and testing.
-- Support Test-time Augmentation.
-- Upgrade API to get pre-defined models of MMPreTrain.
-- Refactor BEiT backbone and support v1/v2 inference.
 
 This release introduced a brand new and flexible training & test engine, but it's still in progress. Welcome
 to try according to [the documentation](https://mmpretrain.readthedocs.io/en/latest/).
 
 And there are some BC-breaking changes. Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/latest/migration.html).
 
 Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and other release history.
@@ -220,14 +221,18 @@
         <li><a href="configs/revvit">RevViT</a></li>
         <li><a href="configs/convnext_v2">ConvNeXt V2</a></li>
         <li><a href="configs/vig">ViG</a></li>
         <li><a href="configs/xcit">XCiT</a></li>
         <li><a href="configs/levit">LeViT</a></li>
         <li><a href="configs/riformer">RIFormer</a></li>
         <li><a href="configs/glip">GLIP</a></li>
+        <li><a href="configs/sam">ViT SAM</a></li>
+        <li><a href="configs/eva02">EVA02</a></li>
+        <li><a href="configs/dinov2">DINO V2</a></li>
+        <li><a href="configs/hivit">HiViT</a></li>
         </ul>
       </td>
       <td>
         <ul>
         <li><a href="configs/mocov2">MoCo V1 (CVPR'2020)</a></li>
         <li><a href="configs/simclr">SimCLR (ICML'2020)</a></li>
         <li><a href="configs/mocov2">MoCo V2 (arXiv'2020)</a></li>
@@ -242,23 +247,28 @@
         <li><a href="configs/simmim">SimMIM (CVPR'2022)</a></li>
         <li><a href="configs/maskfeat">MaskFeat (CVPR'2022)</a></li>
         <li><a href="configs/cae">CAE (arXiv'2022)</a></li>
         <li><a href="configs/milan">MILAN (arXiv'2022)</a></li>
         <li><a href="configs/beitv2">BEiT V2 (arXiv'2022)</a></li>
         <li><a href="configs/eva">EVA (CVPR'2023)</a></li>
         <li><a href="configs/mixmim">MixMIM (arXiv'2022)</a></li>
+        <li><a href="configs/itpn">iTPN (CVPR'2023)</a></li>
+        <li><a href="configs/spark">SparK (ICLR'2023)</a></li>
         </ul>
       </td>
       <td>
         <ul>
         <li><a href="configs/blip">BLIP (arxiv'2022)</a></li>
         <li><a href="configs/blip2">BLIP-2 (arxiv'2023)</a></li>
         <li><a href="configs/ofa">OFA (CoRR'2022)</a></li>
         <li><a href="configs/flamingo">Flamingo (NeurIPS'2022)</a></li>
         <li><a href="configs/chinese_clip">Chinese CLIP (arxiv'2022)</a></li>
+        <li><a href="configs/minigpt4">MiniGPT-4 (arxiv'2023)</a></li>
+        <li><a href="configs/llava">LLaVA (arxiv'2023)</a></li>
+        <li><a href="configs/otter">Otter (arxiv'2023)</a></li>
         </ul>
       </td>
       <td>
       Image Retrieval Task:
         <ul>
         <li><a href="configs/arcface">ArcFace (CVPR'2019)</a></li>
         </ul>
```

#### html2text {}

```diff
@@ -29,50 +29,58 @@
 backbones and pretrained models - Rich training strategies (supervised
 learning, self-supervised learning, multi-modality learning etc.) - Bag of
 training tricks - Large-scale training configs - High efficiency and
 extensibility - Powerful toolkits for model analysis and experiments - Various
 out-of-box inference tasks. - Image Classification - Image Caption - Visual
 Question Answering - Visual Grounding - Retrieval (Image-To-Image, Text-To-
 Image, Image-To-Text) https://github.com/open-mmlab/mmpretrain/assets/26739999/
-e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.0rc8 was released
-in 22/05/2023 - Support multiple **multi-modal** algorithms and inferencers.
-You can explore these features by the [gradio demo](https://github.com/open-
-mmlab/mmpretrain/tree/main/projects/gradio_demo)! - Add EVA-02, Dino-V2, ViT-
-SAM and GLIP backbones. - Register torchvision transforms into MMPretrain, you
-can now easily integrate torchvision's data augmentations in MMPretrain. See
-[the doc](https://mmpretrain.readthedocs.io/en/latest/api/
-data_process.html#torchvision-transforms)  v1.0.0rc7 was released in 07/04/
-2023 - Integrated Self-supervised learning algorithms from **MMSelfSup**, such
-as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
-vision backbone by removing token mixer. - Add t-SNE visualization. - Refactor
-dataset pipeline visualization. Update of previous versions - Support
-**LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**,
-**EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones. - Reproduce
-the training accuracy of **ConvNeXt** and **RepVGG**. - Support confusion
-matrix calculation and plot. - Support **multi-task** training and testing. -
-Support Test-time Augmentation. - Upgrade API to get pre-defined models of
-MMPreTrain. - Refactor BEiT backbone and support v1/v2 inference. This release
-introduced a brand new and flexible training & test engine, but it's still in
-progress. Welcome to try according to [the documentation](https://
-mmpretrain.readthedocs.io/en/latest/). And there are some BC-breaking changes.
-Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/
-latest/migration.html). Please refer to [changelog](https://
-mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and
-other release history. ## Installation Below are quick steps for installation:
-```shell conda create -n open-mmlab python=3.8 pytorch==1.10.1
-torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda activate open-mmlab
-pip install openmim git clone https://github.com/open-mmlab/mmpretrain.git cd
-mmpretrain mim install -e . ``` Please refer to [installation documentation]
-(https://mmpretrain.readthedocs.io/en/latest/get_started.html) for more
-detailed installation and dataset preparation. For multi-modality models
-support, please install the extra dependencies by: ```shell mim install -e ".
-[multimodal]" ``` ## User Guides We provided a series of tutorials about the
-basic usage of MMPreTrain for new users: - [Learn about Configs](https://
-mmpretrain.readthedocs.io/en/latest/user_guides/config.html) - [Prepare
-Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
+e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.1 was released in
+28/07/2023 Fix some bugs and enhance the codebase. Please refer to [changelog]
+(https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more
+details.  v1.0.0 was released in 04/07/2023 - Support inference of more
+**multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-
+4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc. - Support around
+**10 multi-modal** datasets! - Add [**iTPN**](./configs/itpn/), [**SparK**](./
+configs/spark/) self-supervised learning algorithms. - Provide examples of [New
+Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./
+configs/mae/benchmarks/). Here are the documentation links of [New Config]
+(https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-
+pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with
+FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/
+mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner). 
+v1.0.0rc8 was released in 22/05/2023 - Support multiple **multi-modal**
+algorithms and inferencers. You can explore these features by the [gradio demo]
+(https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)! -
+Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones. - Register torchvision
+transforms into MMPretrain, you can now easily integrate torchvision's data
+augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/
+en/latest/api/data_process.html#torchvision-transforms) Update of previous
+versions - Integrated Self-supervised learning algorithms from **MMSelfSup**,
+such as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
+vision backbone by removing token mixer. - Refactor dataset pipeline
+visualization. - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**,
+**EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM**
+backbones. This release introduced a brand new and flexible training & test
+engine, but it's still in progress. Welcome to try according to [the
+documentation](https://mmpretrain.readthedocs.io/en/latest/). And there are
+some BC-breaking changes. Please check [the migration tutorial](https://
+mmpretrain.readthedocs.io/en/latest/migration.html). Please refer to
+[changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html)
+for more details and other release history. ## Installation Below are quick
+steps for installation: ```shell conda create -n open-mmlab python=3.8
+pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda
+activate open-mmlab pip install openmim git clone https://github.com/open-
+mmlab/mmpretrain.git cd mmpretrain mim install -e . ``` Please refer to
+[installation documentation](https://mmpretrain.readthedocs.io/en/latest/
+get_started.html) for more detailed installation and dataset preparation. For
+multi-modality models support, please install the extra dependencies by:
+```shell mim install -e ".[multimodal]" ``` ## User Guides We provided a series
+of tutorials about the basic usage of MMPreTrain for new users: - [Learn about
+Configs](https://mmpretrain.readthedocs.io/en/latest/user_guides/config.html) -
+[Prepare Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 dataset_prepare.html) - [Inference with existing models](https://
 mmpretrain.readthedocs.io/en/latest/user_guides/inference.html) - [Train]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/train.html) - [Test]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/test.html) -
 [Downstream tasks](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 downstream.html) For more information, please refer to [our documentation]
 (https://mmpretrain.readthedocs.io/en/latest/). ## Model zoo Results and models
@@ -87,20 +95,20 @@
     * SE-ResNet             (ICML'2020)          (arxiv'2023)   Training&Test Tips:
     * SE-ResNeXt          * MoCo_V2_           * OFA_               * RandAug
     * RegNet                (arXiv'2020)         (CoRR'2022)        * AutoAug
     * ShuffleNet_V1       * BYOL_              * Flamingo_          * RepeatAugSampler
     * ShuffleNet_V2         (NeurIPS'2020)       (NeurIPS'2022)     * TTA
     * MobileNet_V2        * SwAV_              * Chinese_CLIP_      * ...
     * MobileNet_V3          (NeurIPS'2020)       (arxiv'2022)
-    * Swin-               * DenseCL_
-      Transformer           (CVPR'2021)
-    * Swin-               * SimSiam_
-      Transformer_V2        (CVPR'2021)
-    * RepVGG              * Barlow_Twins_
-    * Vision-               (ICML'2021)
+    * Swin-               * DenseCL_           * MiniGPT-4_
+      Transformer           (CVPR'2021)          (arxiv'2023)
+    * Swin-               * SimSiam_           * LLaVA_
+      Transformer_V2        (CVPR'2021)          (arxiv'2023)
+    * RepVGG              * Barlow_Twins_      * Otter_
+    * Vision-               (ICML'2021)          (arxiv'2023)
       Transformer         * MoCo_V3_
     * Transformer-in-       (ICCV'2021)
       Transformer         * BEiT_
     * Res2Net               (ICLR'2022)
     * MLP-Mixer           * MAE_
     * DeiT                  (CVPR'2022)
     * DeiT-3              * SimMIM_
@@ -113,30 +121,34 @@
     * HRNet                 (arXiv'2022)
     * VAN                 * BEiT_V2_
     * ConvMixer             (arXiv'2022)
     * CSPNet              * EVA_
     * PoolFormer            (CVPR'2023)
     * Inception_V3        * MixMIM_
     * MobileOne             (arXiv'2022)
-    * EfficientFormer
-    * MViT
-    * HorNet
-    * MobileViT
+    * EfficientFormer     * iTPN_
+    * MViT                  (CVPR'2023)
+    * HorNet              * SparK_
+    * MobileViT             (ICLR'2023)
     * DaViT
     * RepLKNet
     * BEiT
     * MixMIM
     * EfficientNet_V2
     * RevViT
     * ConvNeXt_V2
     * ViG
     * XCiT
     * LeViT
     * RIFormer
     * GLIP
+    * ViT_SAM
+    * EVA02
+    * DINO_V2
+    * HiViT
 ## Contributing We appreciate all contributions to improve MMPreTrain. Please
 refer to [CONTRUBUTING](https://mmpretrain.readthedocs.io/en/latest/notes/
 contribution_guide.html) for the contributing guideline. ## Acknowledgement
 MMPreTrain is an open source project that is contributed by researchers and
 engineers from various colleges and companies. We appreciate all the
 contributors who implement their methods or add new features, as well as users
 who give valuable feedbacks. We wish that the toolbox and benchmark could serve
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cifar100_bs16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cifar100_bs16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cifar10_bs16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cifar10_bs16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/inshop_bs32_448.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,69 +1,64 @@
-# data settings
-
+# dataset settings
+dataset_type = 'InShop'
 data_preprocessor = dict(
-    type='MultiModalDataPreprocessor',
-    mean=[122.770938, 116.7460125, 104.09373615],
-    std=[68.5005327, 66.6321579, 70.32316305],
-    to_rgb=True,
-)
+    num_classes=3997,
+    mean=[123.675, 116.28, 103.53],
+    std=[58.395, 57.12, 57.375],
+    # convert image from BGR to RGB
+    to_rgb=True)
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='RandomResizedCrop',
-        scale=384,
-        interpolation='bicubic',
-        backend='pillow'),
+    dict(type='Resize', scale=512),
+    dict(type='RandomCrop', crop_size=448),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='CleanCaption', keys='gt_caption'),
-    dict(
-        type='PackInputs',
-        algorithm_keys=['gt_caption'],
-        meta_keys=['image_id'],
-    ),
+    dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='Resize',
-        scale=(384, 384),
-        interpolation='bicubic',
-        backend='pillow'),
-    dict(type='PackInputs', meta_keys=['image_id']),
+    dict(type='Resize', scale=512),
+    dict(type='CenterCrop', crop_size=448),
+    dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=32,
-    num_workers=5,
+    num_workers=4,
     dataset=dict(
-        type='COCOCaption',
-        data_root='data/coco',
-        ann_file='annotations/coco_karpathy_train.json',
+        type=dataset_type,
+        data_root='data/inshop',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
-    persistent_workers=True,
-    drop_last=True,
 )
 
-val_dataloader = dict(
-    batch_size=16,
-    num_workers=5,
+query_dataloader = dict(
+    batch_size=32,
+    num_workers=4,
     dataset=dict(
-        type='COCOCaption',
-        data_root='data/coco',
-        ann_file='annotations/coco_karpathy_val.json',
-        pipeline=test_pipeline,
-    ),
+        type=dataset_type,
+        data_root='data/inshop',
+        split='query',
+        pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
-    persistent_workers=True,
 )
 
-val_evaluator = dict(
-    type='COCOCaption',
-    ann_file='data/coco/annotations/coco_karpathy_val_gt.json',
+gallery_dataloader = dict(
+    batch_size=32,
+    num_workers=4,
+    dataset=dict(
+        type=dataset_type,
+        data_root='data/inshop',
+        split='gallery',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
 )
+val_dataloader = query_dataloader
+val_evaluator = [
+    dict(type='RetrievalRecall', topk=1),
+    dict(type='RetrievalAveragePrecision', topk=10),
+]
 
-# # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/flickr30k_retrieval.py`

 * *Files 10% similar despite different names*

```diff
@@ -60,36 +60,53 @@
         meta_keys=['image_id']),
 ]
 
 train_dataloader = dict(
     batch_size=32,
     num_workers=16,
     dataset=dict(
-        type='COCORetrieval',
-        data_root='data/coco',
-        ann_file='annotations/caption_karpathy_train2014.json',
+        type='Flickr30kRetrieval',
+        data_root='data/flickr30k',
+        ann_file='annotations/dataset_flickr30k.json',
+        data_prefix='images',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
     persistent_workers=True,
     drop_last=True,
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=16,
     dataset=dict(
-        type='COCORetrieval',
-        data_root='data/coco',
-        ann_file='annotations/caption_karpathy_val2014.json',
+        type='Flickr30kRetrieval',
+        data_root='data/flickr30k',
+        ann_file='annotations/dataset_flickr30k.json',
+        data_prefix='images',
+        split='val',
         pipeline=test_pipeline,
-        # This is required for evaluation
-        test_mode=True,
+        test_mode=True,  # This is required for evaluation
     ),
     sampler=dict(type='SequentialSampler', subsample_type='sequential'),
     persistent_workers=True,
 )
 
 val_evaluator = dict(type='RetrievalRecall', topk=(1, 5, 10))
 
 # If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
+test_dataloader = dict(
+    batch_size=64,
+    num_workers=16,
+    dataset=dict(
+        type='Flickr30kRetrieval',
+        data_root='data/flickr30k',
+        ann_file='annotations/dataset_flickr30k.json',
+        data_prefix='images',
+        split='test',
+        pipeline=test_pipeline,
+        test_mode=True,  # This is required for evaluation
+    ),
+    sampler=dict(type='SequentialSampler', subsample_type='sequential'),
+    persistent_workers=True,
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_vg_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_vg_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/coco_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_448.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/cub_bs8_448.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet21k_bs128.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # dataset settings
-dataset_type = 'ImageNet21k'
+dataset_type = 'ImageNet'
 data_preprocessor = dict(
-    num_classes=21842,
+    num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
@@ -20,33 +20,31 @@
     dict(type='LoadImageFromFile'),
     dict(type='ResizeEdge', scale=256, edge='short'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=128,
+    batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root='data/imagenet21k',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        data_root='data/imagenet',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=128,
+    batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root='data/imagenet21k',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_mbv3.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_mbv3.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,28 +40,26 @@
 
 train_dataloader = dict(
     batch_size=128,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=128,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_medium_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_hivit_224.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # dataset settings
 dataset_type = 'ImageNet'
+data_root = 'data/imagenet/'
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
@@ -40,40 +41,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=236,
+        scale=256,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=128,
+    batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root='data/imagenet',
+        data_root=data_root,
         ann_file='meta/train.txt',
         data_prefix='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=128,
+    batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root='data/imagenet',
+        data_root=data_root,
         ann_file='meta/val.txt',
         data_prefix='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_small_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_revvit_224.py`

 * *Files 9% similar despite different names*

```diff
@@ -21,62 +21,63 @@
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=9,
+        magnitude_level=7,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
-        mode='rand',
+        mode='rand',  # should be 'pixel', but currently not supported
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
         fill_color=bgr_mean,
         fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=248,
+        scale=256,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=128,
+    batch_size=256,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
+    persistent_workers=True,
 )
 
 val_dataloader = dict(
-    batch_size=128,
+    batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_revvit_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_levit_224.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-# dataset settings
 dataset_type = 'ImageNet'
+
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
@@ -21,23 +21,22 @@
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=7,
+        magnitude_level=9,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
-        mode='rand',  # should be 'pixel', but currently not supported
+        mode='rand',
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
         fill_color=bgr_mean,
         fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
@@ -51,35 +50,31 @@
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=256,
-    num_workers=5,
+    num_workers=4,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
-    persistent_workers=True,
 )
 
 val_dataloader = dict(
-    batch_size=64,
-    num_workers=5,
+    batch_size=256,
+    num_workers=4,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
-    persistent_workers=True,
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_medium_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a12.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,70 +12,60 @@
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=384,
+        scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=9,
+        magnitude_level=7,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=404,
+        scale=236,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=384),
-    dict(type='PackInputs'),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='PackInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=128,
+    batch_size=256,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=256,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_small_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_small_384.py`

 * *Files 10% similar despite different names*

```diff
@@ -54,28 +54,26 @@
 
 train_dataloader = dict(
     batch_size=128,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_vig_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a3.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # dataset settings
 dataset_type = 'ImageNet'
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
+    mean=[123.675, 116.28, 103.53],
+    std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
@@ -21,61 +21,51 @@
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=9,
+        magnitude_level=6,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=248,
+        scale=236,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs'),
+    dict(type='PackInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=128,
+    batch_size=256,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=128,
+    batch_size=256,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_196.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_448.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,30 +9,30 @@
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=196,
+        scale=448,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=196,
+        scale=448,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=196),
+    dict(type='CenterCrop', crop_size=448),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
@@ -41,15 +41,15 @@
         ann_file='meta/train.txt',
         data_prefix='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=8,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
         ann_file='meta/val.txt',
         data_prefix='val',
         pipeline=test_pipeline),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_336.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_336.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,28 +34,26 @@
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_448.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_196.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,53 +9,51 @@
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=448,
+        scale=196,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=448,
+        scale=196,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=448),
+    dict(type='CenterCrop', crop_size=196),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_560.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_560.py`

 * *Files 6% similar despite different names*

```diff
@@ -34,28 +34,26 @@
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_pil_bicubic_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_pil_bicubic_384.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,28 +27,26 @@
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_beitv2.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_beitv2.py`

 * *Files 6% similar despite different names*

```diff
@@ -39,10 +39,9 @@
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
+        split='train',
         pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_davit_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b0_8xb32_in1k.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,14 @@
+_base_ = [
+    '../_base_/models/efficientnet_v2/efficientnetv2_b0.py',
+    '../_base_/datasets/imagenet_bs32.py',
+    '../_base_/schedules/imagenet_bs256.py',
+    '../_base_/default_runtime.py',
+]
+
 # dataset settings
 dataset_type = 'ImageNet'
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
@@ -12,15 +19,15 @@
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=224,
+        scale=192,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
@@ -38,45 +45,14 @@
         fill_color=bgr_mean,
         fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='ResizeEdge',
-        scale=236,
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='EfficientNetCenterCrop', crop_size=224, crop_padding=0),
     dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=64,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
-        pipeline=train_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=True),
-)
-
-val_dataloader = dict(
-    batch_size=64,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-val_evaluator = dict(type='Accuracy', topk=(1, 5))
-
-# If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
-test_evaluator = val_evaluator
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_levit_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_medium_224.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
+# dataset settings
 dataset_type = 'ImageNet'
-
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
@@ -40,42 +40,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=256,
+        scale=236,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=256,
-    num_workers=4,
+    batch_size=128,
+    num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root=r'E:\imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='ILSVRC2012_img_val',
+        data_root='data/imagenet',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=256,
-    num_workers=4,
+    batch_size=128,
+    num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root=r'E:\imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='ILSVRC2012_img_val',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a12.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs8_pil_bicubic_320.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,73 +1,58 @@
 # dataset settings
 dataset_type = 'ImageNet'
 data_preprocessor = dict(
-    num_classes=1000,
     # RGB format normalization parameters
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
+    mean=[122.5, 122.5, 122.5],
+    std=[122.5, 122.5, 122.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
-bgr_mean = data_preprocessor['mean'][::-1]
-bgr_std = data_preprocessor['std'][::-1]
-
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=224,
+        scale=320,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(
-        type='RandAugment',
-        policies='timm_increasing',
-        num_policies=2,
-        total_level=10,
-        magnitude_level=7,
-        magnitude_std=0.5,
-        hparams=dict(
-            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=236,
+        scale=int(320 / 224 * 256),
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs')
+    dict(type='CenterCrop', crop_size=320),
+    dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=256,
+    batch_size=8,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=256,
+    batch_size=8,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a3.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-large_8xb128_in1k.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,18 @@
-# dataset settings
-dataset_type = 'ImageNet'
+_base_ = [
+    '../_base_/models/van/van_large.py',
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
+    '../_base_/default_runtime.py'
+]
+
+# dataset setting
 data_preprocessor = dict(
-    num_classes=1000,
-    # RGB format normalization parameters
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
@@ -21,54 +25,41 @@
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=6,
+        magnitude_level=9,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
+    dict(
+        type='RandomErasing',
+        erase_prob=0.25,
+        mode='rand',
+        min_area_ratio=0.02,
+        max_area_ratio=1 / 3,
+        fill_color=bgr_mean,
+        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=236,
+        scale=248,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs')
+    dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=256,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
-        pipeline=train_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=True),
-)
-
-val_dataloader = dict(
-    batch_size=256,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-val_evaluator = dict(type='Accuracy', topk=(1, 5))
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 
-# If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
-test_evaluator = val_evaluator
+# schedule settings
+optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_simmim_192.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_simmim_192.py`

 * *Files 4% similar despite different names*

```diff
@@ -25,10 +25,9 @@
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
+        split='train',
         pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_swin_192.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_small_224.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,83 +1,80 @@
 # dataset settings
 dataset_type = 'ImageNet'
-data_root = 'data/imagenet/'
 data_preprocessor = dict(
     num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
+bgr_mean = data_preprocessor['mean'][::-1]
+bgr_std = data_preprocessor['std'][::-1]
+
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=192,
+        scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
         magnitude_level=9,
         magnitude_std=0.5,
-        hparams=dict(pad_val=[104, 116, 124], interpolation='bicubic')),
+        hparams=dict(
+            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
         mode='rand',
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
-        fill_color=[103.53, 116.28, 123.675],
-        fill_std=[57.375, 57.12, 58.395]),
+        fill_color=bgr_mean,
+        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=219,
+        scale=248,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=192),
+    dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=256,
-    num_workers=8,
-    collate_fn=dict(type='default_collate'),
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
+    batch_size=128,
+    num_workers=5,
     dataset=dict(
         type=dataset_type,
-        data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        data_root='data/imagenet',
+        split='train',
         pipeline=train_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=64,
+    batch_size=128,
     num_workers=5,
-    collate_fn=dict(type='default_collate'),
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        data_root=data_root,
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,33 +20,31 @@
     dict(type='LoadImageFromFile'),
     dict(type='ResizeEdge', scale=256, edge='short'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_byol.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_byol.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,10 +81,9 @@
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
+        split='train',
         pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_mocov2.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mocov3.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,57 +3,88 @@
 data_root = 'data/imagenet/'
 data_preprocessor = dict(
     type='SelfSupDataPreprocessor',
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     to_rgb=True)
 
-# The difference between mocov2 and mocov1 is the transforms in the pipeline
-view_pipeline = [
+view_pipeline1 = [
     dict(
         type='RandomResizedCrop',
         scale=224,
         crop_ratio_range=(0.2, 1.),
         backend='pillow'),
     dict(
         type='RandomApply',
         transforms=[
             dict(
                 type='ColorJitter',
                 brightness=0.4,
                 contrast=0.4,
-                saturation=0.4,
+                saturation=0.2,
                 hue=0.1)
         ],
         prob=0.8),
     dict(
         type='RandomGrayscale',
         prob=0.2,
         keep_channels=True,
         channel_weights=(0.114, 0.587, 0.2989)),
     dict(
         type='GaussianBlur',
         magnitude_range=(0.1, 2.0),
         magnitude_std='inf',
-        prob=0.5),
+        prob=1.),
+    dict(type='Solarize', thr=128, prob=0.),
+    dict(type='RandomFlip', prob=0.5),
+]
+view_pipeline2 = [
+    dict(
+        type='RandomResizedCrop',
+        scale=224,
+        crop_ratio_range=(0.2, 1.),
+        backend='pillow'),
+    dict(
+        type='RandomApply',
+        transforms=[
+            dict(
+                type='ColorJitter',
+                brightness=0.4,
+                contrast=0.4,
+                saturation=0.2,
+                hue=0.1)
+        ],
+        prob=0.8),
+    dict(
+        type='RandomGrayscale',
+        prob=0.2,
+        keep_channels=True,
+        channel_weights=(0.114, 0.587, 0.2989)),
+    dict(
+        type='GaussianBlur',
+        magnitude_range=(0.1, 2.0),
+        magnitude_std='inf',
+        prob=0.1),
+    dict(type='Solarize', thr=128, prob=0.2),
     dict(type='RandomFlip', prob=0.5),
 ]
-
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='MultiView', num_views=2, transforms=[view_pipeline]),
+    dict(
+        type='MultiView',
+        num_views=[1, 1],
+        transforms=[view_pipeline1, view_pipeline2]),
     dict(type='PackInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=512,
     num_workers=8,
-    drop_last=True,
     persistent_workers=True,
+    pin_memory=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
+        split='train',
         pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_bicubic.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_bicubic.py`

 * *Files 9% similar despite different names*

```diff
@@ -34,28 +34,26 @@
 
 train_dataloader = dict(
     batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_resize.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_pil_resize.py`

 * *Files 8% similar despite different names*

```diff
@@ -25,28 +25,26 @@
 
 train_dataloader = dict(
     batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=32,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_simclr.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_simclr.py`

 * *Files 3% similar despite different names*

```diff
@@ -44,10 +44,9 @@
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
+        split='train',
         pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mae.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-384px.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,33 +1,40 @@
-# dataset settings
-dataset_type = 'ImageNet'
-data_root = 'data/imagenet/'
-data_preprocessor = dict(
-    type='SelfSupDataPreprocessor',
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
-    to_rgb=True)
+_base_ = [
+    '../_base_/models/vit-base-p16.py',
+    '../_base_/datasets/imagenet_bs64_pil_resize.py',
+    '../_base_/schedules/imagenet_bs4096_AdamW.py',
+    '../_base_/default_runtime.py'
+]
+
+# model setting
+model = dict(backbone=dict(pre_norm=True))
 
+# data settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=224,
-        crop_ratio_range=(0.2, 1.0),
+        scale=384,
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='RandomFlip', prob=0.5),
-    dict(type='PackInputs')
+    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
+    dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=512,
-    num_workers=8,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    collate_fn=dict(type='default_collate'),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
-        pipeline=train_pipeline))
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='ResizeEdge',
+        scale=384,
+        edge='short',
+        backend='pillow',
+        interpolation='bicubic'),
+    dict(type='CenterCrop', crop_size=384),
+    dict(type='PackInputs'),
+]
+
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+
+# schedule setting
+optim_wrapper = dict(clip_grad=dict(max_norm=1.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mocov3.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-base_8xb128_in1k.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,91 +1,65 @@
-# dataset settings
-dataset_type = 'ImageNet'
-data_root = 'data/imagenet/'
+_base_ = [
+    '../_base_/models/van/van_base.py',
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
+    '../_base_/default_runtime.py',
+]
+
+# dataset setting
 data_preprocessor = dict(
-    type='SelfSupDataPreprocessor',
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
-    to_rgb=True)
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
+    # convert image from BGR to RGB
+    to_rgb=True,
+)
 
-view_pipeline1 = [
-    dict(
-        type='RandomResizedCrop',
-        scale=224,
-        crop_ratio_range=(0.2, 1.),
-        backend='pillow'),
-    dict(
-        type='RandomApply',
-        transforms=[
-            dict(
-                type='ColorJitter',
-                brightness=0.4,
-                contrast=0.4,
-                saturation=0.2,
-                hue=0.1)
-        ],
-        prob=0.8),
-    dict(
-        type='RandomGrayscale',
-        prob=0.2,
-        keep_channels=True,
-        channel_weights=(0.114, 0.587, 0.2989)),
-    dict(
-        type='GaussianBlur',
-        magnitude_range=(0.1, 2.0),
-        magnitude_std='inf',
-        prob=1.),
-    dict(type='Solarize', thr=128, prob=0.),
-    dict(type='RandomFlip', prob=0.5),
-]
-view_pipeline2 = [
+bgr_mean = data_preprocessor['mean'][::-1]
+bgr_std = data_preprocessor['std'][::-1]
+
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
         scale=224,
-        crop_ratio_range=(0.2, 1.),
-        backend='pillow'),
-    dict(
-        type='RandomApply',
-        transforms=[
-            dict(
-                type='ColorJitter',
-                brightness=0.4,
-                contrast=0.4,
-                saturation=0.2,
-                hue=0.1)
-        ],
-        prob=0.8),
-    dict(
-        type='RandomGrayscale',
-        prob=0.2,
-        keep_channels=True,
-        channel_weights=(0.114, 0.587, 0.2989)),
-    dict(
-        type='GaussianBlur',
-        magnitude_range=(0.1, 2.0),
-        magnitude_std='inf',
-        prob=0.1),
-    dict(type='Solarize', thr=128, prob=0.2),
-    dict(type='RandomFlip', prob=0.5),
+        backend='pillow',
+        interpolation='bicubic'),
+    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
+    dict(
+        type='RandAugment',
+        policies='timm_increasing',
+        num_policies=2,
+        total_level=10,
+        magnitude_level=9,
+        magnitude_std=0.5,
+        hparams=dict(
+            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
+    dict(
+        type='RandomErasing',
+        erase_prob=0.25,
+        mode='rand',
+        min_area_ratio=0.02,
+        max_area_ratio=1 / 3,
+        fill_color=bgr_mean,
+        fill_std=bgr_std),
+    dict(type='PackInputs'),
 ]
-train_pipeline = [
+
+test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
-        type='MultiView',
-        num_views=[1, 1],
-        transforms=[view_pipeline1, view_pipeline2]),
-    dict(type='PackInputs')
+        type='ResizeEdge',
+        scale=248,
+        edge='short',
+        backend='pillow',
+        interpolation='bicubic'),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=512,
-    num_workers=8,
-    persistent_workers=True,
-    pin_memory=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    collate_fn=dict(type='default_collate'),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='meta/train.txt',
-        data_prefix=dict(img_path='train/'),
-        pipeline=train_pipeline))
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+
+# schedule settings
+optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_384.py`

 * *Files 7% similar despite different names*

```diff
@@ -7,46 +7,47 @@
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='RandomResizedCrop', scale=224),
+    dict(
+        type='RandomResizedCrop',
+        scale=384,
+        backend='pillow',
+        interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='ResizeEdge', scale=256, edge='short'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='Resize', scale=384, backend='pillow', interpolation='bicubic'),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_autoaug.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_autoaug.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,28 +33,26 @@
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_448.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # dataset settings
 dataset_type = 'ImageNet'
 img_norm_cfg = dict(
     mean=[0.48145466 * 255, 0.4578275 * 255, 0.40821073 * 255],
     std=[0.26862954 * 255, 0.26130258 * 255, 0.27577711 * 255],
     to_rgb=True)
-image_size = 224
+image_size = 448
+
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
         size=image_size,
         backend='pillow',
         interpolation='bicubic'),
@@ -51,22 +52,23 @@
 ]
 
 data = dict(
     samples_per_gpu=64,
     workers_per_gpu=8,
     train=dict(
         type=dataset_type,
-        data_prefix='data/imagenet/train',
+        data_root='data/imagenet',
+        split='train',
         pipeline=train_pipeline),
     val=dict(
         type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline),
     test=dict(
         # replace `data/val` with `data/test` for standard test
         type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline))
 
 evaluation = dict(interval=10, metric='accuracy')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_384.py`

 * *Files 16% similar despite different names*

```diff
@@ -51,22 +51,23 @@
 ]
 
 data = dict(
     samples_per_gpu=64,
     workers_per_gpu=8,
     train=dict(
         type=dataset_type,
-        data_prefix='data/imagenet/train',
+        data_root='data/imagenet',
+        split='train',
         pipeline=train_pipeline),
     val=dict(
         type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline),
     test=dict(
         # replace `data/val` with `data/test` for standard test
         type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
+        data_root='data/imagenet',
+        split='val',
         pipeline=test_pipeline))
 
 evaluation = dict(interval=10, metric='accuracy')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_448.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/ocrvqa.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,73 +1,81 @@
-# dataset settings
-dataset_type = 'ImageNet'
-img_norm_cfg = dict(
-    mean=[0.48145466 * 255, 0.4578275 * 255, 0.40821073 * 255],
-    std=[0.26862954 * 255, 0.26130258 * 255, 0.27577711 * 255],
-    to_rgb=True)
-image_size = 448
+# data settings
+
+data_preprocessor = dict(
+    mean=[122.770938, 116.7460125, 104.09373615],
+    std=[68.5005327, 66.6321579, 70.32316305],
+    to_rgb=True,
+)
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        size=image_size,
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
-    # dict(
-    #     type='RandAugment',
-    #     policies={{_base_.rand_increasing_policies}},
-    #     num_policies=2,
-    #     total_level=10,
-    #     magnitude_level=9,
-    #     magnitude_std=0.5,
-    #     hparams=dict(
-    #         pad_val=[round(x) for x in img_norm_cfg['mean'][::-1]],
-    #         interpolation='bicubic')),
+        scale=384,
+        interpolation='bicubic',
+        backend='pillow'),
+    dict(type='CleanCaption', keys=['question', 'gt_answer']),
     dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=img_norm_cfg['mean'][::-1],
-        fill_std=img_norm_cfg['std'][::-1]),
-    dict(type='Normalize', **img_norm_cfg),
-    dict(type='ImageToTensor', keys=['img']),
-    dict(type='ToTensor', keys=['gt_label']),
-    dict(type='Collect', keys=['img', 'gt_label'])
+        type='PackInputs',
+        algorithm_keys=['question', 'gt_answer', 'gt_answer_weight'],
+        meta_keys=[],
+    ),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='Resize',
-        size=(image_size, -1),
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=image_size),
-    dict(type='Normalize', **img_norm_cfg),
-    dict(type='ImageToTensor', keys=['img']),
-    dict(type='Collect', keys=['img'])
+        scale=(480, 480),
+        interpolation='bicubic',
+        backend='pillow'),
+    dict(type='CleanCaption', keys=['question', 'gt_answer']),
+    dict(
+        type='PackInputs',
+        algorithm_keys=['question', 'gt_answer', 'gt_answer_weight'],
+        meta_keys=[],
+    ),
 ]
 
-data = dict(
-    samples_per_gpu=64,
-    workers_per_gpu=8,
-    train=dict(
-        type=dataset_type,
-        data_prefix='data/imagenet/train',
+train_dataloader = dict(
+    batch_size=16,
+    num_workers=8,
+    dataset=dict(
+        type='OCRVQA',
+        data_root='data/ocrvqa',
+        data_prefix='images',
+        ann_file='annotations/dataset.json',
+        split='train',
         pipeline=train_pipeline),
-    val=dict(
-        type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    persistent_workers=True,
+    drop_last=True,
+)
+
+val_dataloader = dict(
+    batch_size=64,
+    num_workers=8,
+    dataset=dict(
+        type='OCRVQA',
+        data_root='data/ocrvqa',
+        data_prefix='images',
+        ann_file='annotations/dataset.json',
+        split='val',
         pipeline=test_pipeline),
-    test=dict(
-        # replace `data/val` with `data/test` for standard test
-        type=dataset_type,
-        data_prefix='data/imagenet/val',
-        ann_file='data/imagenet/meta/val.txt',
-        pipeline=test_pipeline))
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
+)
+val_evaluator = dict(type='VQAAcc')
 
-evaluation = dict(interval=10, metric='accuracy')
+test_dataloader = dict(
+    batch_size=64,
+    num_workers=8,
+    dataset=dict(
+        type='OCRVQA',
+        data_root='data/ocrvqa',
+        data_prefix='images',
+        ann_file='annotations/dataset.json',
+        split='test',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
+)
+test_evaluator = dict(type='VQAAcc')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_convmixer_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize_autoaug.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,65 +17,51 @@
     dict(
         type='RandomResizedCrop',
         scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
-        type='RandAugment',
-        policies='timm_increasing',
-        num_policies=2,
-        total_level=10,
-        magnitude_level=9,
-        magnitude_std=0.5,
+        type='AutoAugment',
+        policies='imagenet',
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
-    dict(type='PackInputs')
+    dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=233,
+        scale=256,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs')
+    dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_224.py`

 * *Files 10% similar despite different names*

```diff
@@ -40,42 +40,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=224,
+        scale=256,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_384.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,28 +34,26 @@
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_edgenext_256.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-tiny_8xb128_in1k.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,38 +1,43 @@
-# dataset settings
-dataset_type = 'ImageNet'
+_base_ = [
+    '../_base_/models/van/van_tiny.py',
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
+    '../_base_/default_runtime.py'
+]
+
+# dataset setting
 data_preprocessor = dict(
-    num_classes=1000,
-    # RGB format normalization parameters
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=256,
+        scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
         magnitude_level=9,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
         mode='rand',
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
         fill_color=bgr_mean,
@@ -40,43 +45,21 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=292,
+        scale=248,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=256),
-    dict(type='PackInputs')
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=64,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
-        pipeline=train_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=True),
-)
-
-val_dataloader = dict(
-    batch_size=64,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-val_evaluator = dict(type='Accuracy', topk=(1, 5))
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 
-# If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
-test_evaluator = val_evaluator
+# schedule settings
+optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_mixer_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_mixer_224.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,28 +26,26 @@
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize.py`

 * *Files 11% similar despite different names*

```diff
@@ -25,28 +25,26 @@
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize_autoaug.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_256.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,58 +12,68 @@
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=224,
+        scale=256,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
-        type='AutoAugment',
-        policies='imagenet',
+        type='RandAugment',
+        policies='timm_increasing',
+        num_policies=2,
+        total_level=10,
+        magnitude_level=9,
+        magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(
+        type='RandomErasing',
+        erase_prob=0.25,
+        mode='rand',
+        min_area_ratio=0.02,
+        max_area_ratio=1 / 3,
+        fill_color=bgr_mean,
+        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=256,
+        scale=292,  # ( 256 / 224 * 256 )
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='CenterCrop', crop_size=256),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_davit_224.py`

 * *Files 16% similar despite different names*

```diff
@@ -40,42 +40,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=256,
+        scale=236,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_256.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_224.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=256,
+        scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
@@ -40,42 +40,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=292,  # ( 256 / 224 * 256 )
+        scale=224,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=256),
+    dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
     batch_size=64,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/coco_caption.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,56 +1,70 @@
-# dataset settings
-dataset_type = 'ImageNet'
+# data settings
+# coco caption annotations can be grabbed from LAVIS repo
+# https://github.com/salesforce/LAVIS/blob/main/lavis/configs/datasets/coco/defaults_cap.yaml
 data_preprocessor = dict(
-    num_classes=1000,
-    # RGB format normalization parameters
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
-    # convert image from BGR to RGB
+    type='MultiModalDataPreprocessor',
+    mean=[122.770938, 116.7460125, 104.09373615],
+    std=[68.5005327, 66.6321579, 70.32316305],
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
         scale=384,
-        backend='pillow',
-        interpolation='bicubic'),
+        interpolation='bicubic',
+        backend='pillow'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='PackInputs'),
+    dict(type='CleanCaption', keys='gt_caption'),
+    dict(
+        type='PackInputs',
+        algorithm_keys=['gt_caption'],
+        meta_keys=['image_id'],
+    ),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='Resize', scale=384, backend='pillow', interpolation='bicubic'),
-    dict(type='PackInputs'),
+    dict(
+        type='Resize',
+        scale=(384, 384),
+        interpolation='bicubic',
+        backend='pillow'),
+    dict(type='PackInputs', meta_keys=['image_id']),
 ]
 
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=5,
     dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        type='COCOCaption',
+        data_root='data/coco',
+        ann_file='annotations/coco_karpathy_train.json',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
+    persistent_workers=True,
+    drop_last=True,
 )
 
 val_dataloader = dict(
-    batch_size=64,
+    batch_size=16,
     num_workers=5,
     dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
-        pipeline=test_pipeline),
+        type='COCOCaption',
+        data_root='data/coco',
+        ann_file='annotations/coco_karpathy_val.json',
+        pipeline=test_pipeline,
+    ),
     sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
+)
+
+val_evaluator = dict(
+    type='COCOCaption',
+    ann_file='data/coco/annotations/coco_karpathy_val_gt.json',
 )
-val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
-# If you want standard test, please manually configure the test dataset
+# # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_t2t_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_medium_384.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=224,
+        scale=384,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
@@ -40,42 +40,40 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=248,
+        scale=404,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='CenterCrop', crop_size=384),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=128,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
+        split='train',
         pipeline=train_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=True),
 )
 
 val_dataloader = dict(
-    batch_size=64,
+    batch_size=16,
     num_workers=5,
     dataset=dict(
         type=dataset_type,
         data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        split='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
 )
 val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
 # If you want standard test, please manually configure the test dataset
 test_dataloader = val_dataloader
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs8_pil_bicubic_320.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/gqa.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,61 +1,81 @@
-# dataset settings
-dataset_type = 'ImageNet'
+# data settings
+
 data_preprocessor = dict(
-    # RGB format normalization parameters
-    mean=[122.5, 122.5, 122.5],
-    std=[122.5, 122.5, 122.5],
-    # convert image from BGR to RGB
+    mean=[122.770938, 116.7460125, 104.09373615],
+    std=[68.5005327, 66.6321579, 70.32316305],
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=320,
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='PackInputs'),
+        scale=384,
+        interpolation='bicubic',
+        backend='pillow'),
+    dict(
+        type='PackInputs',
+        algorithm_keys=['question', 'gt_answer', 'gt_answer_weight'],
+        meta_keys=['question_id', 'image_id'],
+    ),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
-        type='ResizeEdge',
-        scale=int(320 / 224 * 256),
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=320),
-    dict(type='PackInputs'),
+        type='Resize',
+        scale=(480, 480),
+        interpolation='bicubic',
+        backend='pillow'),
+    dict(
+        type='CleanCaption',
+        keys=['question'],
+    ),
+    dict(
+        type='PackInputs',
+        algorithm_keys=['question', 'gt_answer', 'gt_answer_weight'],
+        meta_keys=['question_id', 'image_id'],
+    ),
 ]
 
 train_dataloader = dict(
-    batch_size=8,
-    num_workers=5,
+    batch_size=16,
+    num_workers=8,
     dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/train.txt',
-        data_prefix='train',
-        pipeline=train_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=True),
+        type='GQA',
+        data_root='data/gqa',
+        data_prefix='images',
+        ann_file='annotations/train_balanced_questions.json',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
+    drop_last=True,
 )
 
 val_dataloader = dict(
-    batch_size=8,
-    num_workers=5,
+    batch_size=16,
+    num_workers=8,
     dataset=dict(
-        type=dataset_type,
-        data_root='data/imagenet',
-        ann_file='meta/val.txt',
-        data_prefix='val',
+        type='GQA',
+        data_root='data/gqa',
+        data_prefix='images',
+        ann_file='annotations/testdev_balanced_questions.json',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
 )
-val_evaluator = dict(type='Accuracy', topk=(1, 5))
+val_evaluator = dict(type='GQAAcc')
 
-# If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
+test_dataloader = dict(
+    batch_size=16,
+    num_workers=8,
+    dataset=dict(
+        type='GQA',
+        data_root='data/gqa',
+        data_prefix='images',
+        ann_file='annotations/testdev_balanced_questions.json',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    persistent_workers=True,
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/inshop_bs32_448.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspdarknet50_8xb32_in1k.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,61 +1,45 @@
-# dataset settings
-dataset_type = 'InShop'
-data_preprocessor = dict(
-    num_classes=3997,
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
-    # convert image from BGR to RGB
-    to_rgb=True)
+_base_ = [
+    '../_base_/datasets/imagenet_bs32.py',
+    '../_base_/schedules/imagenet_bs256.py',
+    '../_base_/default_runtime.py',
+]
 
+# model settings
+model = dict(
+    type='ImageClassifier',
+    backbone=dict(type='CSPDarkNet', depth=53),
+    neck=dict(type='GlobalAveragePooling'),
+    head=dict(
+        type='LinearClsHead',
+        num_classes=1000,
+        in_channels=1024,
+        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
+    ))
+
+# dataset settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='Resize', scale=512),
-    dict(type='RandomCrop', crop_size=448),
+    dict(
+        type='RandomResizedCrop',
+        scale=224,
+        backend='pillow',
+        interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='Resize', scale=512),
-    dict(type='CenterCrop', crop_size=448),
+    dict(
+        type='ResizeEdge',
+        scale=288,
+        edge='short',
+        backend='pillow',
+        interpolation='bicubic'),
+    dict(type='CenterCrop', crop_size=256),
     dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(
-    batch_size=32,
-    num_workers=4,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/inshop',
-        split='train',
-        pipeline=train_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=True),
-)
-
-query_dataloader = dict(
-    batch_size=32,
-    num_workers=4,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/inshop',
-        split='query',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-
-gallery_dataloader = dict(
-    batch_size=32,
-    num_workers=4,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/inshop',
-        split='gallery',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-val_dataloader = query_dataloader
-val_evaluator = dict(type='RetrievalRecall', topk=1)
-
-test_dataloader = val_dataloader
-test_evaluator = val_evaluator
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/nlvr2.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/nlvr2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/pipelines/auto_aug.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/pipelines/auto_aug.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/pipelines/rand_aug.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/pipelines/rand_aug.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/refcoco.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/refcoco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/datasets/voc_bs16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_mocov2.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,70 +1,58 @@
 # dataset settings
-dataset_type = 'VOC'
+dataset_type = 'ImageNet'
+data_root = 'data/imagenet/'
 data_preprocessor = dict(
-    num_classes=20,
-    # RGB format normalization parameters
+    type='SelfSupDataPreprocessor',
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
-    # convert image from BGR to RGB
-    to_rgb=True,
-    # generate onehot-format labels for multi-label classification.
-    to_onehot=True,
-)
+    to_rgb=True)
 
-train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='RandomResizedCrop', scale=224),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='PackInputs'),
+# The difference between mocov2 and mocov1 is the transforms in the pipeline
+view_pipeline = [
+    dict(
+        type='RandomResizedCrop',
+        scale=224,
+        crop_ratio_range=(0.2, 1.),
+        backend='pillow'),
+    dict(
+        type='RandomApply',
+        transforms=[
+            dict(
+                type='ColorJitter',
+                brightness=0.4,
+                contrast=0.4,
+                saturation=0.4,
+                hue=0.1)
+        ],
+        prob=0.8),
+    dict(
+        type='RandomGrayscale',
+        prob=0.2,
+        keep_channels=True,
+        channel_weights=(0.114, 0.587, 0.2989)),
+    dict(
+        type='GaussianBlur',
+        magnitude_range=(0.1, 2.0),
+        magnitude_std='inf',
+        prob=0.5),
+    dict(type='RandomFlip', prob=0.5),
 ]
 
-test_pipeline = [
+train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='ResizeEdge', scale=256, edge='short'),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs'),
+    dict(type='MultiView', num_views=2, transforms=[view_pipeline]),
+    dict(type='PackInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=16,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/VOCdevkit/VOC2007',
-        image_set_path='ImageSets/Layout/val.txt',
-        pipeline=train_pipeline),
+    batch_size=32,
+    num_workers=8,
+    drop_last=True,
+    persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
-)
-
-val_dataloader = dict(
-    batch_size=16,
-    num_workers=5,
+    collate_fn=dict(type='default_collate'),
     dataset=dict(
         type=dataset_type,
-        data_root='data/VOCdevkit/VOC2007',
-        image_set_path='ImageSets/Layout/val.txt',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-
-test_dataloader = dict(
-    batch_size=16,
-    num_workers=5,
-    dataset=dict(
-        type=dataset_type,
-        data_root='data/VOCdevkit/VOC2007',
-        image_set_path='ImageSets/Layout/val.txt',
-        pipeline=test_pipeline),
-    sampler=dict(type='DefaultSampler', shuffle=False),
-)
-
-# calculate precision_recall_f1 and mAP
-val_evaluator = [
-    dict(type='VOCMultiLabelMetric'),
-    dict(type='VOCMultiLabelMetric', average='micro'),
-    dict(type='VOCAveragePrecision')
-]
-
-# If you want standard test, please manually configure the test dataset
-test_dataloader = val_dataloader
-test_evaluator = val_evaluator
+        data_root=data_root,
+        split='train',
+        pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/default_runtime.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/default_runtime.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/base-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/base-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/small-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/small-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/small-p32.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/small-p32.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/conformer/tiny-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/conformer/tiny-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-large.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-large.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext/convnext-xlarge.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext/convnext-xlarge.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/huge.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/huge.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/large.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/large.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/convnext_v2/tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/convnext_v2/tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-base-p16-384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-huge-p14-224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-huge-p14-224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-large-p16-384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-medium-p16-224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-medium-p16-224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/deit3/deit3-small-p16-384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xsmall.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xsmall.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xxsmall.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/edgenext/edgenext-xxsmall.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/efficientformer-l1.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/efficientformer-l1.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/eva/eva-g.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/eva/eva-g.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/eva/eva-l.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/eva/eva-l.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base-gf.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base-gf.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-large.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small-gf.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small-gf.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny-gf.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny-gf.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/hornet/hornet-tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/levit-256-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/levit-256-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mae_vit-base-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mae_vit-base-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mixmim/mixmim_base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mixmim/mixmim_base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mlp_mixer_base_patch16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mlp_mixer_base_patch16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mlp_mixer_large_patch16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mlp_mixer_large_patch16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_large_imagenet.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_large_imagenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_050_imagenet.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_050_imagenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_075_imagenet.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_075_imagenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_imagenet.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mobilenet_v3/mobilenet_v3_small_imagenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-large.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-large.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/mvit/mvitv2-tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m36.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m36.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m48.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_m48.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s12.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s12.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s24.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s24.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s36.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/poolformer/poolformer_s36.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/replknet-31B_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/replknet-31B_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/repvgg-B3_lbs-mixup_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/repvgg-B3_lbs-mixup_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest101.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest101.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest200.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest200.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest269.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest269.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnest50.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnest50.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_cutmix.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cifar_cutmix.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/resnet50_cutmix.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/resnet50_cutmix.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/revvit/revvit-base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/revvit/revvit-base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/revvit/revvit-small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/revvit/revvit-small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/base_224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/small_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/small_224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer/tiny_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer/tiny_224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_256.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_256.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_384.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/base_384.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/small_256.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/small_256.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/tiny_256.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/swin_transformer_v2/tiny_256.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-14.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-14.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-19.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-19.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-24.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/t2t-vit-t-24.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-11m.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-11m.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-21m.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-21m.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-5m.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tinyvit/tinyvit-5m.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/tnt_s_patch16_224.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/tnt_s_patch16_224.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/twins_pcpvt_base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/twins_pcpvt_base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/twins_svt_base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/twins_svt_base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/van/van_tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/van/van_tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_medium.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_medium.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_base.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_small.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_small.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vig/vig_tiny.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vig/vig_tiny.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-base-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-base-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-base-p32.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-base-p32.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-large-p16.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-large-p16.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/models/vit-large-p32.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/models/vit-large-p32.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/cub_bs64.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/cub_bs64.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_conformer.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_conformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_revvit.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_revvit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_swin.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_swin.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_coslr.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_coslr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_linearlr_bn_nowd.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_linearlr_bn_nowd.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_AdamW.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_AdamW.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_adamw_levit.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_adamw_levit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_coslr.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_coslr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_rsb.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_rsb.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_200e_coslr_warmup.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_200e_coslr_warmup.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr_coswd_300e.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs256_coslr_coswd_300e.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs4096_AdamW.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_bs4096_AdamW.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_200e.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/schedules/imagenet_lars_coslr_200e.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/arcface/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/arcface/metafile.yml`

 * *Files 15% similar despite different names*

```diff
@@ -9,19 +9,20 @@
       Title: 'ArcFace: Additive Angular Margin Loss for Deep Face Recognition'
     README: configs/arcface/README.md
     Code:
       Version: v1.0.0rc3
       URL: https://github.com/open-mmlab/mmpretrain/blob/v1.0.0rc3/mmcls/models/heads/margin_head.py
 
 Models:
-  - Name: resnet50-arcface_8xb32_inshop
+  - Name: resnet50-arcface_inshop
     Metadata:
       FLOPs: 16571226112
       Parameters: 31693888
     In Collection: ArcFace
     Results:
       - Dataset: InShop
         Metrics:
           Recall@1: 90.18
+          mAP@10: 69.30
         Task: Image Retrieval
     Weights: https://download.openmmlab.com/mmclassification/v0/arcface/resnet50-arcface_inshop_20230202-b766fe7f.pth
     Config: configs/arcface/resnet50-arcface_8xb32_inshop.py
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/arcface/resnet50-arcface_8xb32_inshop.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/arcface/resnet50-arcface_8xb32_inshop.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-1000e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-1000e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/barlowtwins_resnet50_8xb256-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/barlowtwins/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/barlowtwins/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/beit_beit-base-p16_8xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/beit_beit-base-p16_8xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,15 +19,16 @@
         arch='base',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.1,
         out_type='avg_featmap',
         use_abs_pos_emb=False,
         use_rel_pos_bias=True,
-        use_shared_rel_pos_bias=False),
+        use_shared_rel_pos_bias=False,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/benchmarks/beit-base-p16_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-1600e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-1600e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,16 @@
         img_size=224,
         patch_size=16,
         # 0.2 for 1600 epochs pretrained models and 0.1 for 300 epochs.
         drop_path_rate=0.1,
         out_type='avg_featmap',
         use_abs_pos_emb=False,
         use_rel_pos_bias=True,
-        use_shared_rel_pos_bias=False),
+        use_shared_rel_pos_bias=False,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/beitv2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/beitv2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb16_refcoco.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb16_refcoco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_nlvr.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_nlvr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/blip-base_8xb32_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/blip-base_8xb32_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -5,14 +5,15 @@
         - COCO
         - VG
         - Conceptual Captions
         - Conceptual 12M
         - SBU captions
       Architecture:
         - Transformer
+      Training Resources: 8x A100 GPUs
     Paper:
       Title: 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language
         Understanding and Generation'
       URL: https://arxiv.org/abs/2201.12086
     README: configs/blip/README.md
 
 Models:
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb32_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb32_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/blip2_8xb32_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/blip2_8xb32_retrieval.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/blip2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/blip2/metafile.yml`

 * *Files 1% similar despite different names*

```diff
@@ -4,14 +4,15 @@
       Training Data:
         - COCO
         - VG
         - CC3M
         - CC12M
         - SBU
         - LAION-400M
+      Training Resources: 8x A100 GPUs
       Architecture:
         - Transformer
         - Q-Former
     Paper:
       Title: 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image
         Encoders and Large Language Models'
       URL: https://arxiv.org/abs/2301.12597
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50-c4_ms-1x_coco.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50-c4_ms-1x_coco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50_fpn_ms-1x_coco.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50_fpn_ms-1x_coco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/byol_resnet50_16xb256-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/byol_resnet50_16xb256-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/byol/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/byol/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 1% similar despite different names*

```diff
@@ -70,15 +70,15 @@
         final_norm=False,  # do not use final norm
         drop_path_rate=0.1,
         layer_scale_init_value=0.1,
         out_type='avg_featmap',
         use_abs_pos_emb=True,
         use_rel_pos_bias=True,
         use_shared_rel_pos_bias=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/cae_beit-base-p16_8xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/cae_beit-base-p16_8xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cae/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cae/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_resnet50_zeroshot-cls_cifar100.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_resnet50_zeroshot-cls_cifar100.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-base-p16_zeroshot-cls_cifar100.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-base-p16_zeroshot-cls_cifar100.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-huge-p14_zeroshot-cls_cifar100.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-huge-p14_zeroshot-cls_cifar100.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-large-p14_zeroshot-cls_cifar100.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/cn-clip_vit-large-p14_zeroshot-cls_cifar100.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/chinese_clip/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/chinese_clip/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-448px.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,30 +9,30 @@
 model = dict(backbone=dict(pre_norm=True))
 
 # data settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=384,
+        scale=448,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=384,
+        scale=448,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=384),
+    dict(type='CenterCrop', crop_size=448),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k-448px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-448px.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = [
-    '../_base_/models/vit-base-p16.py',
+    '../_base_/models/vit-base-p32.py',
     '../_base_/datasets/imagenet_bs64_pil_resize.py',
     '../_base_/schedules/imagenet_bs4096_AdamW.py',
     '../_base_/default_runtime.py'
 ]
 
 # model setting
 model = dict(backbone=dict(pre_norm=True))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p16_pt-64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k-448px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,30 +9,30 @@
 model = dict(backbone=dict(pre_norm=True))
 
 # data settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='RandomResizedCrop',
-        scale=448,
+        scale=224,
         backend='pillow',
         interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=448,
+        scale=224,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=448),
+    dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-base-p32_pt-64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k-384px.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,37 +2,35 @@
     '../_base_/models/vit-base-p32.py',
     '../_base_/datasets/imagenet_bs64_pil_resize.py',
     '../_base_/schedules/imagenet_bs4096_AdamW.py',
     '../_base_/default_runtime.py'
 ]
 
 # model setting
-model = dict(backbone=dict(pre_norm=True))
+model = dict(backbone=dict(img_size=384))
+
+# dataset setting
+data_preprocessor = dict(
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
+    # convert image from BGR to RGB
+    to_rgb=True,
+)
 
-# data settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='RandomResizedCrop',
-        scale=224,
-        backend='pillow',
-        interpolation='bicubic'),
+    dict(type='RandomResizedCrop', scale=384, backend='pillow'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='ResizeEdge',
-        scale=224,
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='ResizeEdge', scale=384, edge='short', backend='pillow'),
+    dict(type='CenterCrop', crop_size=384),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/clip/vit-large-p14_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/clip/vit-large-p14_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/conformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/conformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-1024-20_10xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-1024-20_10xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-1536-20_10xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-1536-20_10xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/convmixer-768-32_10xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/convmixer-768-32_10xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convmixer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convmixer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in21k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-base_32xb128_in21k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in21k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-large_64xb64_in21k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-small_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-tiny_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in21k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/convnext-xlarge_64xb64_in21k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-atto_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-atto_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-base_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-femto_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-femto_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-512px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k-512px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-huge_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-large_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-nano_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-pico_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-pico_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/convnext-v2-tiny_32xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/convnext_v2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/convnext_v2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspdarknet50_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspresnet50_8xb32_in1k.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,15 +3,15 @@
     '../_base_/schedules/imagenet_bs256.py',
     '../_base_/default_runtime.py',
 ]
 
 # model settings
 model = dict(
     type='ImageClassifier',
-    backbone=dict(type='CSPDarkNet', depth=53),
+    backbone=dict(type='CSPResNet', depth=50),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=1024,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
     ))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspresnet50_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/cspresnext50_8xb32_in1k.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,20 +3,20 @@
     '../_base_/schedules/imagenet_bs256.py',
     '../_base_/default_runtime.py',
 ]
 
 # model settings
 model = dict(
     type='ImageClassifier',
-    backbone=dict(type='CSPResNet', depth=50),
+    backbone=dict(type='CSPResNeXt', depth=50),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
-        in_channels=1024,
+        in_channels=2048,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
     ))
 
 # dataset settings
 train_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
@@ -28,18 +28,18 @@
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
     dict(
         type='ResizeEdge',
-        scale=288,
+        scale=256,
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=256),
+    dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/cspresnext50_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32-01norm_in1k.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,45 +1,31 @@
 _base_ = [
+    '../_base_/models/efficientnet_b7.py',
     '../_base_/datasets/imagenet_bs32.py',
     '../_base_/schedules/imagenet_bs256.py',
     '../_base_/default_runtime.py',
 ]
 
-# model settings
-model = dict(
-    type='ImageClassifier',
-    backbone=dict(type='CSPResNeXt', depth=50),
-    neck=dict(type='GlobalAveragePooling'),
-    head=dict(
-        type='LinearClsHead',
-        num_classes=1000,
-        in_channels=2048,
-        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
-    ))
-
 # dataset settings
+data_preprocessor = dict(
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
+    # convert image from BGR to RGB
+    to_rgb=True,
+)
+
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='RandomResizedCrop',
-        scale=224,
-        backend='pillow',
-        interpolation='bicubic'),
+    dict(type='EfficientNetRandomCrop', scale=600),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='ResizeEdge',
-        scale=256,
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='EfficientNetCenterCrop', crop_size=600),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/cspnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/cspnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/csra/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/csra/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/csra/resnet101-csra_1xb16_voc07-448px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/csra/resnet101-csra_1xb16_voc07-448px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/davit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/davit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base-distilled_16xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base_16xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base_16xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-base_16xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-base_16xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-small-distilled_4xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-small-distilled_4xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-small_4xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-small_4xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-tiny-distilled_4xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-tiny-distilled_4xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/deit-tiny_4xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/deit-tiny_4xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-base-p16_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-huge-p14_64xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-huge-p14_64xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb16_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb16_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-large-p16_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-medium-p16_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-medium-p16_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/deit3-small-p16_64xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/deit3/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/deit3/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/densecl_resnet50_8xb32-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/densecl_resnet50_8xb32-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densecl/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/densecl/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/densenet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/densenet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/dinov2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/dinov2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-base_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-small_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-xsmall_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-xsmall_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/edgenext-xxsmall_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/edgenext-xxsmall_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/edgenext/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/edgenext/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b0_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b1_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b2_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b3_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b4_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b5_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b6_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-em_8xb32-01norm_in1k.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = [
-    '../_base_/models/efficientnet_b7.py',
+    '../_base_/models/efficientnet_em.py',
     '../_base_/datasets/imagenet_bs32.py',
     '../_base_/schedules/imagenet_bs256.py',
     '../_base_/default_runtime.py',
 ]
 
 # dataset settings
 data_preprocessor = dict(
@@ -11,21 +11,21 @@
     std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='EfficientNetRandomCrop', scale=600),
+    dict(type='EfficientNetRandomCrop', scale=240),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='EfficientNetCenterCrop', crop_size=600),
+    dict(type='EfficientNetCenterCrop', crop_size=240),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b7_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-b8_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-em_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k-384px.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,31 +1,38 @@
 _base_ = [
-    '../_base_/models/efficientnet_em.py',
-    '../_base_/datasets/imagenet_bs32.py',
-    '../_base_/schedules/imagenet_bs256.py',
-    '../_base_/default_runtime.py',
+    '../_base_/models/vit-large-p32.py',
+    '../_base_/datasets/imagenet_bs64_pil_resize_autoaug.py',
+    '../_base_/schedules/imagenet_bs4096_AdamW.py',
+    '../_base_/default_runtime.py'
 ]
 
-# dataset settings
+# model setting
+model = dict(backbone=dict(img_size=384))
+
+# dataset setting
 data_preprocessor = dict(
     mean=[127.5, 127.5, 127.5],
     std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='EfficientNetRandomCrop', scale=240),
+    dict(type='RandomResizedCrop', scale=384, backend='pillow'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='EfficientNetCenterCrop', crop_size=240),
+    dict(type='ResizeEdge', scale=384, edge='short', backend='pillow'),
+    dict(type='CenterCrop', crop_size=384),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+
+# schedule setting
+optim_wrapper = dict(clip_grad=dict(max_norm=1.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-es_8xb32-01norm_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-es_8xb32-01norm_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb32_in1k-475px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb32_in1k-475px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb8_in1k-800px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/efficientnet-l2_8xb8_in1k-800px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b0_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-B3_8xb32_in1k.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,58 +1,67 @@
 _base_ = [
-    '../_base_/models/efficientnet_v2/efficientnetv2_b0.py',
-    '../_base_/datasets/imagenet_bs32.py',
-    '../_base_/schedules/imagenet_bs256.py',
-    '../_base_/default_runtime.py',
+    '../_base_/models/repvgg-B3_lbs-mixup_in1k.py',
+    '../_base_/datasets/imagenet_bs32_pil_resize.py',
+    '../_base_/schedules/imagenet_bs256_coslr.py',
+    '../_base_/default_runtime.py'
 ]
 
-# dataset settings
-dataset_type = 'ImageNet'
+# schedule settings
+optim_wrapper = dict(
+    paramwise_cfg=dict(
+        bias_decay_mult=0.0,
+        custom_keys={
+            'branch_3x3.norm': dict(decay_mult=0.0),
+            'branch_1x1.norm': dict(decay_mult=0.0),
+            'branch_norm.bias': dict(decay_mult=0.0),
+        }))
+
 data_preprocessor = dict(
-    num_classes=1000,
     # RGB format normalization parameters
     mean=[123.675, 116.28, 103.53],
     std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(
-        type='RandomResizedCrop',
-        scale=192,
-        backend='pillow',
-        interpolation='bicubic'),
+    dict(type='RandomResizedCrop', scale=224, backend='pillow'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=9,
+        magnitude_level=7,
         magnitude_std=0.5,
-        hparams=dict(
-            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
+        hparams=dict(pad_val=[round(x) for x in bgr_mean])),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='EfficientNetCenterCrop', crop_size=224, crop_padding=0),
+    dict(type='ResizeEdge', scale=256, edge='short', backend='pillow'),
+    dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
 train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+
+# schedule settings
+param_scheduler = dict(
+    type='CosineAnnealingLR',
+    T_max=200,
+    by_epoch=True,
+    begin=0,
+    end=200,
+    convert_to_iter_based=True)
+
+train_cfg = dict(by_epoch=True, max_epochs=200)
+
+default_hooks = dict(
+    checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b1_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b1_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b2_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b2_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b3_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-b3_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in1k-480px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-l_8xb32_in1k-480px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in1k-480px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-m_8xb32_in1k-480px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in21k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-s_8xb32_in21k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in1k-512px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/efficientnetv2-xl_8xb32_in1k-512px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/efficientnet_v2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/efficientnet_v2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.1,
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb2048-linear-coslr-90e_in1k.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,70 +1,64 @@
 _base_ = [
     '../../_base_/datasets/imagenet_bs32_pil_resize.py',
     '../../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../../_base_/default_runtime.py'
 ]
 
+# dataset settings
 train_dataloader = dict(batch_size=2048, drop_last=True)
 val_dataloader = dict(drop_last=False)
 test_dataloader = dict(drop_last=False)
 
 # model settings
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='VisionTransformer',
-        arch='base',
+        arch='large',
         img_size=224,
         patch_size=16,
-        frozen_stages=12,
+        frozen_stages=24,
         out_type='cls_token',
         final_norm=True,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
-    neck=dict(type='ClsBatchNormNeck', input_features=768),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
+    neck=dict(type='ClsBatchNormNeck', input_features=1024),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
-        in_channels=768,
+        in_channels=1024,
         loss=dict(type='CrossEntropyLoss'),
-        init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]),
-    data_preprocessor=dict(
-        num_classes=1000,
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        to_rgb=True,
-    ))
+        init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]))
 
 # optimizer
 optim_wrapper = dict(
     _delete_=True,
     type='AmpOptimWrapper',
-    optimizer=dict(type='LARS', lr=3.2, weight_decay=0.0, momentum=0.9),
-)
+    optimizer=dict(type='LARS', lr=6.4, weight_decay=0.0, momentum=0.9))
 
 # learning rate scheduler
 param_scheduler = [
     dict(
         type='LinearLR',
         start_factor=1e-4,
         by_epoch=True,
         begin=0,
         end=10,
         convert_to_iter_based=True),
     dict(
         type='CosineAnnealingLR',
-        T_max=90,
+        T_max=80,
         by_epoch=True,
         begin=10,
-        end=100,
+        end=90,
         eta_min=0.0,
         convert_to_iter_based=True)
 ]
 
 # runtime settings
-train_cfg = dict(by_epoch=True, max_epochs=100)
+train_cfg = dict(by_epoch=True, max_epochs=90)
 
 default_hooks = dict(
     checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3),
     logger=dict(type='LoggerHook', interval=10))
 
 randomness = dict(seed=0, diff_rank_seed=True)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p14_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p14_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-g-p16_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-g-p16_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-l-p14_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-l-p14_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/eva-mae-style_vit-base-p16_16xb256-coslr-400e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/eva-mae-style_vit-base-p16_16xb256-coslr-400e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-base-p14_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-base-p14_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-base-p14_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-base-p14_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-large-p14_headless.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-large-p14_headless.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-large-p14_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-large-p14_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-small-p14_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-small-p14_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/eva02-tiny-p14_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/eva02/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva02/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_fewshot_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/flamingo/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/flamingo/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/glip/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/glip/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hornet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/hornet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/hrnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/hrnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/inception_v3/inception-v3_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/inception_v3/inception-v3_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/inception_v3/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/inception_v3/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/lenet/lenet5_mnist.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/lenet/lenet5_mnist.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/levit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/levit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 5% similar despite different names*

```diff
@@ -53,15 +53,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.1,
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb2048-linear-coslr-90e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb2048-linear-coslr-90e_in1k.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         frozen_stages=12,
         out_type='cls_token',
         final_norm=True,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=dict(type='ClsBatchNormNeck', input_features=768),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(type='CrossEntropyLoss'),
         init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_32xb8-coslr-50e_in1k-448px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_32xb8-coslr-50e_in1k-448px.py`

 * *Files 1% similar despite different names*

```diff
@@ -55,15 +55,15 @@
         type='VisionTransformer',
         arch='huge',
         img_size=448,
         patch_size=14,
         drop_path_rate=0.3,  # set to 0.3
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=1280,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-coslr-50e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-coslr-50e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         type='VisionTransformer',
         arch='huge',
         img_size=224,
         patch_size=14,
         drop_path_rate=0.3,  # set to 0.3
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=1280,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-coslr-50e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-coslr-50e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         type='VisionTransformer',
         arch='large',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.2,  # set to 0.2
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=1024,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb2048-linear-coslr-90e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/eva/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,64 +1,70 @@
 _base_ = [
     '../../_base_/datasets/imagenet_bs32_pil_resize.py',
     '../../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../../_base_/default_runtime.py'
 ]
 
-# dataset settings
 train_dataloader = dict(batch_size=2048, drop_last=True)
 val_dataloader = dict(drop_last=False)
 test_dataloader = dict(drop_last=False)
 
 # model settings
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='VisionTransformer',
-        arch='large',
+        arch='base',
         img_size=224,
         patch_size=16,
-        frozen_stages=24,
+        frozen_stages=12,
         out_type='cls_token',
         final_norm=True,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
-    neck=dict(type='ClsBatchNormNeck', input_features=1024),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
+    neck=dict(type='ClsBatchNormNeck', input_features=768),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
-        in_channels=1024,
+        in_channels=768,
         loss=dict(type='CrossEntropyLoss'),
-        init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]))
+        init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]),
+    data_preprocessor=dict(
+        num_classes=1000,
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        to_rgb=True,
+    ))
 
 # optimizer
 optim_wrapper = dict(
     _delete_=True,
     type='AmpOptimWrapper',
-    optimizer=dict(type='LARS', lr=6.4, weight_decay=0.0, momentum=0.9))
+    optimizer=dict(type='LARS', lr=3.2, weight_decay=0.0, momentum=0.9),
+)
 
 # learning rate scheduler
 param_scheduler = [
     dict(
         type='LinearLR',
         start_factor=1e-4,
         by_epoch=True,
         begin=0,
         end=10,
         convert_to_iter_based=True),
     dict(
         type='CosineAnnealingLR',
-        T_max=80,
+        T_max=90,
         by_epoch=True,
         begin=10,
-        end=90,
+        end=100,
         eta_min=0.0,
         convert_to_iter_based=True)
 ]
 
 # runtime settings
-train_cfg = dict(by_epoch=True, max_epochs=90)
+train_cfg = dict(by_epoch=True, max_epochs=100)
 
 default_hooks = dict(
     checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3),
     logger=dict(type='LoggerHook', interval=10))
 
 randomness = dict(seed=0, diff_rank_seed=True)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-1600e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-1600e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-400e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-400e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-800e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-800e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-huge-p14_8xb512-amp-coslr-1600e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-huge-p14_8xb512-amp-coslr-1600e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-1600e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-1600e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-400e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-400e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-800e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-800e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mae/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mae/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/benchmarks/vit-base-p16_8xb256-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/benchmarks/vit-base-p16_8xb256-coslr-100e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,15 +52,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.1,
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/maskfeat_vit-base-p16_8xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/maskfeat_vit-base-p16_8xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/maskfeat/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/maskfeat/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         drop_path_rate=0.1,
         out_type='avg_featmap',
         final_norm=False,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,15 +15,15 @@
         type='VisionTransformer',
         arch='base',
         img_size=224,
         patch_size=16,
         frozen_stages=12,
         out_type='cls_token',
         final_norm=True,
-        init_cfg=dict(type='Pretrained', checkpoint='')),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=dict(type='ClsBatchNormNeck', input_features=768),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(type='CrossEntropyLoss'),
         init_cfg=[dict(type='TruncNormal', layer='Linear', std=0.01)]),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/milan/milan_vit-base-p16_16xb256-amp-coslr-400e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/milan/milan_vit-base-p16_16xb256-amp-coslr-400e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb128-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb128-coslr-100e_in1k.py`

 * *Files 7% similar despite different names*

```diff
@@ -82,14 +82,18 @@
         data_prefix='val',
         pipeline=test_pipeline),
     sampler=dict(type='DefaultSampler', shuffle=False),
     persistent_workers=True,
 )
 test_dataloader = val_dataloader
 
+model = dict(
+    backbone=dict(
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')))
+
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(
         type='AdamW',
         lr=5e-4 * (8 * 128 / 256),
         betas=(0.9, 0.999),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mixmim/mixmim_mixmim-base_16xb128-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mixmim/mixmim_mixmim-base_16xb128-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mlp_mixer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mlp_mixer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-large_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-large_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-050_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-050_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-075_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small-075_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilenet_v3/mobilenet-v3-small_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s0_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s0_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s1_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s1_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s2_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s2_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s3_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s3_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobileone/mobileone-s4_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobileone/mobileone-s4_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-small_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-small_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-xsmall_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-xsmall_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mobilevit/mobilevit-xxsmall_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mobilevit/mobilevit-xxsmall_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov2/mocov2_resnet50_8xb32-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov2/mocov2_resnet50_8xb32-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/resnet50_8xb128-linear-coslr-90e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/resnet50_8xb128-linear-coslr-90e_in1k.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,15 +4,19 @@
     '../../_base_/schedules/imagenet_sgd_coslr_100e.py',
     '../../_base_/default_runtime.py',
 ]
 
 # dataset settings
 train_dataloader = dict(batch_size=128)
 
-model = dict(backbone=dict(frozen_stages=4, norm_eval=True))
+model = dict(
+    backbone=dict(
+        frozen_stages=4,
+        norm_eval=True,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')))
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(type='SGD', lr=0.4, momentum=0.9, weight_decay=0.))
 
 # learning rate scheduler
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb128-linear-coslr-90e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb128-linear-coslr-90e_in1k.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,15 +12,16 @@
     backbone=dict(
         type='MoCoV3ViT',
         arch='base',  # embed_dim = 768
         img_size=224,
         patch_size=16,
         stop_grad_conv1=True,
         frozen_stages=12,
-        norm_eval=True),
+        norm_eval=True,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
         in_channels=768,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
         init_cfg=dict(type='Normal', std=0.01, layer='Linear'),
     ))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb64-coslr-150e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-large-p16_8xb64-coslr-100e_in1k.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,24 +4,24 @@
 ]
 
 # model settings
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='VisionTransformer',
-        arch='base',
+        arch='large',
         img_size=224,
         patch_size=16,
-        drop_path_rate=0.1,
-    ),
+        drop_path_rate=0.5,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
-        in_channels=768,
+        in_channels=1024,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
         init_cfg=[
             dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.),
             dict(type='Constant', layer='LayerNorm', val=1., bias=0.),
         ]),
     train_cfg=dict(augments=[
@@ -50,24 +50,24 @@
         type='LinearLR',
         start_factor=1e-3,
         begin=0,
         end=5,
         convert_to_iter_based=True),
     dict(
         type='CosineAnnealingLR',
-        T_max=145,
+        T_max=95,
         eta_min=1e-5,
         by_epoch=True,
         begin=5,
-        end=150,
+        end=100,
         convert_to_iter_based=True)
 ]
 
 # runtime settings
-train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=150)
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=100)
 val_cfg = dict()
 test_cfg = dict()
 
 default_hooks = dict(
     checkpoint=dict(type='CheckpointHook', interval=10, max_keep_ckpts=3))
 custom_hooks = [dict(type='EMAHook', momentum=4e-5, priority='ABOVE_NORMAL')]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-large-p16_8xb64-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-base-p16_8xb64-coslr-150e_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,24 +4,24 @@
 ]
 
 # model settings
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='VisionTransformer',
-        arch='large',
+        arch='base',
         img_size=224,
         patch_size=16,
-        drop_path_rate=0.5,
-    ),
+        drop_path_rate=0.1,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     neck=None,
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
-        in_channels=1024,
+        in_channels=768,
         loss=dict(
             type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
         init_cfg=[
             dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.),
             dict(type='Constant', layer='LayerNorm', val=1., bias=0.),
         ]),
     train_cfg=dict(augments=[
@@ -50,24 +50,24 @@
         type='LinearLR',
         start_factor=1e-3,
         begin=0,
         end=5,
         convert_to_iter_based=True),
     dict(
         type='CosineAnnealingLR',
-        T_max=95,
+        T_max=145,
         eta_min=1e-5,
         by_epoch=True,
         begin=5,
-        end=100,
+        end=150,
         convert_to_iter_based=True)
 ]
 
 # runtime settings
-train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=100)
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=150)
 val_cfg = dict()
 test_cfg = dict()
 
 default_hooks = dict(
     checkpoint=dict(type='CheckpointHook', interval=10, max_keep_ckpts=3))
 custom_hooks = [dict(type='EMAHook', momentum=4e-5, priority='ABOVE_NORMAL')]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/benchmarks/vit-small-p16_8xb128-linear-coslr-90e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/benchmarks/vit-small-p16_8xb128-linear-coslr-90e_in1k.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,15 +12,16 @@
     backbone=dict(
         type='MoCoV3ViT',
         arch='mocov3-small',  # embed_dim = 384
         img_size=224,
         patch_size=16,
         stop_grad_conv1=True,
         frozen_stages=12,
-        norm_eval=True),
+        norm_eval=True,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     head=dict(
         type='VisionTransformerClsHead',
         num_classes=1000,
         in_channels=384,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
         init_cfg=dict(type='Normal', std=0.01, layer='Linear'),
     ))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-100e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-800e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_resnet50_8xb512-amp-coslr-800e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-base-p16_16xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-base-p16_16xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-large-p16_64xb64-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-large-p16_64xb64-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mocov3/mocov3_vit-small-p16_16xb256-amp-coslr-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mocov3/mocov3_vit-small-p16_16xb256-amp-coslr-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-base_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-base_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-large_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-large_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-small_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-small_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/mvit/mvitv2-tiny_8xb256_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/mvit/mvitv2-tiny_8xb256_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_caption.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_refcoco.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_refcoco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_finetuned_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-base_zeroshot_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-base_zeroshot_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/ofa/ofa-large_zeroshot_vqa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/ofa/ofa-large_zeroshot_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-m36_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-m36_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-m48_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-m48_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s12_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s12_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s24_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s24_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/poolformer/poolformer-s36_32xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/poolformer/poolformer-s36_32xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-12gf_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-12gf_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-3.2gf_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-3.2gf_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-4.0gf_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-4.0gf_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-400mf_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-400mf_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-6.4gf_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-6.4gf_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/regnet/regnetx-8.0gf_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/regnet/regnetx-8.0gf_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/replknet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/replknet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k-256px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k-256px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repmlp/repmlp-base_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-A0_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-A0_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-B3_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/van/van-small_8xb128_in1k.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,67 +1,65 @@
 _base_ = [
-    '../_base_/models/repvgg-B3_lbs-mixup_in1k.py',
-    '../_base_/datasets/imagenet_bs32_pil_resize.py',
-    '../_base_/schedules/imagenet_bs256_coslr.py',
+    '../_base_/models/van/van_small.py',
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../_base_/default_runtime.py'
 ]
 
-# schedule settings
-optim_wrapper = dict(
-    paramwise_cfg=dict(
-        bias_decay_mult=0.0,
-        custom_keys={
-            'branch_3x3.norm': dict(decay_mult=0.0),
-            'branch_1x1.norm': dict(decay_mult=0.0),
-            'branch_norm.bias': dict(decay_mult=0.0),
-        }))
-
+# dataset setting
 data_preprocessor = dict(
-    # RGB format normalization parameters
-    mean=[123.675, 116.28, 103.53],
-    std=[58.395, 57.12, 57.375],
+    mean=[127.5, 127.5, 127.5],
+    std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='RandomResizedCrop', scale=224, backend='pillow'),
+    dict(
+        type='RandomResizedCrop',
+        scale=224,
+        backend='pillow',
+        interpolation='bicubic'),
     dict(type='RandomFlip', prob=0.5, direction='horizontal'),
     dict(
         type='RandAugment',
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
-        magnitude_level=7,
+        magnitude_level=9,
         magnitude_std=0.5,
-        hparams=dict(pad_val=[round(x) for x in bgr_mean])),
+        hparams=dict(
+            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
+    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
+    dict(
+        type='RandomErasing',
+        erase_prob=0.25,
+        mode='rand',
+        min_area_ratio=0.02,
+        max_area_ratio=1 / 3,
+        fill_color=bgr_mean,
+        fill_std=bgr_std),
     dict(type='PackInputs'),
 ]
 
 test_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='ResizeEdge', scale=256, edge='short', backend='pillow'),
+    dict(
+        type='ResizeEdge',
+        scale=248,
+        edge='short',
+        backend='pillow',
+        interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 
 # schedule settings
-param_scheduler = dict(
-    type='CosineAnnealingLR',
-    T_max=200,
-    by_epoch=True,
-    begin=0,
-    end=200,
-    convert_to_iter_based=True)
-
-train_cfg = dict(by_epoch=True, max_epochs=200)
-
-default_hooks = dict(
-    checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3))
+optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/repvgg/repvgg-D2se_8xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/repvgg/repvgg-D2se_8xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/res2net/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/res2net/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/_randaug_policies.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/_randaug_policies.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest101_32xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest101_32xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest200_64xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest200_64xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest269_64xb32_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest269_64xb32_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnest/resnest50_32xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnest/resnest50_32xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a1-600e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a1-600e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a2-300e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a2-300e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a3-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb256-rsb-a3-100e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnet/resnet50_8xb8_cub.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnet/resnet50_8xb8_cub.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/resnext/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/resnext/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/revvit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/revvit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/metafile.yml`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 Collections:
   - Name: RIFormer
     Metadata:
       Training Data: ImageNet-1k
+      Training Resources: 8x A100 GPUs
       Architecture:
         - Affine
         - 1x1 Convolution
         - LayerScale
     Paper:
       URL: https://arxiv.org/abs/xxxx.xxxxx
       Title: "RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer"
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m36_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m36_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m36_8xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m36_8xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-m48_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s12_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s24_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s36_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s36_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/riformer/riformer-s36_8xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/riformer/riformer-s36_8xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/sam/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/sam/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/seresnet/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/seresnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v1/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v1/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/shufflenet_v2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/shufflenet_v2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-800e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_16xb256-coslr-800e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simclr/simclr_resnet50_8xb32-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simclr/simclr_resnet50_8xb32-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w6_8xb256-coslr-100e_in1k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w6_8xb256-coslr-100e_in1k-192px.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 ]
 
 # model settings
 model = dict(
     backbone=dict(
         img_size=192,
         drop_path_rate=0.1,
-        stage_cfgs=dict(block_cfgs=dict(window_size=6))))
+        stage_cfgs=dict(block_cfgs=dict(window_size=6)),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')))
 
 # optimizer settings
 optim_wrapper = dict(
     type='AmpOptimWrapper',
     optimizer=dict(type='AdamW', lr=5e-3, weight_decay=0.05),
     clip_grad=dict(max_norm=5.0),
     constructor='LearningRateDecayOptimWrapperConstructor',
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w7_8xb256-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w7_8xb256-coslr-100e_in1k.py`

 * *Files 6% similar despite different names*

```diff
@@ -48,15 +48,16 @@
 test_dataloader = val_dataloader
 
 # model settings
 model = dict(
     backbone=dict(
         img_size=224,
         drop_path_rate=0.1,
-        stage_cfgs=dict(block_cfgs=dict(window_size=7))))
+        stage_cfgs=dict(block_cfgs=dict(window_size=7)),
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')))
 
 # optimizer settings
 optim_wrapper = dict(
     type='AmpOptimWrapper',
     optimizer=dict(type='AdamW', lr=5e-3, weight_decay=0.05),
     clip_grad=dict(max_norm=5.0),
     constructor='LearningRateDecayOptimWrapperConstructor',
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/benchmarks/swin-large-w14_8xb256-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/benchmarks/swin-large-w14_8xb256-coslr-100e_in1k.py`

 * *Files 3% similar despite different names*

```diff
@@ -50,15 +50,16 @@
 # model settings
 model = dict(
     backbone=dict(
         arch='large',
         img_size=224,
         drop_path_rate=0.2,
         stage_cfgs=dict(block_cfgs=dict(window_size=14)),
-        pad_small_map=True),
+        pad_small_map=True,
+        init_cfg=dict(type='Pretrained', checkpoint='', prefix='backbone.')),
     head=dict(in_channels=1536))
 
 # optimizer settings
 optim_wrapper = dict(
     type='AmpOptimWrapper',
     optimizer=dict(type='AdamW', lr=5e-3, weight_decay=0.05),
     clip_grad=dict(max_norm=5.0),
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-800e_in1k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_16xb128-amp-coslr-800e_in1k-192px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_8xb256-amp-coslr-100e_in1k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-base-w6_8xb256-amp-coslr-100e_in1k-192px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simmim/simmim_swin-large-w12_16xb128-amp-coslr-800e_in1k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simmim/simmim_swin-large-w12_16xb128-amp-coslr-800e_in1k-192px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-100e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-100e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-200e_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-200e_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swav/swav_resnet50_8xb32-mcrop-coslr-200e_in1k-224px-96px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swav/swav_resnet50_8xb32-mcrop-coslr-200e_in1k-224px-96px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer/swin-large_8xb8_cub-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer/swin-large_8xb8_cub-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w12_8xb128_in21k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-base-w12_8xb128_in21k-192px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w12_8xb128_in21k-192px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/swin_transformer_v2/swinv2-large-w12_8xb128_in21k-192px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-14_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-14_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-19_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-19_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-24_8xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/t2t_vit/t2t-vit-t-24_8xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-512px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/tinyvit/tinyvit-21m-distill_8xb256_in1k-512px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tnt/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/tnt/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/tnt/tnt-s-p16_16xb64_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/tnt/tnt-s-p16_16xb64_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-pcpvt-base_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-pcpvt-base_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/twins/twins-svt-base_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/twins/twins-svt-base_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/van/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-base_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/spark/spark_sparse-convnextv2-tiny_16xb256-amp-coslr-800e_in1k.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,84 @@
 _base_ = [
-    '../_base_/models/van/van_base.py',
-    '../_base_/datasets/imagenet_bs64_swin_224.py',
-    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
+    '../_base_/datasets/imagenet_bs512_mae.py',
     '../_base_/default_runtime.py',
 ]
 
-# dataset setting
-data_preprocessor = dict(
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
-    # convert image from BGR to RGB
-    to_rgb=True,
-)
+# dataset 16 x 256
+train_dataloader = dict(batch_size=256, num_workers=8)
 
-bgr_mean = data_preprocessor['mean'][::-1]
-bgr_std = data_preprocessor['std'][::-1]
+# model settings, use ConvNeXt V2
+model = dict(
+    type='SparK',
+    input_size=224,
+    downsample_raito=32,
+    mask_ratio=0.6,
+    enc_dec_norm_cfg=dict(type='SparseLN2d', eps=1e-6),
+    enc_dec_norm_dim=768,
+    backbone=dict(
+        type='SparseConvNeXt',
+        arch='tiny',
+        drop_path_rate=0.2,
+        out_indices=(0, 1, 2, 3),
+        gap_before_output=False,
+        layer_scale_init_value=0.,
+        use_grn=True,
+    ),
+    neck=dict(
+        type='SparKLightDecoder',
+        feature_dim=512,
+        upsample_ratio=32,  # equal to downsample_raito
+        mid_channels=0,
+        last_act=False),
+    head=dict(
+        type='SparKPretrainHead',
+        loss=dict(type='PixelReconstructionLoss', criterion='L2')))
+
+# optimizer wrapper
+optimizer = dict(
+    type='Lamb', lr=2e-4 * 4096 / 512, betas=(0.9, 0.95), weight_decay=0.04)
+optim_wrapper = dict(
+    type='AmpOptimWrapper',
+    optimizer=optimizer,
+    clip_grad=dict(max_norm=5.0),
+    paramwise_cfg=dict(
+        bias_decay_mult=0.0,
+        flat_decay_mult=0.0,
+        custom_keys={
+            'mask_token': dict(decay_mult=0.),
+        }))
 
-train_pipeline = [
-    dict(type='LoadImageFromFile'),
+# learning rate scheduler
+param_scheduler = [
     dict(
-        type='RandomResizedCrop',
-        scale=224,
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
+        type='LinearLR',
+        start_factor=1e-4,
+        by_epoch=True,
+        begin=0,
+        end=20,
+        convert_to_iter_based=True),
     dict(
-        type='RandAugment',
-        policies='timm_increasing',
-        num_policies=2,
-        total_level=10,
-        magnitude_level=9,
-        magnitude_std=0.5,
-        hparams=dict(
-            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
+        type='CosineAnnealingLR',
+        T_max=780,
+        by_epoch=True,
+        begin=20,
+        end=800,
+        convert_to_iter_based=True),
     dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
-    dict(type='PackInputs'),
+        type='CosineAnnealingWeightDecay',
+        eta_min=0.2,
+        T_max=800,
+        by_epoch=True,
+        begin=0,
+        end=800,
+        convert_to_iter_based=True)
 ]
 
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(
-        type='ResizeEdge',
-        scale=248,
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs'),
-]
-
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+# runtime settings
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=800)
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=100),
+    # only keeps the latest 3 checkpoints
+    checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=2))
 
-# schedule settings
-optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
+# randomness
+randomness = dict(seed=0, diff_rank_seed=True)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-large_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_t2t_224.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,14 @@
-_base_ = [
-    '../_base_/models/van/van_large.py',
-    '../_base_/datasets/imagenet_bs64_swin_224.py',
-    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
-    '../_base_/default_runtime.py'
-]
-
-# dataset setting
+# dataset settings
+dataset_type = 'ImageNet'
 data_preprocessor = dict(
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
+    num_classes=1000,
+    # RGB format normalization parameters
+    mean=[123.675, 116.28, 103.53],
+    std=[58.395, 57.12, 57.375],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
 bgr_std = data_preprocessor['std'][::-1]
 
@@ -29,15 +25,14 @@
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
         magnitude_level=9,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
         mode='rand',
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
         fill_color=bgr_mean,
@@ -53,13 +48,33 @@
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+train_dataloader = dict(
+    batch_size=64,
+    num_workers=5,
+    dataset=dict(
+        type=dataset_type,
+        data_root='data/imagenet',
+        split='train',
+        pipeline=train_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=True),
+)
+
+val_dataloader = dict(
+    batch_size=64,
+    num_workers=5,
+    dataset=dict(
+        type=dataset_type,
+        data_root='data/imagenet',
+        split='val',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
+)
+val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
-# schedule settings
-optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
+# If you want standard test, please manually configure the test dataset
+test_dataloader = val_dataloader
+test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-small_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_vig_224.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,16 +1,12 @@
-_base_ = [
-    '../_base_/models/van/van_small.py',
-    '../_base_/datasets/imagenet_bs64_swin_224.py',
-    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
-    '../_base_/default_runtime.py'
-]
-
-# dataset setting
+# dataset settings
+dataset_type = 'ImageNet'
 data_preprocessor = dict(
+    num_classes=1000,
+    # RGB format normalization parameters
     mean=[127.5, 127.5, 127.5],
     std=[127.5, 127.5, 127.5],
     # convert image from BGR to RGB
     to_rgb=True,
 )
 
 bgr_mean = data_preprocessor['mean'][::-1]
@@ -29,15 +25,14 @@
         policies='timm_increasing',
         num_policies=2,
         total_level=10,
         magnitude_level=9,
         magnitude_std=0.5,
         hparams=dict(
             pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
     dict(
         type='RandomErasing',
         erase_prob=0.25,
         mode='rand',
         min_area_ratio=0.02,
         max_area_ratio=1 / 3,
         fill_color=bgr_mean,
@@ -53,13 +48,33 @@
         edge='short',
         backend='pillow',
         interpolation='bicubic'),
     dict(type='CenterCrop', crop_size=224),
     dict(type='PackInputs'),
 ]
 
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+train_dataloader = dict(
+    batch_size=128,
+    num_workers=5,
+    dataset=dict(
+        type=dataset_type,
+        data_root='data/imagenet',
+        split='train',
+        pipeline=train_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=True),
+)
+
+val_dataloader = dict(
+    batch_size=128,
+    num_workers=5,
+    dataset=dict(
+        type=dataset_type,
+        data_root='data/imagenet',
+        split='val',
+        pipeline=test_pipeline),
+    sampler=dict(type='DefaultSampler', shuffle=False),
+)
+val_evaluator = dict(type='Accuracy', topk=(1, 5))
 
-# schedule settings
-optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
+# If you want standard test, please manually configure the test dataset
+test_dataloader = val_dataloader
+test_evaluator = val_evaluator
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/van/van-tiny_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_32xb128-mae_in1k.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,65 +1,58 @@
 _base_ = [
-    '../_base_/models/van/van_tiny.py',
     '../_base_/datasets/imagenet_bs64_swin_224.py',
     '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../_base_/default_runtime.py'
 ]
 
-# dataset setting
-data_preprocessor = dict(
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
-    # convert image from BGR to RGB
-    to_rgb=True,
-)
+# model settings
+model = dict(
+    type='ImageClassifier',
+    backbone=dict(
+        type='VisionTransformer',
+        arch='base',
+        img_size=224,
+        patch_size=16,
+        drop_path_rate=0.1),
+    neck=None,
+    head=dict(
+        type='VisionTransformerClsHead',
+        num_classes=1000,
+        in_channels=768,
+        loss=dict(
+            type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
+    ),
+    init_cfg=[
+        dict(type='TruncNormal', layer='Linear', std=.02),
+        dict(type='Constant', layer='LayerNorm', val=1., bias=0.),
+    ],
+    train_cfg=dict(augments=[
+        dict(type='Mixup', alpha=0.8),
+        dict(type='CutMix', alpha=1.0)
+    ]))
 
-bgr_mean = data_preprocessor['mean'][::-1]
-bgr_std = data_preprocessor['std'][::-1]
-
-train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(
-        type='RandomResizedCrop',
-        scale=224,
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(
-        type='RandAugment',
-        policies='timm_increasing',
-        num_policies=2,
-        total_level=10,
-        magnitude_level=9,
-        magnitude_std=0.5,
-        hparams=dict(
-            pad_val=[round(x) for x in bgr_mean], interpolation='bicubic')),
-    dict(type='ColorJitter', brightness=0.4, contrast=0.4, saturation=0.4),
-    dict(
-        type='RandomErasing',
-        erase_prob=0.25,
-        mode='rand',
-        min_area_ratio=0.02,
-        max_area_ratio=1 / 3,
-        fill_color=bgr_mean,
-        fill_std=bgr_std),
-    dict(type='PackInputs'),
-]
-
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(
-        type='ResizeEdge',
-        scale=248,
-        edge='short',
-        backend='pillow',
-        interpolation='bicubic'),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='PackInputs'),
-]
-
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline), batch_size=128)
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+# dataset settings
+train_dataloader = dict(batch_size=128)
 
 # schedule settings
-optim_wrapper = dict(clip_grad=dict(max_norm=5.0))
+optim_wrapper = dict(
+    optimizer=dict(
+        type='AdamW',
+        lr=1e-4 * 4096 / 256,
+        weight_decay=0.3,
+        eps=1e-8,
+        betas=(0.9, 0.95)),
+    paramwise_cfg=dict(
+        norm_decay_mult=0.0,
+        bias_decay_mult=0.0,
+        custom_keys={
+            '.cls_token': dict(decay_mult=0.0),
+            '.pos_embed': dict(decay_mult=0.0)
+        }))
+
+# runtime settings
+custom_hooks = [dict(type='EMAHook', momentum=1e-4)]
+
+# NOTE: `auto_scale_lr` is for automatically scaling LR
+# based on the actual training batch size.
+# base_batch_size = (32 GPUs) x (128 samples per GPU)
+auto_scale_lr = dict(base_batch_size=4096)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vgg/vgg16_8xb16_voc.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vgg/vgg16_8xb16_voc.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vig/pvig-base_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vig/pvig-base_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_32xb128-mae_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-400e_in1k.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,58 +1,56 @@
 _base_ = [
-    '../_base_/datasets/imagenet_bs64_swin_224.py',
-    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
-    '../_base_/default_runtime.py'
+    '../_base_/models/itpn_hivit-base-p16.py',
+    '../_base_/datasets/imagenet_bs512_mae.py',
+    '../_base_/default_runtime.py',
 ]
 
-# model settings
-model = dict(
-    type='ImageClassifier',
-    backbone=dict(
-        type='VisionTransformer',
-        arch='base',
-        img_size=224,
-        patch_size=16,
-        drop_path_rate=0.1),
-    neck=None,
-    head=dict(
-        type='VisionTransformerClsHead',
-        num_classes=1000,
-        in_channels=768,
-        loss=dict(
-            type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
-    ),
-    init_cfg=[
-        dict(type='TruncNormal', layer='Linear', std=.02),
-        dict(type='Constant', layer='LayerNorm', val=1., bias=0.),
-    ],
-    train_cfg=dict(augments=[
-        dict(type='Mixup', alpha=0.8),
-        dict(type='CutMix', alpha=1.0)
-    ]))
-
-# dataset settings
-train_dataloader = dict(batch_size=128)
-
-# schedule settings
+# optimizer wrapper
 optim_wrapper = dict(
+    type='AmpOptimWrapper',
+    loss_scale='dynamic',
     optimizer=dict(
         type='AdamW',
-        lr=1e-4 * 4096 / 256,
-        weight_decay=0.3,
-        eps=1e-8,
-        betas=(0.9, 0.95)),
+        lr=1.5e-4 * 4096 / 256,
+        betas=(0.9, 0.95),
+        weight_decay=0.05),
     paramwise_cfg=dict(
-        norm_decay_mult=0.0,
-        bias_decay_mult=0.0,
         custom_keys={
-            '.cls_token': dict(decay_mult=0.0),
-            '.pos_embed': dict(decay_mult=0.0)
+            'norm': dict(decay_mult=0.0),
+            'bias': dict(decay_mult=0.0),
+            'pos_embed': dict(decay_mult=0.),
+            'mask_token': dict(decay_mult=0.),
         }))
 
+# learning rate scheduler
+param_scheduler = [
+    dict(
+        type='LinearLR',
+        start_factor=1e-4,
+        by_epoch=True,
+        begin=0,
+        end=40,
+        convert_to_iter_based=True),
+    dict(
+        type='CosineAnnealingLR',
+        T_max=360,
+        by_epoch=True,
+        begin=40,
+        end=400,
+        convert_to_iter_based=True)
+]
+
 # runtime settings
-custom_hooks = [dict(type='EMAHook', momentum=1e-4)]
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=400)
+default_hooks = dict(
+    # only keeps the latest 3 checkpoints
+    checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3))
+
+randomness = dict(seed=0, diff_rank_seed=True)
+
+# auto resume
+resume = True
+find_unused_parameters = True
 
 # NOTE: `auto_scale_lr` is for automatically scaling LR
 # based on the actual training batch size.
-# base_batch_size = (32 GPUs) x (128 samples per GPU)
 auto_scale_lr = dict(base_batch_size=4096)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_4xb544-ipu_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_4xb544-ipu_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k-384px.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = [
-    '../_base_/models/vit-base-p32.py',
+    '../_base_/models/vit-large-p16.py',
     '../_base_/datasets/imagenet_bs64_pil_resize.py',
     '../_base_/schedules/imagenet_bs4096_AdamW.py',
     '../_base_/default_runtime.py'
 ]
 
 # model setting
 model = dict(backbone=dict(img_size=384))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/_base_/datasets/imagenet_bs512_mae.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,38 +1,32 @@
-_base_ = [
-    '../_base_/models/vit-large-p16.py',
-    '../_base_/datasets/imagenet_bs64_pil_resize.py',
-    '../_base_/schedules/imagenet_bs4096_AdamW.py',
-    '../_base_/default_runtime.py'
-]
-
-# model setting
-model = dict(backbone=dict(img_size=384))
-
-# dataset setting
+# dataset settings
+dataset_type = 'ImageNet'
+data_root = 'data/imagenet/'
 data_preprocessor = dict(
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
-    # convert image from BGR to RGB
-    to_rgb=True,
-)
+    type='SelfSupDataPreprocessor',
+    mean=[123.675, 116.28, 103.53],
+    std=[58.395, 57.12, 57.375],
+    to_rgb=True)
 
 train_pipeline = [
     dict(type='LoadImageFromFile'),
-    dict(type='RandomResizedCrop', scale=384, backend='pillow'),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='PackInputs'),
+    dict(
+        type='RandomResizedCrop',
+        scale=224,
+        crop_ratio_range=(0.2, 1.0),
+        backend='pillow',
+        interpolation='bicubic'),
+    dict(type='RandomFlip', prob=0.5),
+    dict(type='PackInputs')
 ]
 
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='ResizeEdge', scale=384, edge='short', backend='pillow'),
-    dict(type='CenterCrop', crop_size=384),
-    dict(type='PackInputs'),
-]
-
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-
-# schedule setting
-optim_wrapper = dict(clip_grad=dict(max_norm=1.0))
+train_dataloader = dict(
+    batch_size=512,
+    num_workers=8,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    collate_fn=dict(type='default_collate'),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        split='train',
+        pipeline=train_pipeline))
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,38 +1,34 @@
 _base_ = [
-    '../_base_/models/vit-large-p32.py',
-    '../_base_/datasets/imagenet_bs64_pil_resize_autoaug.py',
-    '../_base_/schedules/imagenet_bs4096_AdamW.py',
-    '../_base_/default_runtime.py'
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
+    '../_base_/default_runtime.py',
 ]
 
-# model setting
-model = dict(backbone=dict(img_size=384))
-
-# dataset setting
-data_preprocessor = dict(
-    mean=[127.5, 127.5, 127.5],
-    std=[127.5, 127.5, 127.5],
-    # convert image from BGR to RGB
-    to_rgb=True,
+model = dict(
+    type='ImageClassifier',
+    backbone=dict(
+        type='XCiT',
+        patch_size=8,
+        embed_dims=768,
+        depth=24,
+        num_heads=16,
+        mlp_ratio=4,
+        qkv_bias=True,
+        layer_scale_init_value=1e-5,
+        tokens_norm=True,
+        out_type='cls_token',
+    ),
+    head=dict(
+        type='LinearClsHead',
+        num_classes=1000,
+        in_channels=768,
+        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
+    ),
+    train_cfg=dict(augments=[
+        dict(type='Mixup', alpha=0.8),
+        dict(type='CutMix', alpha=1.0),
+    ]),
 )
 
-train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='RandomResizedCrop', scale=384, backend='pillow'),
-    dict(type='RandomFlip', prob=0.5, direction='horizontal'),
-    dict(type='PackInputs'),
-]
-
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='ResizeEdge', scale=384, edge='short', backend='pillow'),
-    dict(type='CenterCrop', crop_size=384),
-    dict(type='PackInputs'),
-]
-
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-
-# schedule setting
-optim_wrapper = dict(clip_grad=dict(max_norm=1.0))
+# dataset settings
+train_dataloader = dict(batch_size=128)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/wrn/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/wrn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/metafile.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-large-24-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,27 +5,27 @@
 ]
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='XCiT',
         patch_size=8,
-        embed_dims=768,
+        embed_dims=512,
         depth=24,
-        num_heads=16,
+        num_heads=8,
         mlp_ratio=4,
         qkv_bias=True,
         layer_scale_init_value=1e-5,
         tokens_norm=True,
         out_type='cls_token',
     ),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
-        in_channels=768,
+        in_channels=512,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
     ),
     train_cfg=dict(augments=[
         dict(type='Mixup', alpha=0.8),
         dict(type='CutMix', alpha=1.0),
     ]),
 )
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-medium-24-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,27 +5,27 @@
 ]
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='XCiT',
         patch_size=8,
-        embed_dims=512,
-        depth=24,
-        num_heads=8,
+        embed_dims=128,
+        depth=12,
+        num_heads=4,
         mlp_ratio=4,
         qkv_bias=True,
-        layer_scale_init_value=1e-5,
-        tokens_norm=True,
+        layer_scale_init_value=1.0,
+        tokens_norm=False,
         out_type='cls_token',
     ),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
-        in_channels=512,
+        in_channels=128,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
     ),
     train_cfg=dict(augments=[
         dict(type='Mixup', alpha=0.8),
         dict(type='CutMix', alpha=1.0),
     ]),
 )
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-nano-12-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k-384px.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 _base_ = [
-    '../_base_/datasets/imagenet_bs64_swin_224.py',
+    '../_base_/datasets/imagenet_bs64_swin_384.py',
     '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../_base_/default_runtime.py',
 ]
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='XCiT',
-        patch_size=8,
-        embed_dims=128,
+        patch_size=16,
+        embed_dims=192,
         depth=12,
         num_heads=4,
         mlp_ratio=4,
         qkv_bias=True,
         layer_scale_init_value=1.0,
-        tokens_norm=False,
+        tokens_norm=True,
         out_type='cls_token',
     ),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
-        in_channels=128,
+        in_channels=192,
         loss=dict(type='CrossEntropyLoss', loss_weight=1.0),
     ),
     train_cfg=dict(augments=[
         dict(type='Mixup', alpha=0.8),
         dict(type='CutMix', alpha=1.0),
     ]),
 )
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-12-p8_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-small-24-p8_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = [
-    '../_base_/datasets/imagenet_bs64_swin_384.py',
+    '../_base_/datasets/imagenet_bs64_swin_224.py',
     '../_base_/schedules/imagenet_bs1024_adamw_swin.py',
     '../_base_/default_runtime.py',
 ]
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
     '../_base_/default_runtime.py',
 ]
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='XCiT',
-        patch_size=16,
+        patch_size=8,
         embed_dims=192,
         depth=12,
         num_heads=4,
         mlp_ratio=4,
         qkv_bias=True,
         layer_scale_init_value=1.0,
         tokens_norm=True,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-12-p8_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,19 +6,19 @@
 
 model = dict(
     type='ImageClassifier',
     backbone=dict(
         type='XCiT',
         patch_size=8,
         embed_dims=192,
-        depth=12,
+        depth=24,
         num_heads=4,
         mlp_ratio=4,
         qkv_bias=True,
-        layer_scale_init_value=1.0,
+        layer_scale_init_value=1e-5,
         tokens_norm=True,
         out_type='cls_token',
     ),
     head=dict(
         type='LinearClsHead',
         num_classes=1000,
         in_channels=192,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p16_8xb128_in1k.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k-384px.py` & `mmpretrain-1.0.1/mmpretrain/.mim/configs/xcit/xcit-tiny-24-p8_8xb128_in1k-384px.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/model-index.yml` & `mmpretrain-1.0.1/mmpretrain/.mim/model-index.yml`

 * *Files 3% similar despite different names*

```diff
@@ -72,7 +72,13 @@
   - configs/glip/metafile.yml
   - configs/eva02/metafile.yml
   - configs/dinov2/metafile.yml
   - configs/blip/metafile.yml
   - configs/flamingo/metafile.yml
   - configs/blip2/metafile.yml
   - configs/chinese_clip/metafile.yml
+  - configs/itpn/metafile.yml
+  - configs/hivit/metafile.yml
+  - configs/spark/metafile.yml
+  - configs/minigpt4/metafile.yml
+  - configs/llava/metafile.yml
+  - configs/otter/metafile.yml
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/analyze_logs.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/analyze_logs.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/analyze_results.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/analyze_results.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/confusion_matrix.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/confusion_matrix.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/eval_metric.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/eval_metric.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/analysis_tools/get_flops.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/analysis_tools/get_flops.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_c4.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_c4.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_c4.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_c4.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_fpn.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_fpn.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_train.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_train.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dataset_converters/convert_imagenet_subsets.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/convert_imagenet_subsets.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/dataset_converters/convert_inaturalist.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/dataset_converters/convert_inaturalist.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/kfold-cross-valid.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/kfold-cross-valid.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/misc/print_config.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/misc/print_config.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/misc/verify_dataset.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/misc/verify_dataset.py`

 * *Files 9% similar despite different names*

```diff
@@ -48,17 +48,20 @@
 
 class DatasetValidator():
     """the dataset tool class to check if all file are broken."""
 
     def __init__(self, dataset_cfg, log_file_path):
         super(DatasetValidator, self).__init__()
         # keep only LoadImageFromFile pipeline
-        assert dataset_cfg.pipeline[0]['type'] == 'LoadImageFromFile', (
-            'This tool is only for datasets needs to load image from files.')
-        self.pipeline = TRANSFORMS.build(dataset_cfg.pipeline[0])
+        from mmpretrain.datasets import get_transform_idx
+
+        load_idx = get_transform_idx(dataset_cfg.pipeline, 'LoadImageFromFile')
+        assert load_idx >= 0, \
+            'This tool is only for datasets needs to load image from files.'
+        self.pipeline = TRANSFORMS.build(dataset_cfg.pipeline[load_idx])
         dataset_cfg.pipeline = []
         dataset = build_dataset(dataset_cfg)
 
         self.dataset = dataset
         self.log_file_path = log_file_path
 
     def valid_idx(self, idx):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/clip_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/clip_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/convnext_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/convnext_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/davit_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/davit_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/deit3_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/deit3_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/edgenext_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/edgenext_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/efficientnet_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/efficientnet_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/efficientnetv2_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/efficientnetv2_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/eva02_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/eva02_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/eva_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/eva_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/glip_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/glip_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/hornet2mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/hornet2mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/levit2mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/levit2mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mixmim_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mixmim_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mlpmixer_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mlpmixer_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/mobilenetv2_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/mobilenetv2_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/ofa.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/ofa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/publish_model.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/publish_model.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/reparameterize_model.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/reparameterize_model.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/replknet_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/replknet_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/repvgg_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/repvgg_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/revvit_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/revvit_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/shufflenetv2_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/shufflenetv2_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/tinyvit_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/tinyvit_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/torchvision_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/torchvision_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/twins2mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/twins2mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/van2mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/van2mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/vgg_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/vgg_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/model_converters/vig_to_mmpretrain.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/model_converters/vig_to_mmpretrain.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/slurm_test.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/slurm_test.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/slurm_train.sh` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/slurm_train.sh`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/test.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/test.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import os
 import os.path as osp
 from copy import deepcopy
 
 import mmengine
 from mmengine.config import Config, ConfigDict, DictAction
 from mmengine.evaluator import DumpResults
+from mmengine.registry import RUNNERS
 from mmengine.runner import Runner
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description='MMPreTrain test (and eval) a model')
     parser.add_argument('config', help='test config file path')
@@ -165,15 +166,21 @@
     # load config
     cfg = Config.fromfile(args.config)
 
     # merge cli arguments to config
     cfg = merge_args(cfg, args)
 
     # build the runner from config
-    runner = Runner.from_cfg(cfg)
+    if 'runner_type' not in cfg:
+        # build the default runner
+        runner = Runner.from_cfg(cfg)
+    else:
+        # build customized runner from the registry
+        # if 'runner_type' is set in the cfg
+        runner = RUNNERS.build(cfg)
 
     if args.out and args.out_item in ['pred', None]:
         runner.test_evaluator.metrics.append(
             DumpResults(out_file_path=args.out))
 
     # start testing
     metrics = runner.test()
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/mmpretrain2torchserve.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/mmpretrain2torchserve.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/mmpretrain_handler.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/mmpretrain_handler.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/torchserve/test_torchserver.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/torchserve/test_torchserver.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/train.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/train.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
 from copy import deepcopy
 
 from mmengine.config import Config, ConfigDict, DictAction
+from mmengine.registry import RUNNERS
 from mmengine.runner import Runner
 from mmengine.utils import digit_version
 from mmengine.utils.dl_utils import TORCH_VERSION
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a model')
@@ -145,15 +146,21 @@
     # load config
     cfg = Config.fromfile(args.config)
 
     # merge cli arguments to config
     cfg = merge_args(cfg, args)
 
     # build the runner from config
-    runner = Runner.from_cfg(cfg)
+    if 'runner_type' not in cfg:
+        # build the default runner
+        runner = Runner.from_cfg(cfg)
+    else:
+        # build customized runner from the registry
+        # if 'runner_type' is set in the cfg
+        runner = RUNNERS.build(cfg)
 
     # start training
     runner.train()
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/browse_dataset.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/browse_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_cam.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_cam.py`

 * *Files 0% similar despite different names*

```diff
@@ -179,15 +179,15 @@
 
     if out_path:
         mmcv.imwrite(visualization_img, str(out_path))
     else:
         mmcv.imshow(visualization_img, win_name=title)
 
 
-def get_default_traget_layers(model, args):
+def get_default_target_layers(model, args):
     """get default target layers from given model, here choose nrom type layer
     as default target layer."""
     norm_layers = [
         (name, layer)
         for name, layer in model.backbone.named_modules(prefix='backbone')
         if is_norm(layer)
     ]
@@ -195,15 +195,16 @@
         # For ViT models, the final classification is done on the class token.
         # And the patch tokens and class tokens won't interact each other after
         # the final attention layer. Therefore, we need to choose the norm
         # layer before the last attention layer.
         num_extra_tokens = args.num_extra_tokens or getattr(
             model.backbone, 'num_extra_tokens', 1)
 
-        out_type = getattr(model.backbone, 'out_type')
+        # models like swin have no attr 'out_type', set out_type to avg_featmap
+        out_type = getattr(model.backbone, 'out_type', 'avg_featmap')
         if out_type == 'cls_token' or num_extra_tokens > 0:
             # Assume the backbone feature is class token.
             name, layer = norm_layers[-3]
             print('Automatically choose the last norm layer before the '
                   f'final attention block "{name}" as the target layer.')
             return [layer]
 
@@ -236,15 +237,15 @@
 
     # build target layers
     if args.target_layers:
         target_layers = [
             get_layer(layer, model) for layer in args.target_layers
         ]
     else:
-        target_layers = get_default_traget_layers(model, args)
+        target_layers = get_default_target_layers(model, args)
 
     # init a cam grad calculator
     use_cuda = ('cuda' in args.device)
     cam = init_cam(args.method, model, target_layers, use_cuda,
                    partial(reshape_transform, model=model, args=args))
 
     # warp the target_category with ClassifierOutputTarget in grad_cam>=1.3.7,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_scheduler.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_scheduler.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/.mim/tools/visualization/vis_tsne.py` & `mmpretrain-1.0.1/mmpretrain/.mim/tools/visualization/vis_tsne.py`

 * *Files 4% similar despite different names*

```diff
@@ -208,14 +208,17 @@
             # post process
             if batch_features[0].ndim == 4:
                 # For (N, C, H, W) feature
                 batch_features = [
                     F.adaptive_avg_pool2d(inputs, 1).squeeze()
                     for inputs in batch_features
                 ]
+            elif batch_features[0].ndim == 3:
+                # For (N, L, C) feature
+                batch_features = [inputs.mean(1) for inputs in batch_features]
 
         # save batch features
         features.append(batch_features)
         labels.extend(batch_labels.cpu().numpy())
 
     for i in range(len(features[0])):
         key = 'feat_' + str(model.backbone.out_indices[i])
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/__init__.py` & `mmpretrain-1.0.1/mmpretrain/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,19 +2,19 @@
 import mmcv
 import mmengine
 from mmengine.utils import digit_version
 
 from .apis import *  # noqa: F401, F403
 from .version import __version__
 
-mmcv_minimum_version = '2.0.0rc4'
+mmcv_minimum_version = '2.0.0'
 mmcv_maximum_version = '2.1.0'
 mmcv_version = digit_version(mmcv.__version__)
 
-mmengine_minimum_version = '0.7.1'
+mmengine_minimum_version = '0.8.3'
 mmengine_maximum_version = '1.0.0'
 mmengine_version = digit_version(mmengine.__version__)
 
 assert (mmcv_version >= digit_version(mmcv_minimum_version)
         and mmcv_version < digit_version(mmcv_maximum_version)), \
     f'MMCV=={mmcv.__version__} is used but incompatible. ' \
     f'Please install mmcv>={mmcv_minimum_version}, <{mmcv_maximum_version}.'
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/__init__.py` & `mmpretrain-1.0.1/mmpretrain/apis/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/base.py` & `mmpretrain-1.0.1/mmpretrain/apis/base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/feature_extractor.py` & `mmpretrain-1.0.1/mmpretrain/apis/feature_extractor.py`

 * *Files 2% similar despite different names*

```diff
@@ -79,17 +79,19 @@
         for i in range(inputs.shape[0]):
             results.append(scatter(outputs, i))
 
         return results
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[InputType], batch_size: int = 1):
 
         def load_image(input_):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/image_caption.py` & `mmpretrain-1.0.1/mmpretrain/apis/image_caption.py`

 * *Files 4% similar despite different names*

```diff
@@ -66,17 +66,19 @@
             list: The inference results.
         """
         return super().__call__(images, return_datasamples, batch_size,
                                 **kwargs)
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[InputType], batch_size: int = 1):
 
         def load_image(input_):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/image_classification.py` & `mmpretrain-1.0.1/mmpretrain/apis/image_classification.py`

 * *Files 2% similar despite different names*

```diff
@@ -106,17 +106,19 @@
             inputs,
             return_datasamples=return_datasamples,
             batch_size=batch_size,
             **kwargs)
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[InputType], batch_size: int = 1):
 
         def load_image(input_):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/image_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/apis/image_retrieval.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,15 @@
             device will be automatically used. Defaults to None.
         **kwargs: Other keyword arguments to initialize the model (only work if
             the ``model`` is a model name).
 
     Example:
         >>> from mmpretrain import ImageRetrievalInferencer
         >>> inferencer = ImageRetrievalInferencer(
-        ...     'resnet50-arcface_8xb32_inshop',
+        ...     'resnet50-arcface_inshop',
         ...     prototype='./demo/',
         ...     prototype_cache='img_retri.pth')
         >>> inferencer('demo/cat-dog.png', topk=2)[0][1]
         {'match_score': tensor(0.4088, device='cuda:0'),
          'sample_idx': 3,
          'sample': {'img_path': './demo/dog.jpg'}}
     """  # noqa: E501
@@ -168,17 +168,19 @@
             list: The inference results.
         """
         return super().__call__(inputs, return_datasamples, batch_size,
                                 **kwargs)
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[InputType], batch_size: int = 1):
 
         def load_image(input_):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/model.py` & `mmpretrain-1.0.1/mmpretrain/apis/model.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/multimodal_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/apis/multimodal_retrieval.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/nlvr.py` & `mmpretrain-1.0.1/mmpretrain/apis/nlvr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/utils.py` & `mmpretrain-1.0.1/mmpretrain/apis/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/visual_grounding.py` & `mmpretrain-1.0.1/mmpretrain/apis/visual_grounding.py`

 * *Files 4% similar despite different names*

```diff
@@ -82,17 +82,19 @@
                 inputs.append(input_)
 
         return super().__call__(inputs, return_datasamples, batch_size,
                                 **kwargs)
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[dict], batch_size: int = 1):
 
         def load_image(input_: dict):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/apis/visual_question_answering.py` & `mmpretrain-1.0.1/mmpretrain/apis/visual_question_answering.py`

 * *Files 4% similar despite different names*

```diff
@@ -84,17 +84,19 @@
                 inputs.append(input_)
 
         return super().__call__(inputs, return_datasamples, batch_size,
                                 **kwargs)
 
     def _init_pipeline(self, cfg: Config) -> Callable:
         test_pipeline_cfg = cfg.test_dataloader.dataset.pipeline
-        if test_pipeline_cfg[0]['type'] == 'LoadImageFromFile':
-            # Image loading is finished in `self.preprocess`.
-            test_pipeline_cfg = test_pipeline_cfg[1:]
+        from mmpretrain.datasets import remove_transform
+
+        # Image loading is finished in `self.preprocess`.
+        test_pipeline_cfg = remove_transform(test_pipeline_cfg,
+                                             'LoadImageFromFile')
         test_pipeline = Compose(
             [TRANSFORMS.build(t) for t in test_pipeline_cfg])
         return test_pipeline
 
     def preprocess(self, inputs: List[dict], batch_size: int = 1):
 
         def load_image(input_: dict):
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/__init__.py` & `mmpretrain-1.0.1/mmpretrain/datasets/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -34,21 +34,25 @@
 ]
 
 if WITH_MULTIMODAL:
     from .coco_caption import COCOCaption
     from .coco_retrieval import COCORetrieval
     from .coco_vqa import COCOVQA
     from .flamingo import FlamingoEvalCOCOCaption, FlamingoEvalCOCOVQA
+    from .flickr30k_caption import Flickr30kCaption
+    from .flickr30k_retrieval import Flickr30kRetrieval
+    from .gqa_dataset import GQA
+    from .nocaps import NoCaps
+    from .ocr_vqa import OCRVQA
     from .refcoco import RefCOCO
     from .scienceqa import ScienceQA
+    from .textvqa import TextVQA
     from .visual_genome import VisualGenomeQA
+    from .vizwiz import VizWiz
+    from .vsr import VSR
 
     __all__.extend([
-        'COCOCaption',
-        'COCORetrieval',
-        'COCOVQA',
-        'FlamingoEvalCOCOCaption',
-        'FlamingoEvalCOCOVQA',
-        'RefCOCO',
-        'VisualGenomeQA',
-        'ScienceQA',
+        'COCOCaption', 'COCORetrieval', 'COCOVQA', 'FlamingoEvalCOCOCaption',
+        'FlamingoEvalCOCOVQA', 'Flickr30kCaption', 'Flickr30kRetrieval',
+        'RefCOCO', 'VisualGenomeQA', 'ScienceQA', 'NoCaps', 'GQA', 'TextVQA',
+        'VSR', 'VizWiz', 'OCRVQA'
     ])
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/base_dataset.py` & `mmpretrain-1.0.1/mmpretrain/datasets/base_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/builder.py` & `mmpretrain-1.0.1/mmpretrain/datasets/builder.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/caltech101.py` & `mmpretrain-1.0.1/mmpretrain/datasets/caltech101.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/categories.py` & `mmpretrain-1.0.1/mmpretrain/datasets/categories.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/cifar.py` & `mmpretrain-1.0.1/mmpretrain/datasets/cifar.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_caption.py` & `mmpretrain-1.0.1/mmpretrain/datasets/coco_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/datasets/coco_retrieval.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/coco_vqa.py` & `mmpretrain-1.0.1/mmpretrain/datasets/coco_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/cub.py` & `mmpretrain-1.0.1/mmpretrain/datasets/cub.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/custom.py` & `mmpretrain-1.0.1/mmpretrain/datasets/custom.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/dataset_wrappers.py` & `mmpretrain-1.0.1/mmpretrain/datasets/dataset_wrappers.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/dtd.py` & `mmpretrain-1.0.1/mmpretrain/datasets/dtd.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/fgvcaircraft.py` & `mmpretrain-1.0.1/mmpretrain/datasets/fgvcaircraft.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/flamingo.py` & `mmpretrain-1.0.1/mmpretrain/datasets/flamingo.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/flowers102.py` & `mmpretrain-1.0.1/mmpretrain/datasets/flowers102.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/food101.py` & `mmpretrain-1.0.1/mmpretrain/datasets/food101.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/inshop.py` & `mmpretrain-1.0.1/mmpretrain/datasets/inshop.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/mnist.py` & `mmpretrain-1.0.1/mmpretrain/datasets/mnist.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from typing import List, Optional
 from urllib.parse import urljoin
 
 import mmengine.dist as dist
 import numpy as np
 import torch
 from mmengine.fileio import LocalBackend, exists, get_file_backend, join_path
+from mmengine.logging import MMLogger
 
 from mmpretrain.registry import DATASETS
 from .base_dataset import BaseDataset
 from .categories import FASHIONMNIST_CATEGORITES, MNIST_CATEGORITES
 from .utils import (download_and_extract_archive, open_maybe_compressed_file,
                     rm_suffix)
 
@@ -19,21 +20,19 @@
 class MNIST(BaseDataset):
     """`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.
 
     This implementation is modified from
     https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py
 
     Args:
-        data_prefix (str): Prefix for data.
-        test_mode (bool): ``test_mode=True`` means in test phase.
-            It determines to use the training set or test set.
+        data_root (str): The root directory of the MNIST Dataset.
+        split (str, optional): The dataset split, supports "train" and "test".
+            Default to "train".
         metainfo (dict, optional): Meta information for dataset, such as
             categories information. Defaults to None.
-        data_root (str): The root directory for ``data_prefix``.
-            Defaults to ''.
         download (bool): Whether to download the dataset if not exists.
             Defaults to True.
         **kwargs: Other keyword arguments in :class:`BaseDataset`.
     """  # noqa: E501
 
     url_prefix = 'http://yann.lecun.com/exdb/mnist/'
     # train images and labels
@@ -45,20 +44,37 @@
     test_list = [
         ['t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'],
         ['t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c'],
     ]
     METAINFO = {'classes': MNIST_CATEGORITES}
 
     def __init__(self,
-                 data_prefix: str,
-                 test_mode: bool,
-                 metainfo: Optional[dict] = None,
                  data_root: str = '',
+                 split: str = 'train',
+                 metainfo: Optional[dict] = None,
                  download: bool = True,
+                 data_prefix: str = '',
+                 test_mode: bool = False,
                  **kwargs):
+
+        splits = ['train', 'test']
+        assert split in splits, \
+            f"The split must be one of {splits}, but get '{split}'"
+        self.split = split
+
+        # To handle the BC-breaking
+        if split == 'train' and test_mode:
+            logger = MMLogger.get_current_instance()
+            logger.warning('split="train" but test_mode=True. '
+                           'The training set will be used.')
+
+        if not data_root and not data_prefix:
+            raise RuntimeError('Please set ``data_root`` to'
+                               'specify the dataset path')
+
         self.download = download
         super().__init__(
             # The MNIST dataset doesn't need specify annotation file
             ann_file='',
             metainfo=metainfo,
             data_root=data_root,
             data_prefix=dict(root=data_prefix),
@@ -134,21 +150,19 @@
 
 @DATASETS.register_module()
 class FashionMNIST(MNIST):
     """`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_
     Dataset.
 
     Args:
-        data_prefix (str): Prefix for data.
-        test_mode (bool): ``test_mode=True`` means in test phase.
-            It determines to use the training set or test set.
+        data_root (str): The root directory of the MNIST Dataset.
+        split (str, optional): The dataset split, supports "train" and "test".
+            Default to "train".
         metainfo (dict, optional): Meta information for dataset, such as
             categories information. Defaults to None.
-        data_root (str): The root directory for ``data_prefix``.
-            Defaults to ''.
         download (bool): Whether to download the dataset if not exists.
             Defaults to True.
         **kwargs: Other keyword arguments in :class:`BaseDataset`.
     """
 
     url_prefix = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'
     # train images and labels
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/multi_label.py` & `mmpretrain-1.0.1/mmpretrain/datasets/multi_label.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/multi_task.py` & `mmpretrain-1.0.1/mmpretrain/datasets/multi_task.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/nlvr2.py` & `mmpretrain-1.0.1/mmpretrain/datasets/nlvr2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/oxfordiiitpet.py` & `mmpretrain-1.0.1/mmpretrain/datasets/oxfordiiitpet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/places205.py` & `mmpretrain-1.0.1/mmpretrain/datasets/places205.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/refcoco.py` & `mmpretrain-1.0.1/mmpretrain/datasets/refcoco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/repeat_aug.py` & `mmpretrain-1.0.1/mmpretrain/datasets/samplers/repeat_aug.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/samplers/sequential.py` & `mmpretrain-1.0.1/mmpretrain/datasets/samplers/sequential.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/scienceqa.py` & `mmpretrain-1.0.1/mmpretrain/datasets/scienceqa.py`

 * *Files 10% similar despite different names*

```diff
@@ -19,34 +19,37 @@
         data_root (str): The root directory for ``data_prefix`` and
             ``ann_file``.
         split (str): The split of dataset. Options: ``train``, ``val``,
             ``test``, ``trainval``, ``minival``, and ``minitest``.
         split_file (str): The split file of dataset, which contains the
             ids of data samples in the split.
         ann_file (str): Annotation file path.
+        image_only (bool): Whether only to load data with image. Defaults to
+            False.
         data_prefix (dict): Prefix for data field. Defaults to
             ``dict(img_path='')``.
         pipeline (Sequence): Processing pipeline. Defaults to an empty tuple.
         **kwargs: Other keyword arguments in :class:`BaseDataset`.
     """
 
     def __init__(self,
                  data_root: str,
                  split: str,
                  split_file: str,
                  ann_file: str,
+                 image_only: bool = False,
                  data_prefix: dict = dict(img_path=''),
                  pipeline: Sequence[Callable] = (),
                  **kwargs):
-
         assert split in [
             'train', 'val', 'test', 'trainval', 'minival', 'minitest'
         ], f'Invalid split {split}'
         self.split = split
         self.split_file = os.path.join(data_root, split_file)
+        self.image_only = image_only
 
         super().__init__(
             data_root=data_root,
             ann_file=ann_file,
             data_prefix=data_prefix,
             pipeline=pipeline,
             **kwargs)
@@ -58,14 +61,16 @@
         current_data_split = mmengine.load(self.split_file)[self.split]  # noqa
 
         file_backend = get_file_backend(img_prefix)
 
         data_list = []
         for data_id in current_data_split:
             ann = annotations[data_id]
+            if self.image_only and ann['image'] is None:
+                continue
             data_info = {
                 'image_id':
                 data_id,
                 'question':
                 ann['question'],
                 'choices':
                 ann['choices'],
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/stanfordcars.py` & `mmpretrain-1.0.1/mmpretrain/datasets/stanfordcars.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/__init__.py` & `mmpretrain-1.0.1/mmpretrain/datasets/transforms/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,16 +9,18 @@
                            RandAugment, Rotate, Sharpness, Shear, Solarize,
                            SolarizeAdd, Translate)
 from .formatting import (Collect, NumpyToPIL, PackInputs, PackMultiTaskInputs,
                          PILToNumpy, Transpose)
 from .processing import (Albumentations, BEiTMaskGenerator, CleanCaption,
                          ColorJitter, EfficientNetCenterCrop,
                          EfficientNetRandomCrop, Lighting, RandomCrop,
-                         RandomErasing, RandomResizedCrop, RandomTranslatePad,
-                         ResizeEdge, SimMIMMaskGenerator)
+                         RandomErasing, RandomResizedCrop,
+                         RandomResizedCropAndInterpolationWithTwoPic,
+                         RandomTranslatePad, ResizeEdge, SimMIMMaskGenerator)
+from .utils import get_transform_idx, remove_transform
 from .wrappers import ApplyToList, MultiView
 
 for t in (CenterCrop, LoadImageFromFile, Normalize, RandomFlip,
           RandomGrayscale, RandomResize, Resize):
     TRANSFORMS.register_module(module=t)
 
 __all__ = [
@@ -28,9 +30,11 @@
     'Contrast', 'Brightness', 'Sharpness', 'AutoAugment', 'SolarizeAdd',
     'Cutout', 'RandAugment', 'Lighting', 'ColorJitter', 'RandomErasing',
     'PackInputs', 'Albumentations', 'EfficientNetRandomCrop',
     'EfficientNetCenterCrop', 'ResizeEdge', 'BaseAugTransform',
     'PackMultiTaskInputs', 'GaussianBlur', 'BEiTMaskGenerator',
     'SimMIMMaskGenerator', 'CenterCrop', 'LoadImageFromFile', 'Normalize',
     'RandomFlip', 'RandomGrayscale', 'RandomResize', 'Resize', 'MultiView',
-    'ApplyToList', 'CleanCaption', 'RandomTranslatePad'
+    'ApplyToList', 'CleanCaption', 'RandomTranslatePad',
+    'RandomResizedCropAndInterpolationWithTwoPic', 'get_transform_idx',
+    'remove_transform'
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/auto_augment.py` & `mmpretrain-1.0.1/mmpretrain/datasets/transforms/auto_augment.py`

 * *Files 0% similar despite different names*

```diff
@@ -1165,15 +1165,15 @@
         if self.radius is not None:
             radius = self.radius
         else:
             radius = self.random_magnitude()
 
         img = results['img']
         pil_img = Image.fromarray(img)
-        pil_img.filter(ImageFilter.GaussianBlur(radius=radius))
+        pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=radius))
         results['img'] = np.array(pil_img, dtype=img.dtype)
 
         return results
 
     def __repr__(self):
         repr_str = self.__class__.__name__
         repr_str += f'(radius={self.radius}, '
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/formatting.py` & `mmpretrain-1.0.1/mmpretrain/datasets/transforms/formatting.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/processing.py` & `mmpretrain-1.0.1/mmpretrain/datasets/transforms/processing.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import inspect
 import math
 import numbers
 import re
 import string
-import traceback
 from enum import EnumMeta
 from numbers import Number
 from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import mmcv
 import mmengine
 import numpy as np
@@ -41,51 +40,31 @@
         'box': InterpolationMode.BOX,
         'hammimg': InterpolationMode.HAMMING,
         'lanczos': InterpolationMode.LANCZOS,
     }
     return inverse_modes_mapping[t]
 
 
-def _warpper_vision_transform_cls(vision_transform_cls, new_name):
-    """build a transform warpper class for specific torchvison.transform to
-    handle the different input type between torchvison.transforms with
-    mmcls.datasets.transforms."""
+class TorchVisonTransformWrapper:
 
-    def new_init(self, *args, **kwargs):
+    def __init__(self, transform, *args, **kwargs):
         if 'interpolation' in kwargs and isinstance(kwargs['interpolation'],
                                                     str):
             kwargs['interpolation'] = _interpolation_modes_from_str(
                 kwargs['interpolation'])
         if 'dtype' in kwargs and isinstance(kwargs['dtype'], str):
             kwargs['dtype'] = _str_to_torch_dtype(kwargs['dtype'])
+        self.t = transform(*args, **kwargs)
 
-        try:
-            self.t = vision_transform_cls(*args, **kwargs)
-        except TypeError as e:
-            traceback.print_exc()
-            raise TypeError(
-                f'Error when init the {vision_transform_cls}, please '
-                f'check the argmemnts of {args} and {kwargs}. \n{e}')
-
-    def new_call(self, input):
-        try:
-            input['img'] = self.t(input['img'])
-        except Exception as e:
-            traceback.print_exc()
-            raise Exception('Error when processing of transform(`torhcvison/'
-                            f'{vision_transform_cls.__name__}`). \n{e}')
-        return input
-
-    def new_str(self):
-        return str(self.t)
-
-    new_transforms_cls = type(
-        new_name, (),
-        dict(__init__=new_init, __call__=new_call, __str__=new_str))
-    return new_transforms_cls
+    def __call__(self, results):
+        results['img'] = self.t(results['img'])
+        return results
+
+    def __repr__(self) -> str:
+        return f'TorchVision{repr(self.t)}'
 
 
 def register_vision_transforms() -> List[str]:
     """Register transforms in ``torchvision.transforms`` to the ``TRANSFORMS``
     registry.
 
     Returns:
@@ -95,18 +74,19 @@
     for module_name in dir(torchvision.transforms):
         if not re.match('[A-Z]', module_name):
             # must startswith a capital letter
             continue
         _transform = getattr(torchvision.transforms, module_name)
         if inspect.isclass(_transform) and callable(
                 _transform) and not isinstance(_transform, (EnumMeta)):
-            new_cls = _warpper_vision_transform_cls(
-                _transform, f'TorchVison{module_name}')
+            from functools import partial
             TRANSFORMS.register_module(
-                module=new_cls, name=f'torchvision/{module_name}')
+                module=partial(
+                    TorchVisonTransformWrapper, transform=_transform),
+                name=f'torchvision/{module_name}')
             vision_transforms.append(f'torchvision/{module_name}')
     return vision_transforms
 
 
 # register all the transforms in torchvision by using a transform wrapper
 VISION_TRANSFORMS = register_vision_transforms()
 
@@ -189,15 +169,15 @@
         """
         h, w = img.shape[:2]
         target_h, target_w = self.crop_size
         if w == target_w and h == target_h:
             return 0, 0, h, w
         elif w < target_w or h < target_h:
             target_w = min(w, target_w)
-            target_h = min(w, target_h)
+            target_h = min(h, target_h)
 
         offset_h = np.random.randint(0, h - target_h + 1)
         offset_w = np.random.randint(0, w - target_w + 1)
 
         return offset_h, offset_w, target_h, target_w
 
     def transform(self, results: dict) -> dict:
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/transforms/wrappers.py` & `mmpretrain-1.0.1/mmpretrain/datasets/transforms/wrappers.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/utils.py` & `mmpretrain-1.0.1/mmpretrain/datasets/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/vg_vqa.py` & `mmpretrain-1.0.1/mmpretrain/datasets/vg_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/visual_genome.py` & `mmpretrain-1.0.1/mmpretrain/datasets/visual_genome.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/datasets/voc.py` & `mmpretrain-1.0.1/mmpretrain/datasets/voc.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import xml.etree.ElementTree as ET
 from typing import List, Optional, Union
 
 from mmengine import get_file_backend, list_from_file
+from mmengine.logging import MMLogger
 
 from mmpretrain.registry import DATASETS
 from .base_dataset import expanduser
 from .categories import VOC2007_CATEGORIES
 from .multi_label import MultiLabelDataset
 
 
@@ -14,69 +15,118 @@
 class VOC(MultiLabelDataset):
     """`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Dataset.
 
     After decompression, the dataset directory structure is as follows:
 
     VOC dataset directory: ::
 
-        VOC2007 (data_root)/
-         JPEGImages (data_prefix['img_path'])
+        VOC2007
+         JPEGImages
             xxx.jpg
             xxy.jpg
             ...
-         Annotations (data_prefix['ann_path'])
+         Annotations
             xxx.xml
             xxy.xml
             ...
-         ImageSets (directory contains various imageset file)
+         ImageSets
+             Main
+                 train.txt
+                 val.txt
+                 trainval.txt
+                 test.txt
+                 ...
 
     Extra difficult label is in VOC annotations, we will use
     `gt_label_difficult` to record the difficult labels in each sample
     and corresponding evaluation should take care of this field
     to calculate metrics. Usually, difficult labels are reckoned as
     negative in defaults.
 
     Args:
         data_root (str): The root directory for VOC dataset.
-        image_set_path (str): The path of image set, The file which
+        split (str, optional): The dataset split, supports "train",
+            "val", "trainval", and "test". Default to "trainval".
+        image_set_path (str, optional): The path of image set, The file which
             lists image ids of the sub dataset, and this path is relative
-            to ``data_root``.
+            to ``data_root``. Default to ''.
         data_prefix (dict): Prefix for data and annotation, keyword
             'img_path' and 'ann_path' can be set. Defaults to be
             ``dict(img_path='JPEGImages', ann_path='Annotations')``.
-        test_mode (bool): ``test_mode=True`` means in test phase.
-            It determines to use the training set or test set.
         metainfo (dict, optional): Meta information for dataset, such as
             categories information. Defaults to None.
         **kwargs: Other keyword arguments in :class:`BaseDataset`.
+
+    Examples:
+        >>> from mmpretrain.datasets import VOC
+        >>> train_dataset = VOC(data_root='data/VOC2007', split='trainval')
+        >>> train_dataset
+        Dataset VOC
+            Number of samples:  5011
+            Number of categories:       20
+            Prefix of dataset:  data/VOC2007
+            Path of image set:  data/VOC2007/ImageSets/Main/trainval.txt
+            Prefix of images:   data/VOC2007/JPEGImages
+            Prefix of annotations:      data/VOC2007/Annotations
+        >>> test_dataset = VOC(data_root='data/VOC2007', split='test')
+        >>> test_dataset
+        Dataset VOC
+            Number of samples:  4952
+            Number of categories:       20
+            Prefix of dataset:  data/VOC2007
+            Path of image set:  data/VOC2007/ImageSets/Main/test.txt
+            Prefix of images:   data/VOC2007/JPEGImages
+            Prefix of annotations:      data/VOC2007/Annotations
     """  # noqa: E501
 
     METAINFO = {'classes': VOC2007_CATEGORIES}
 
     def __init__(self,
                  data_root: str,
-                 image_set_path: str,
+                 split: str = 'trainval',
+                 image_set_path: str = '',
                  data_prefix: Union[str, dict] = dict(
                      img_path='JPEGImages', ann_path='Annotations'),
                  test_mode: bool = False,
                  metainfo: Optional[dict] = None,
                  **kwargs):
+
+        self.backend = get_file_backend(data_root, enable_singleton=True)
+
+        if split:
+            splits = ['train', 'val', 'trainval', 'test']
+            assert split in splits, \
+                f"The split must be one of {splits}, but get '{split}'"
+            self.split = split
+
+            if not data_prefix:
+                data_prefix = dict(
+                    img_path='JPEGImages', ann_path='Annotations')
+            if not image_set_path:
+                image_set_path = self.backend.join_path(
+                    'ImageSets', 'Main', f'{split}.txt')
+
+        # To handle the BC-breaking
+        if (split == 'train' or split == 'trainval') and test_mode:
+            logger = MMLogger.get_current_instance()
+            logger.warning(f'split="{split}" but test_mode=True. '
+                           f'The {split} set will be used.')
+
         if isinstance(data_prefix, str):
             data_prefix = dict(img_path=expanduser(data_prefix))
         assert isinstance(data_prefix, dict) and 'img_path' in data_prefix, \
             '`data_prefix` must be a dict with key img_path'
 
-        if test_mode is False:
+        if (split and split not in ['val', 'test']) or not test_mode:
             assert 'ann_path' in data_prefix and data_prefix[
                 'ann_path'] is not None, \
-                '"ann_path" must be set in `data_prefix` if `test_mode` is' \
-                ' False.'
+                '"ann_path" must be set in `data_prefix`' \
+                'when validation or test set is used.'
 
         self.data_root = data_root
-        self.backend = get_file_backend(data_root, enable_singleton=True)
         self.image_set_path = self.backend.join_path(data_root, image_set_path)
 
         super().__init__(
             ann_file='',
             metainfo=metainfo,
             data_root=data_root,
             data_prefix=data_prefix,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/__init__.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/class_num_check_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/class_num_check_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/densecl_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/densecl_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/ema_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/ema_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/margin_head_hooks.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/margin_head_hooks.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/precise_bn_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/precise_bn_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/retriever_hooks.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/retriever_hooks.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/simsiam_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/simsiam_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/swav_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/swav_hook.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 from typing import Dict, List, Optional, Sequence
 
 import torch
+from mmengine.device import get_device
 from mmengine.dist import get_rank, get_world_size, is_distributed
 from mmengine.hooks import Hook
 from mmengine.logging import MMLogger
 
 from mmpretrain.registry import HOOKS
 from mmpretrain.utils import get_ori_model
 
@@ -93,19 +94,21 @@
 
     def before_train_epoch(self, runner) -> None:
         """Check the queues' state."""
         # optionally starts a queue
         if self.queue_length > 0 \
             and runner.epoch >= self.epoch_queue_starts \
                 and self.queue is None:
+
             self.queue = torch.zeros(
                 len(self.crops_for_assign),
                 self.queue_length // runner.world_size,
                 self.feat_dim,
-            ).cuda()
+                device=get_device(),
+            )
 
         # set the boolean type of use_the_queue
         get_ori_model(runner.model).head.loss_module.queue = self.queue
         get_ori_model(runner.model).head.loss_module.use_queue = False
 
     def after_train_epoch(self, runner) -> None:
         """Save the queues locally."""
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/switch_recipe_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/switch_recipe_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/visualization_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/visualization_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/hooks/warmup_param_hook.py` & `mmpretrain-1.0.1/mmpretrain/engine/hooks/warmup_param_hook.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/adan_t.py` & `mmpretrain-1.0.1/mmpretrain/engine/optimizers/adan_t.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/lamb.py` & `mmpretrain-1.0.1/mmpretrain/engine/optimizers/lamb.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/lars.py` & `mmpretrain-1.0.1/mmpretrain/engine/optimizers/lars.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/optimizers/layer_decay_optim_wrapper_constructor.py` & `mmpretrain-1.0.1/mmpretrain/engine/optimizers/layer_decay_optim_wrapper_constructor.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/engine/runners/retrieval_loop.py` & `mmpretrain-1.0.1/mmpretrain/engine/runners/retrieval_loop.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/__init__.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .caption import COCOCaption
+from .gqa import GQAAcc
 from .multi_label import AveragePrecision, MultiLabelMetric
 from .multi_task import MultiTasksMetric
-from .retrieval import RetrievalRecall
+from .nocaps import NocapsSave
+from .retrieval import RetrievalAveragePrecision, RetrievalRecall
 from .scienceqa import ScienceQAMetric
+from .shape_bias_label import ShapeBiasMetric
 from .single_label import Accuracy, ConfusionMatrix, SingleLabelMetric
 from .visual_grounding_eval import VisualGroundingMetric
 from .voc_multi_label import VOCAveragePrecision, VOCMultiLabelMetric
 from .vqa import ReportVQA, VQAAcc
 
 __all__ = [
     'Accuracy', 'SingleLabelMetric', 'MultiLabelMetric', 'AveragePrecision',
     'MultiTasksMetric', 'VOCAveragePrecision', 'VOCMultiLabelMetric',
     'ConfusionMatrix', 'RetrievalRecall', 'VQAAcc', 'ReportVQA', 'COCOCaption',
-    'VisualGroundingMetric', 'ScienceQAMetric'
+    'VisualGroundingMetric', 'ScienceQAMetric', 'GQAAcc', 'NocapsSave',
+    'RetrievalAveragePrecision', 'ShapeBiasMetric'
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/caption.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/multi_label.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/multi_label.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/multi_task.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/multi_task.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/scienceqa.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/scienceqa.py`

 * *Files 1% similar despite different names*

```diff
@@ -82,17 +82,17 @@
             result['prediction'] = get_pred_idx(
                 data_sample.get('pred_answer'), choices, self.options)
             result['grade'] = data_sample.get('grade')
             result['subject'] = data_sample.get('subject')
             result['answer'] = data_sample.get('gt_answer')
             hint = data_sample.get('hint')
             has_image = data_sample.get('has_image', False)
-            result[
-                'no_context'] = True if not has_image and hint is None else False  # noqa
-            result['has_text'] = True if hint is not None else False
+            result['no_context'] = True if not has_image and len(
+                hint) == 0 else False  # noqa
+            result['has_text'] = True if len(hint) > 0 else False
             result['has_image'] = has_image
 
             # Save the result to `self.results`.
             self.results.append(result)
 
     def compute_metrics(self, results: List) -> dict:
         """Compute the metrics from processed results.
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/single_label.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/single_label.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/visual_grounding_eval.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/visual_grounding_eval.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/voc_multi_label.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/voc_multi_label.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/evaluation/metrics/vqa.py` & `mmpretrain-1.0.1/mmpretrain/evaluation/metrics/vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,14 +4,15 @@
                       build_backbone, build_classifier, build_head, build_loss,
                       build_neck)
 from .classifiers import *  # noqa: F401,F403
 from .heads import *  # noqa: F401,F403
 from .losses import *  # noqa: F401,F403
 from .multimodal import *  # noqa: F401,F403
 from .necks import *  # noqa: F401,F403
+from .peft import *  # noqa: F401,F403
 from .retrievers import *  # noqa: F401,F403
 from .selfsup import *  # noqa: F401,F403
 from .tta import *  # noqa: F401,F403
 from .utils import *  # noqa: F401,F403
 
 __all__ = [
     'BACKBONES', 'HEADS', 'NECKS', 'LOSSES', 'CLASSIFIERS', 'build_backbone',
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 from .deit import DistilledVisionTransformer
 from .deit3 import DeiT3
 from .densenet import DenseNet
 from .edgenext import EdgeNeXt
 from .efficientformer import EfficientFormer
 from .efficientnet import EfficientNet
 from .efficientnet_v2 import EfficientNetV2
+from .hivit import HiViT
 from .hornet import HorNet
 from .hrnet import HRNet
 from .inception_v3 import InceptionV3
 from .lenet import LeNet5
 from .levit import LeViT
 from .mixmim import MixMIMTransformer
 from .mlp_mixer import MlpMixer
@@ -37,14 +38,16 @@
 from .resnext import ResNeXt
 from .revvit import RevVisionTransformer
 from .riformer import RIFormer
 from .seresnet import SEResNet
 from .seresnext import SEResNeXt
 from .shufflenet_v1 import ShuffleNetV1
 from .shufflenet_v2 import ShuffleNetV2
+from .sparse_convnext import SparseConvNeXt
+from .sparse_resnet import SparseResNet
 from .swin_transformer import SwinTransformer
 from .swin_transformer_v2 import SwinTransformerV2
 from .t2t_vit import T2T_ViT
 from .timm_backbone import TIMMBackbone
 from .tinyvit import TinyViT
 from .tnt import TNT
 from .twins import PCPVT, SVT
@@ -116,8 +119,11 @@
     'TinyViT',
     'LeViT',
     'Vig',
     'PyramidVig',
     'XCiT',
     'ViTSAM',
     'ViTEVA02',
+    'HiViT',
+    'SparseResNet',
+    'SparseConvNeXt',
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/alexnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/alexnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/base_backbone.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/base_backbone.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/beit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/beit.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,19 +3,21 @@
 
 import numpy as np
 import torch
 import torch.nn as nn
 from mmcv.cnn.bricks.drop import build_dropout
 from mmcv.cnn.bricks.transformer import FFN, PatchEmbed
 from mmengine.model import BaseModule, ModuleList
+from mmengine.model.weight_init import trunc_normal_
 
 from mmpretrain.registry import MODELS
 from ..utils import (BEiTAttention, build_norm_layer, resize_pos_embed,
                      resize_relative_position_bias_table, to_2tuple)
-from .vision_transformer import TransformerEncoderLayer, VisionTransformer
+from .base_backbone import BaseBackbone
+from .vision_transformer import TransformerEncoderLayer
 
 
 class RelativePositionBias(BaseModule):
     """Relative Position Bias.
 
     This module is copied from
     https://github.com/microsoft/unilm/blob/master/beit/modeling_finetune.py#L209.
@@ -208,15 +210,15 @@
             x = x + self.drop_path(self.gamma_1 * self.attn(
                 self.ln1(x), rel_pos_bias=rel_pos_bias))
             x = x + self.drop_path(self.gamma_2 * self.ffn(self.ln2(x)))
         return x
 
 
 @MODELS.register_module()
-class BEiTViT(VisionTransformer):
+class BEiTViT(BaseBackbone):
     """Backbone for BEiT.
 
     A PyTorch implement of : `BEiT: BERT Pre-Training of Image Transformers
     <https://arxiv.org/abs/2106.08254>`_
     A PyTorch implement of : `BEiT v2: Masked Image Modeling with
     Vector-Quantized Visual Tokenizers <https://arxiv.org/abs/2208.06366>`_
 
@@ -278,14 +280,70 @@
             embeding vector resize. Defaults to "bicubic".
         patch_cfg (dict): Configs of patch embeding. Defaults to an empty dict.
         layer_cfgs (Sequence | dict): Configs of each transformer layer in
             encoder. Defaults to an empty dict.
         init_cfg (dict, optional): Initialization config dict.
             Defaults to None.
     """
+    arch_zoo = {
+        **dict.fromkeys(
+            ['s', 'small'], {
+                'embed_dims': 768,
+                'num_layers': 8,
+                'num_heads': 8,
+                'feedforward_channels': 768 * 3,
+            }),
+        **dict.fromkeys(
+            ['b', 'base'], {
+                'embed_dims': 768,
+                'num_layers': 12,
+                'num_heads': 12,
+                'feedforward_channels': 3072
+            }),
+        **dict.fromkeys(
+            ['l', 'large'], {
+                'embed_dims': 1024,
+                'num_layers': 24,
+                'num_heads': 16,
+                'feedforward_channels': 4096
+            }),
+        **dict.fromkeys(
+            ['eva-g', 'eva-giant'],
+            {
+                # The implementation in EVA
+                # <https://arxiv.org/abs/2211.07636>
+                'embed_dims': 1408,
+                'num_layers': 40,
+                'num_heads': 16,
+                'feedforward_channels': 6144
+            }),
+        **dict.fromkeys(
+            ['deit-t', 'deit-tiny'], {
+                'embed_dims': 192,
+                'num_layers': 12,
+                'num_heads': 3,
+                'feedforward_channels': 192 * 4
+            }),
+        **dict.fromkeys(
+            ['deit-s', 'deit-small'], {
+                'embed_dims': 384,
+                'num_layers': 12,
+                'num_heads': 6,
+                'feedforward_channels': 384 * 4
+            }),
+        **dict.fromkeys(
+            ['deit-b', 'deit-base'], {
+                'embed_dims': 768,
+                'num_layers': 12,
+                'num_heads': 12,
+                'feedforward_channels': 768 * 4
+            }),
+    }
+    num_extra_tokens = 1  # class token
+    OUT_TYPES = {'raw', 'cls_token', 'featmap', 'avg_featmap'}
 
     def __init__(self,
                  arch='base',
                  img_size=224,
                  patch_size=16,
                  in_channels=3,
                  out_indices=-1,
@@ -296,20 +354,20 @@
                  final_norm=False,
                  out_type='avg_featmap',
                  with_cls_token=True,
                  frozen_stages=-1,
                  use_abs_pos_emb=False,
                  use_rel_pos_bias=True,
                  use_shared_rel_pos_bias=False,
-                 layer_scale_init_value=0.1,
                  interpolate_mode='bicubic',
+                 layer_scale_init_value=0.1,
                  patch_cfg=dict(),
                  layer_cfgs=dict(),
                  init_cfg=None):
-        super(VisionTransformer, self).__init__(init_cfg)
+        super(BEiTViT, self).__init__(init_cfg)
 
         if isinstance(arch, str):
             arch = arch.lower()
             assert arch in set(self.arch_zoo), \
                 f'Arch {arch} is not in default archs {set(self.arch_zoo)}'
             self.arch_settings = self.arch_zoo[arch]
         else:
@@ -341,14 +399,15 @@
         # Set out type
         if out_type not in self.OUT_TYPES:
             raise ValueError(f'Unsupported `out_type` {out_type}, please '
                              f'choose from {self.OUT_TYPES}')
         self.out_type = out_type
 
         # Set cls token
+        self.with_cls_token = with_cls_token
         if with_cls_token:
             self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))
             self.num_extra_tokens = 1
         elif out_type != 'cls_token':
             self.cls_token = None
             self.num_extra_tokens = 0
         else:
@@ -422,14 +481,95 @@
         if out_type == 'avg_featmap':
             self.ln2 = build_norm_layer(norm_cfg, self.embed_dims)
 
         # freeze stages only when self.frozen_stages > 0
         if self.frozen_stages > 0:
             self._freeze_stages()
 
+    @property
+    def norm1(self):
+        return self.ln1
+
+    @property
+    def norm2(self):
+        return self.ln2
+
+    def init_weights(self):
+        super(BEiTViT, self).init_weights()
+
+        if not (isinstance(self.init_cfg, dict)
+                and self.init_cfg['type'] == 'Pretrained'):
+            if self.pos_embed is not None:
+                trunc_normal_(self.pos_embed, std=0.02)
+
+    def _prepare_pos_embed(self, state_dict, prefix, *args, **kwargs):
+        name = prefix + 'pos_embed'
+        if name not in state_dict.keys():
+            return
+
+        ckpt_pos_embed_shape = state_dict[name].shape
+        if (not self.with_cls_token
+                and ckpt_pos_embed_shape[1] == self.pos_embed.shape[1] + 1):
+            # Remove cls token from state dict if it's not used.
+            state_dict[name] = state_dict[name][:, 1:]
+            ckpt_pos_embed_shape = state_dict[name].shape
+
+        if self.pos_embed.shape != ckpt_pos_embed_shape:
+            from mmengine.logging import MMLogger
+            logger = MMLogger.get_current_instance()
+            logger.info(
+                f'Resize the pos_embed shape from {ckpt_pos_embed_shape} '
+                f'to {self.pos_embed.shape}.')
+
+            ckpt_pos_embed_shape = to_2tuple(
+                int(np.sqrt(ckpt_pos_embed_shape[1] - self.num_extra_tokens)))
+            pos_embed_shape = self.patch_embed.init_out_size
+
+            state_dict[name] = resize_pos_embed(state_dict[name],
+                                                ckpt_pos_embed_shape,
+                                                pos_embed_shape,
+                                                self.interpolate_mode,
+                                                self.num_extra_tokens)
+
+    @staticmethod
+    def resize_pos_embed(*args, **kwargs):
+        """Interface for backward-compatibility."""
+        return resize_pos_embed(*args, **kwargs)
+
+    def _freeze_stages(self):
+        # freeze position embedding
+        if self.pos_embed is not None:
+            self.pos_embed.requires_grad = False
+        # set dropout to eval model
+        self.drop_after_pos.eval()
+        # freeze patch embedding
+        self.patch_embed.eval()
+        for param in self.patch_embed.parameters():
+            param.requires_grad = False
+        # freeze cls_token
+        if self.with_cls_token:
+            self.cls_token.requires_grad = False
+        # freeze layers
+        for i in range(1, self.frozen_stages + 1):
+            m = self.layers[i - 1]
+            m.eval()
+            for param in m.parameters():
+                param.requires_grad = False
+        # freeze the last layer norm
+        if self.frozen_stages == len(self.layers):
+            if self.final_norm:
+                self.ln1.eval()
+                for param in self.ln1.parameters():
+                    param.requires_grad = False
+
+            if self.out_type == 'avg_featmap':
+                self.ln2.eval()
+                for param in self.ln2.parameters():
+                    param.requires_grad = False
+
     def forward(self, x):
         B = x.shape[0]
         x, patch_resolution = self.patch_embed(x)
 
         if self.cls_token is not None:
             # stole cls_tokens impl from Phil Wang, thanks
             cls_token = self.cls_token.expand(B, -1, -1)
@@ -516,7 +656,42 @@
                                 f'{new_rel_pos_bias.shape}')
                     state_dict[ckpt_key] = new_rel_pos_bias
 
                     # The index buffer need to be re-generated.
                     index_buffer = ckpt_key.replace('bias_table', 'index')
                     if index_buffer in state_dict:
                         del state_dict[index_buffer]
+
+    def get_layer_depth(self, param_name: str, prefix: str = ''):
+        """Get the layer-wise depth of a parameter.
+
+        Args:
+            param_name (str): The name of the parameter.
+            prefix (str): The prefix for the parameter.
+                Defaults to an empty string.
+
+        Returns:
+            Tuple[int, int]: The layer-wise depth and the num of layers.
+
+        Note:
+            The first depth is the stem module (``layer_depth=0``), and the
+            last depth is the subsequent module (``layer_depth=num_layers-1``)
+        """
+        num_layers = self.num_layers + 2
+
+        if not param_name.startswith(prefix):
+            # For subsequent module like head
+            return num_layers - 1, num_layers
+
+        param_name = param_name[len(prefix):]
+
+        if param_name in ('cls_token', 'pos_embed'):
+            layer_depth = 0
+        elif param_name.startswith('patch_embed'):
+            layer_depth = 0
+        elif param_name.startswith('layers'):
+            layer_id = int(param_name.split('.')[1])
+            layer_depth = layer_id + 1
+        else:
+            layer_depth = num_layers - 1
+
+        return layer_depth, num_layers
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/conformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/conformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/convmixer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/convmixer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/convnext.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/edgenext.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,271 +1,276 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from functools import partial
+import math
 from itertools import chain
 from typing import Sequence
 
 import torch
 import torch.nn as nn
-import torch.utils.checkpoint as cp
 from mmcv.cnn.bricks import DropPath
 from mmengine.model import BaseModule, ModuleList, Sequential
 
 from mmpretrain.registry import MODELS
-from ..utils import GRN, build_norm_layer
+from ..utils import (ChannelMultiheadAttention, PositionEncodingFourier,
+                     build_norm_layer)
 from .base_backbone import BaseBackbone
+from .convnext import ConvNeXtBlock
 
 
-class ConvNeXtBlock(BaseModule):
-    """ConvNeXt Block.
+class SDTAEncoder(BaseModule):
+    """A PyTorch implementation of split depth-wise transpose attention (SDTA)
+    encoder.
 
+    Inspiration from
+    https://github.com/mmaaz60/EdgeNeXt
     Args:
-        in_channels (int): The number of input channels.
-        dw_conv_cfg (dict): Config of depthwise convolution.
-            Defaults to ``dict(kernel_size=7, padding=3)``.
-        norm_cfg (dict): The config dict for norm layers.
-            Defaults to ``dict(type='LN2d', eps=1e-6)``.
-        act_cfg (dict): The config dict for activation between pointwise
-            convolution. Defaults to ``dict(type='GELU')``.
-        mlp_ratio (float): The expansion ratio in both pointwise convolution.
+        in_channel (int): Number of input channels.
+        drop_path_rate (float): Stochastic depth dropout rate.
+            Defaults to 0.
+        layer_scale_init_value (float): Initial value of layer scale.
+            Defaults to 1e-6.
+        mlp_ratio (int): Number of channels ratio in the MLP.
             Defaults to 4.
-        linear_pw_conv (bool): Whether to use linear layer to do pointwise
-            convolution. More details can be found in the note.
+        use_pos_emb (bool): Whether to use position encoding.
             Defaults to True.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
-        layer_scale_init_value (float): Init value for Layer Scale.
+        num_heads (int): Number of heads in the multihead attention.
+            Defaults to 8.
+        qkv_bias (bool): Whether to use bias in the multihead attention.
+            Defaults to True.
+        attn_drop (float): Dropout rate of the attention.
+            Defaults to 0.
+        proj_drop (float): Dropout rate of the projection.
+            Defaults to 0.
+        layer_scale_init_value (float): Initial value of layer scale.
             Defaults to 1e-6.
-
-    Note:
-        There are two equivalent implementations:
-
-        1. DwConv -> LayerNorm -> 1x1 Conv -> GELU -> 1x1 Conv;
-           all outputs are in (N, C, H, W).
-        2. DwConv -> LayerNorm -> Permute to (N, H, W, C) -> Linear -> GELU
-           -> Linear; Permute back
-
-        As default, we use the second to align with the official repository.
-        And it may be slightly faster.
+        norm_cfg (dict): Dictionary to construct normalization layer.
+            Defaults to ``dict(type='LN')``.
+        act_cfg (dict): Dictionary to construct activation layer.
+            Defaults to ``dict(type='GELU')``.
+        scales (int): Number of scales. Default to 1.
     """
 
     def __init__(self,
-                 in_channels,
-                 dw_conv_cfg=dict(kernel_size=7, padding=3),
-                 norm_cfg=dict(type='LN2d', eps=1e-6),
-                 act_cfg=dict(type='GELU'),
-                 mlp_ratio=4.,
-                 linear_pw_conv=True,
+                 in_channel,
                  drop_path_rate=0.,
                  layer_scale_init_value=1e-6,
-                 use_grn=False,
-                 with_cp=False):
-        super().__init__()
-        self.with_cp = with_cp
-
-        self.depthwise_conv = nn.Conv2d(
-            in_channels, in_channels, groups=in_channels, **dw_conv_cfg)
-
-        self.linear_pw_conv = linear_pw_conv
-        self.norm = build_norm_layer(norm_cfg, in_channels)
-
-        mid_channels = int(mlp_ratio * in_channels)
-        if self.linear_pw_conv:
-            # Use linear layer to do pointwise conv.
-            pw_conv = nn.Linear
-        else:
-            pw_conv = partial(nn.Conv2d, kernel_size=1)
+                 mlp_ratio=4,
+                 use_pos_emb=True,
+                 num_heads=8,
+                 qkv_bias=True,
+                 attn_drop=0.,
+                 proj_drop=0.,
+                 norm_cfg=dict(type='LN'),
+                 act_cfg=dict(type='GELU'),
+                 scales=1,
+                 init_cfg=None):
+        super(SDTAEncoder, self).__init__(init_cfg=init_cfg)
+        conv_channels = max(
+            int(math.ceil(in_channel / scales)),
+            int(math.floor(in_channel // scales)))
+        self.conv_channels = conv_channels
+        self.num_convs = scales if scales == 1 else scales - 1
+
+        self.conv_modules = ModuleList()
+        for i in range(self.num_convs):
+            self.conv_modules.append(
+                nn.Conv2d(
+                    conv_channels,
+                    conv_channels,
+                    kernel_size=3,
+                    padding=1,
+                    groups=conv_channels))
+
+        self.pos_embed = PositionEncodingFourier(
+            embed_dims=in_channel) if use_pos_emb else None
+
+        self.norm_csa = build_norm_layer(norm_cfg, in_channel)
+        self.gamma_csa = nn.Parameter(
+            layer_scale_init_value * torch.ones(in_channel),
+            requires_grad=True) if layer_scale_init_value > 0 else None
+        self.csa = ChannelMultiheadAttention(
+            embed_dims=in_channel,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            attn_drop=attn_drop,
+            proj_drop=proj_drop)
 
-        self.pointwise_conv1 = pw_conv(in_channels, mid_channels)
+        self.norm = build_norm_layer(norm_cfg, in_channel)
+        self.pointwise_conv1 = nn.Linear(in_channel, mlp_ratio * in_channel)
         self.act = MODELS.build(act_cfg)
-        self.pointwise_conv2 = pw_conv(mid_channels, in_channels)
-
-        if use_grn:
-            self.grn = GRN(mid_channels)
-        else:
-            self.grn = None
-
+        self.pointwise_conv2 = nn.Linear(mlp_ratio * in_channel, in_channel)
         self.gamma = nn.Parameter(
-            layer_scale_init_value * torch.ones((in_channels)),
+            layer_scale_init_value * torch.ones(in_channel),
             requires_grad=True) if layer_scale_init_value > 0 else None
-
         self.drop_path = DropPath(
             drop_path_rate) if drop_path_rate > 0. else nn.Identity()
 
     def forward(self, x):
-
-        def _inner_forward(x):
-            shortcut = x
-            x = self.depthwise_conv(x)
-
-            if self.linear_pw_conv:
-                x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)
-                x = self.norm(x, data_format='channel_last')
-                x = self.pointwise_conv1(x)
-                x = self.act(x)
-                if self.grn is not None:
-                    x = self.grn(x, data_format='channel_last')
-                x = self.pointwise_conv2(x)
-                x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)
+        shortcut = x
+        spx = torch.split(x, self.conv_channels, dim=1)
+        for i in range(self.num_convs):
+            if i == 0:
+                sp = spx[i]
+            else:
+                sp = sp + spx[i]
+            sp = self.conv_modules[i](sp)
+            if i == 0:
+                out = sp
             else:
-                x = self.norm(x, data_format='channel_first')
-                x = self.pointwise_conv1(x)
-                x = self.act(x)
-
-                if self.grn is not None:
-                    x = self.grn(x, data_format='channel_first')
-                x = self.pointwise_conv2(x)
+                out = torch.cat((out, sp), 1)
 
-            if self.gamma is not None:
-                x = x.mul(self.gamma.view(1, -1, 1, 1))
+        x = torch.cat((out, spx[self.num_convs]), 1)
 
-            x = shortcut + self.drop_path(x)
-            return x
+        # Channel Self-attention
+        B, C, H, W = x.shape
+        x = x.reshape(B, C, H * W).permute(0, 2, 1)
+        if self.pos_embed:
+            pos_encoding = self.pos_embed((B, H, W))
+            pos_encoding = pos_encoding.reshape(B, -1,
+                                                x.shape[1]).permute(0, 2, 1)
+            x += pos_encoding
+
+        x = x + self.drop_path(self.gamma_csa * self.csa(self.norm_csa(x)))
+        x = x.reshape(B, H, W, C)
+
+        # Inverted Bottleneck
+        x = self.norm(x)
+        x = self.pointwise_conv1(x)
+        x = self.act(x)
+        x = self.pointwise_conv2(x)
+
+        if self.gamma is not None:
+            x = self.gamma * x
+        x = x.permute(0, 3, 1, 2)  # (B, H, W, C) -> (B, C, H, W)
+
+        x = shortcut + self.drop_path(x)
 
-        if self.with_cp and x.requires_grad:
-            x = cp.checkpoint(_inner_forward, x)
-        else:
-            x = _inner_forward(x)
         return x
 
 
 @MODELS.register_module()
-class ConvNeXt(BaseBackbone):
-    """ConvNeXt v1&v2 backbone.
+class EdgeNeXt(BaseBackbone):
+    """EdgeNeXt.
 
-    A PyTorch implementation of `A ConvNet for the 2020s
-    <https://arxiv.org/abs/2201.03545>`_ and
-    `ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders
-    <http://arxiv.org/abs/2301.00808>`_
-
-    Modified from the `official repo
-    <https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py>`_
-    and `timm
-    <https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/convnext.py>`_.
+    A PyTorch implementation of: `EdgeNeXt: Efficiently Amalgamated
+    CNN-Transformer Architecture for Mobile Vision Applications
+    <https://arxiv.org/abs/2206.10589>`_
 
-    To use ConvNeXt v2, please set ``use_grn=True`` and ``layer_scale_init_value=0.``.
+    Inspiration from
+    https://github.com/mmaaz60/EdgeNeXt
 
     Args:
         arch (str | dict): The model's architecture. If string, it should be
-            one of architecture in ``ConvNeXt.arch_settings``. And if dict, it
-            should include the following two keys:
+            one of architectures in ``EdgeNeXt.arch_settings``.
+            And if dict, it should include the following keys:
 
-            - depths (list[int]): Number of blocks at each stage.
             - channels (list[int]): The number of channels at each stage.
+            - depths (list[int]): The number of blocks at each stage.
+            - num_heads (list[int]): The number of heads at each stage.
 
-            Defaults to 'tiny'.
-        in_channels (int): Number of input image channels. Defaults to 3.
-        stem_patch_size (int): The size of one patch in the stem layer.
+            Defaults to 'xxsmall'.
+        in_channels (int): The number of input channels.
+            Defaults to 3.
+        global_blocks (list[int]): The number of global blocks.
+            Defaults to [0, 1, 1, 1].
+        global_block_type (list[str]): The type of global blocks.
+            Defaults to ['None', 'SDTA', 'SDTA', 'SDTA'].
+        drop_path_rate (float): Stochastic depth dropout rate.
+            Defaults to 0.
+        layer_scale_init_value (float): Initial value of layer scale.
+            Defaults to 1e-6.
+        linear_pw_conv (bool): Whether to use linear layer to do pointwise
+            convolution. Defaults to False.
+        mlp_ratio (int): The number of channel ratio in MLP layers.
             Defaults to 4.
-        norm_cfg (dict): The config dict for norm layers.
+        conv_kernel_size (list[int]): The kernel size of convolutional layers
+            at each stage. Defaults to [3, 5, 7, 9].
+        use_pos_embd_csa (list[bool]): Whether to use positional embedding in
+            Channel Self-Attention. Defaults to [False, True, False, False].
+        use_pos_emebd_global (bool): Whether to use positional embedding for
+            whole network. Defaults to False.
+        d2_scales (list[int]): The number of channel groups used for SDTA at
+            each stage. Defaults to [2, 2, 3, 4].
+        norm_cfg (dict): The config of normalization layer.
             Defaults to ``dict(type='LN2d', eps=1e-6)``.
-        act_cfg (dict): The config dict for activation between pointwise
-            convolution. Defaults to ``dict(type='GELU')``.
-        linear_pw_conv (bool): Whether to use linear layer to do pointwise
-            convolution. Defaults to True.
-        use_grn (bool): Whether to add Global Response Normalization in the
-            blocks. Defaults to False.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
-        layer_scale_init_value (float): Init value for Layer Scale.
-            Defaults to 1e-6.
         out_indices (Sequence | int): Output from which stages.
             Defaults to -1, means the last stage.
         frozen_stages (int): Stages to be frozen (all param fixed).
             Defaults to 0, which means not freezing any parameters.
         gap_before_final_norm (bool): Whether to globally average the feature
-            map before the final norm layer. In the official repo, it's only
-            used in classification task. Defaults to True.
-        with_cp (bool): Use checkpoint or not. Using checkpoint will save some
-            memory while slowing down the training speed. Defaults to False.
-        init_cfg (dict, optional): Initialization config dict
-    """  # noqa: E501
+            map before the final norm layer. Defaults to True.
+        act_cfg (dict): The config of activation layer.
+            Defaults to ``dict(type='GELU')``.
+        init_cfg (dict, optional): Config for initialization.
+            Defaults to None.
+    """
     arch_settings = {
-        'atto': {
-            'depths': [2, 2, 6, 2],
-            'channels': [40, 80, 160, 320]
-        },
-        'femto': {
+        'xxsmall': {  # parameters: 1.3M
+            'channels': [24, 48, 88, 168],
             'depths': [2, 2, 6, 2],
-            'channels': [48, 96, 192, 384]
-        },
-        'pico': {
-            'depths': [2, 2, 6, 2],
-            'channels': [64, 128, 256, 512]
-        },
-        'nano': {
-            'depths': [2, 2, 8, 2],
-            'channels': [80, 160, 320, 640]
+            'num_heads': [4, 4, 4, 4]
         },
-        'tiny': {
+        'xsmall': {  # parameters: 2.3M
+            'channels': [32, 64, 100, 192],
             'depths': [3, 3, 9, 3],
-            'channels': [96, 192, 384, 768]
+            'num_heads': [4, 4, 4, 4]
         },
-        'small': {
-            'depths': [3, 3, 27, 3],
-            'channels': [96, 192, 384, 768]
-        },
-        'base': {
-            'depths': [3, 3, 27, 3],
-            'channels': [128, 256, 512, 1024]
-        },
-        'large': {
-            'depths': [3, 3, 27, 3],
-            'channels': [192, 384, 768, 1536]
+        'small': {  # parameters: 5.6M
+            'channels': [48, 96, 160, 304],
+            'depths': [3, 3, 9, 3],
+            'num_heads': [8, 8, 8, 8]
         },
-        'xlarge': {
-            'depths': [3, 3, 27, 3],
-            'channels': [256, 512, 1024, 2048]
+        'base': {  # parameters: 18.51M
+            'channels': [80, 160, 288, 584],
+            'depths': [3, 3, 9, 3],
+            'num_heads': [8, 8, 8, 8]
         },
-        'huge': {
-            'depths': [3, 3, 27, 3],
-            'channels': [352, 704, 1408, 2816]
-        }
     }
 
     def __init__(self,
-                 arch='tiny',
+                 arch='xxsmall',
                  in_channels=3,
-                 stem_patch_size=4,
-                 norm_cfg=dict(type='LN2d', eps=1e-6),
-                 act_cfg=dict(type='GELU'),
-                 linear_pw_conv=True,
-                 use_grn=False,
+                 global_blocks=[0, 1, 1, 1],
+                 global_block_type=['None', 'SDTA', 'SDTA', 'SDTA'],
                  drop_path_rate=0.,
                  layer_scale_init_value=1e-6,
+                 linear_pw_conv=True,
+                 mlp_ratio=4,
+                 conv_kernel_sizes=[3, 5, 7, 9],
+                 use_pos_embd_csa=[False, True, False, False],
+                 use_pos_embd_global=False,
+                 d2_scales=[2, 2, 3, 4],
+                 norm_cfg=dict(type='LN2d', eps=1e-6),
                  out_indices=-1,
                  frozen_stages=0,
                  gap_before_final_norm=True,
-                 with_cp=False,
-                 init_cfg=[
-                     dict(
-                         type='TruncNormal',
-                         layer=['Conv2d', 'Linear'],
-                         std=.02,
-                         bias=0.),
-                     dict(
-                         type='Constant', layer=['LayerNorm'], val=1.,
-                         bias=0.),
-                 ]):
-        super().__init__(init_cfg=init_cfg)
+                 act_cfg=dict(type='GELU'),
+                 init_cfg=None):
+        super(EdgeNeXt, self).__init__(init_cfg=init_cfg)
 
         if isinstance(arch, str):
+            arch = arch.lower()
             assert arch in self.arch_settings, \
-                f'Unavailable arch, please choose from ' \
-                f'({set(self.arch_settings)}) or pass a dict.'
-            arch = self.arch_settings[arch]
+                f'Arch {arch} is not in default archs ' \
+                f'{set(self.arch_settings)}'
+            self.arch_settings = self.arch_settings[arch]
         elif isinstance(arch, dict):
-            assert 'depths' in arch and 'channels' in arch, \
-                f'The arch dict must have "depths" and "channels", ' \
-                f'but got {list(arch.keys())}.'
-
-        self.depths = arch['depths']
-        self.channels = arch['channels']
-        assert (isinstance(self.depths, Sequence)
-                and isinstance(self.channels, Sequence)
-                and len(self.depths) == len(self.channels)), \
-            f'The "depths" ({self.depths}) and "channels" ({self.channels}) ' \
-            'should be both sequence with the same length.'
+            essential_keys = {'channels', 'depths', 'num_heads'}
+            assert isinstance(arch, dict) and set(arch) == essential_keys, \
+                f'Custom arch needs a dict with keys {essential_keys}'
+            self.arch_settings = arch
+
+        self.channels = self.arch_settings['channels']
+        self.depths = self.arch_settings['depths']
+        self.num_heads = self.arch_settings['num_heads']
+        self.num_layers = len(self.depths)
+        self.use_pos_embd_global = use_pos_embd_global
+
+        for g in global_block_type:
+            assert g in ['None',
+                         'SDTA'], f'Global block type {g} is not supported'
 
         self.num_stages = len(self.depths)
 
         if isinstance(out_indices, int):
             out_indices = [out_indices]
         assert isinstance(out_indices, Sequence), \
             f'"out_indices" must by a sequence or int, ' \
@@ -275,94 +280,119 @@
                 out_indices[i] = 4 + index
                 assert out_indices[i] >= 0, f'Invalid out_indices {index}'
         self.out_indices = out_indices
 
         self.frozen_stages = frozen_stages
         self.gap_before_final_norm = gap_before_final_norm
 
+        if self.use_pos_embd_global:
+            self.pos_embed = PositionEncodingFourier(
+                embed_dims=self.channels[0])
+        else:
+            self.pos_embed = None
+
         # stochastic depth decay rule
         dpr = [
             x.item()
             for x in torch.linspace(0, drop_path_rate, sum(self.depths))
         ]
-        block_idx = 0
 
-        # 4 downsample layers between stages, including the stem layer.
         self.downsample_layers = ModuleList()
         stem = nn.Sequential(
-            nn.Conv2d(
-                in_channels,
-                self.channels[0],
-                kernel_size=stem_patch_size,
-                stride=stem_patch_size),
+            nn.Conv2d(in_channels, self.channels[0], kernel_size=4, stride=4),
             build_norm_layer(norm_cfg, self.channels[0]),
         )
         self.downsample_layers.append(stem)
 
-        # 4 feature resolution stages, each consisting of multiple residual
-        # blocks
-        self.stages = nn.ModuleList()
-
+        self.stages = ModuleList()
+        block_idx = 0
         for i in range(self.num_stages):
             depth = self.depths[i]
             channels = self.channels[i]
 
             if i >= 1:
                 downsample_layer = nn.Sequential(
                     build_norm_layer(norm_cfg, self.channels[i - 1]),
                     nn.Conv2d(
                         self.channels[i - 1],
                         channels,
                         kernel_size=2,
-                        stride=2),
-                )
+                        stride=2,
+                    ))
                 self.downsample_layers.append(downsample_layer)
 
-            stage = Sequential(*[
-                ConvNeXtBlock(
-                    in_channels=channels,
-                    drop_path_rate=dpr[block_idx + j],
-                    norm_cfg=norm_cfg,
-                    act_cfg=act_cfg,
-                    linear_pw_conv=linear_pw_conv,
-                    layer_scale_init_value=layer_scale_init_value,
-                    use_grn=use_grn,
-                    with_cp=with_cp) for j in range(depth)
-            ])
+            stage_blocks = []
+            for j in range(depth):
+                if j > depth - global_blocks[i] - 1:
+                    stage_blocks.append(
+                        SDTAEncoder(
+                            in_channel=channels,
+                            drop_path_rate=dpr[block_idx + j],
+                            mlp_ratio=mlp_ratio,
+                            scales=d2_scales[i],
+                            use_pos_emb=use_pos_embd_csa[i],
+                            num_heads=self.num_heads[i],
+                        ))
+                else:
+                    dw_conv_cfg = dict(
+                        kernel_size=conv_kernel_sizes[i],
+                        padding=conv_kernel_sizes[i] // 2,
+                    )
+                    stage_blocks.append(
+                        ConvNeXtBlock(
+                            in_channels=channels,
+                            dw_conv_cfg=dw_conv_cfg,
+                            norm_cfg=norm_cfg,
+                            act_cfg=act_cfg,
+                            linear_pw_conv=linear_pw_conv,
+                            drop_path_rate=dpr[block_idx + j],
+                            layer_scale_init_value=layer_scale_init_value,
+                        ))
             block_idx += depth
 
-            self.stages.append(stage)
+            stage_blocks = Sequential(*stage_blocks)
+            self.stages.append(stage_blocks)
 
             if i in self.out_indices:
-                norm_layer = build_norm_layer(norm_cfg, channels)
+                out_norm_cfg = dict(type='LN') if self.gap_before_final_norm \
+                    else norm_cfg
+                norm_layer = build_norm_layer(out_norm_cfg, channels)
                 self.add_module(f'norm{i}', norm_layer)
 
-        self._freeze_stages()
+    def init_weights(self) -> None:
+        # TODO: need to be implemented in the future
+        return super().init_weights()
 
     def forward(self, x):
         outs = []
         for i, stage in enumerate(self.stages):
             x = self.downsample_layers[i](x)
             x = stage(x)
+            if self.pos_embed and i == 0:
+                B, _, H, W = x.shape
+                x += self.pos_embed((B, H, W))
+
             if i in self.out_indices:
                 norm_layer = getattr(self, f'norm{i}')
                 if self.gap_before_final_norm:
                     gap = x.mean([-2, -1], keepdim=True)
-                    outs.append(norm_layer(gap).flatten(1))
+                    outs.append(norm_layer(gap.flatten(1)))
                 else:
-                    outs.append(norm_layer(x))
+                    # The output of LayerNorm2d may be discontiguous, which
+                    # may cause some problem in the downstream tasks
+                    outs.append(norm_layer(x).contiguous())
 
         return tuple(outs)
 
     def _freeze_stages(self):
         for i in range(self.frozen_stages):
             downsample_layer = self.downsample_layers[i]
             stage = self.stages[i]
             downsample_layer.eval()
             stage.eval()
             for param in chain(downsample_layer.parameters(),
                                stage.parameters()):
                 param.requires_grad = False
 
     def train(self, mode=True):
-        super(ConvNeXt, self).train(mode)
+        super(EdgeNeXt, self).train(mode)
         self._freeze_stages()
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/cspnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/cspnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/davit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/davit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/deit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/deit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/deit3.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/deit3.py`

 * *Files 0% similar despite different names*

```diff
@@ -122,15 +122,15 @@
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         qkv_bias (bool): enable bias for qkv if True. Defaults to True.
         use_layer_scale (bool): Whether to use layer_scale in
             DeiT3TransformerEncoderLayer. Defaults to True.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         init_cfg (dict, optional): Initialization config dict.
             Defaults to None.
     """
 
     def __init__(self,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/densenet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/densenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/edgenext.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/riformer.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,398 +1,390 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import math
-from itertools import chain
 from typing import Sequence
 
 import torch
 import torch.nn as nn
-from mmcv.cnn.bricks import DropPath
-from mmengine.model import BaseModule, ModuleList, Sequential
+from mmcv.cnn.bricks import DropPath, build_norm_layer
+from mmengine.model import BaseModule
 
 from mmpretrain.registry import MODELS
-from ..utils import (ChannelMultiheadAttention, PositionEncodingFourier,
-                     build_norm_layer)
 from .base_backbone import BaseBackbone
-from .convnext import ConvNeXtBlock
+from .poolformer import Mlp, PatchEmbed
 
 
-class SDTAEncoder(BaseModule):
-    """A PyTorch implementation of split depth-wise transpose attention (SDTA)
-    encoder.
+class Affine(nn.Module):
+    """Affine Transformation module.
 
-    Inspiration from
-    https://github.com/mmaaz60/EdgeNeXt
     Args:
-        in_channel (int): Number of input channels.
-        drop_path_rate (float): Stochastic depth dropout rate.
-            Defaults to 0.
-        layer_scale_init_value (float): Initial value of layer scale.
-            Defaults to 1e-6.
-        mlp_ratio (int): Number of channels ratio in the MLP.
-            Defaults to 4.
-        use_pos_emb (bool): Whether to use position encoding.
-            Defaults to True.
-        num_heads (int): Number of heads in the multihead attention.
-            Defaults to 8.
-        qkv_bias (bool): Whether to use bias in the multihead attention.
-            Defaults to True.
-        attn_drop (float): Dropout rate of the attention.
-            Defaults to 0.
-        proj_drop (float): Dropout rate of the projection.
-            Defaults to 0.
-        layer_scale_init_value (float): Initial value of layer scale.
-            Defaults to 1e-6.
-        norm_cfg (dict): Dictionary to construct normalization layer.
-            Defaults to ``dict(type='LN')``.
-        act_cfg (dict): Dictionary to construct activation layer.
-            Defaults to ``dict(type='GELU')``.
-        scales (int): Number of scales. Default to 1.
+        in_features (int): Input dimension.
+    """
+
+    def __init__(self, in_features):
+        super().__init__()
+        self.affine = nn.Conv2d(
+            in_features,
+            in_features,
+            kernel_size=1,
+            stride=1,
+            padding=0,
+            groups=in_features,
+            bias=True)
+
+    def forward(self, x):
+        return self.affine(x) - x
+
+
+class RIFormerBlock(BaseModule):
+    """RIFormer Block.
+
+    Args:
+        dim (int): Embedding dim.
+        mlp_ratio (float): Mlp expansion ratio. Defaults to 4.
+        norm_cfg (dict): The config dict for norm layers.
+            Defaults to ``dict(type='GN', num_groups=1)``.
+        act_cfg (dict): The config dict for activation between pointwise
+            convolution. Defaults to ``dict(type='GELU')``.
+        drop (float): Dropout rate. Defaults to 0.
+        drop_path (float): Stochastic depth rate. Defaults to 0.
+        layer_scale_init_value (float): Init value for Layer Scale.
+            Defaults to 1e-5.
+        deploy (bool): Whether to switch the model structure to
+            deployment mode. Default: False.
     """
 
     def __init__(self,
-                 in_channel,
-                 drop_path_rate=0.,
-                 layer_scale_init_value=1e-6,
-                 mlp_ratio=4,
-                 use_pos_emb=True,
-                 num_heads=8,
-                 qkv_bias=True,
-                 attn_drop=0.,
-                 proj_drop=0.,
-                 norm_cfg=dict(type='LN'),
+                 dim,
+                 mlp_ratio=4.,
+                 norm_cfg=dict(type='GN', num_groups=1),
                  act_cfg=dict(type='GELU'),
-                 scales=1,
-                 init_cfg=None):
-        super(SDTAEncoder, self).__init__(init_cfg=init_cfg)
-        conv_channels = max(
-            int(math.ceil(in_channel / scales)),
-            int(math.floor(in_channel // scales)))
-        self.conv_channels = conv_channels
-        self.num_convs = scales if scales == 1 else scales - 1
-
-        self.conv_modules = ModuleList()
-        for i in range(self.num_convs):
-            self.conv_modules.append(
-                nn.Conv2d(
-                    conv_channels,
-                    conv_channels,
-                    kernel_size=3,
-                    padding=1,
-                    groups=conv_channels))
-
-        self.pos_embed = PositionEncodingFourier(
-            embed_dims=in_channel) if use_pos_emb else None
-
-        self.norm_csa = build_norm_layer(norm_cfg, in_channel)
-        self.gamma_csa = nn.Parameter(
-            layer_scale_init_value * torch.ones(in_channel),
-            requires_grad=True) if layer_scale_init_value > 0 else None
-        self.csa = ChannelMultiheadAttention(
-            embed_dims=in_channel,
-            num_heads=num_heads,
-            qkv_bias=qkv_bias,
-            attn_drop=attn_drop,
-            proj_drop=proj_drop)
-
-        self.norm = build_norm_layer(norm_cfg, in_channel)
-        self.pointwise_conv1 = nn.Linear(in_channel, mlp_ratio * in_channel)
-        self.act = MODELS.build(act_cfg)
-        self.pointwise_conv2 = nn.Linear(mlp_ratio * in_channel, in_channel)
-        self.gamma = nn.Parameter(
-            layer_scale_init_value * torch.ones(in_channel),
-            requires_grad=True) if layer_scale_init_value > 0 else None
-        self.drop_path = DropPath(
-            drop_path_rate) if drop_path_rate > 0. else nn.Identity()
+                 drop=0.,
+                 drop_path=0.,
+                 layer_scale_init_value=1e-5,
+                 deploy=False):
 
-    def forward(self, x):
-        shortcut = x
-        spx = torch.split(x, self.conv_channels, dim=1)
-        for i in range(self.num_convs):
-            if i == 0:
-                sp = spx[i]
-            else:
-                sp = sp + spx[i]
-            sp = self.conv_modules[i](sp)
-            if i == 0:
-                out = sp
-            else:
-                out = torch.cat((out, sp), 1)
-
-        x = torch.cat((out, spx[self.num_convs]), 1)
-
-        # Channel Self-attention
-        B, C, H, W = x.shape
-        x = x.reshape(B, C, H * W).permute(0, 2, 1)
-        if self.pos_embed:
-            pos_encoding = self.pos_embed((B, H, W))
-            pos_encoding = pos_encoding.reshape(B, -1,
-                                                x.shape[1]).permute(0, 2, 1)
-            x += pos_encoding
-
-        x = x + self.drop_path(self.gamma_csa * self.csa(self.norm_csa(x)))
-        x = x.reshape(B, H, W, C)
-
-        # Inverted Bottleneck
-        x = self.norm(x)
-        x = self.pointwise_conv1(x)
-        x = self.act(x)
-        x = self.pointwise_conv2(x)
-
-        if self.gamma is not None:
-            x = self.gamma * x
-        x = x.permute(0, 3, 1, 2)  # (B, H, W, C) -> (B, C, H, W)
+        super().__init__()
 
-        x = shortcut + self.drop_path(x)
+        if deploy:
+            self.norm_reparam = build_norm_layer(norm_cfg, dim)[1]
+        else:
+            self.norm1 = build_norm_layer(norm_cfg, dim)[1]
+            self.token_mixer = Affine(in_features=dim)
+        self.norm2 = build_norm_layer(norm_cfg, dim)[1]
+        mlp_hidden_dim = int(dim * mlp_ratio)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_cfg=act_cfg,
+            drop=drop)
+
+        # The following two techniques are useful to train deep RIFormers.
+        self.drop_path = DropPath(drop_path) if drop_path > 0. \
+            else nn.Identity()
+        self.layer_scale_1 = nn.Parameter(
+            layer_scale_init_value * torch.ones((dim)), requires_grad=True)
+        self.layer_scale_2 = nn.Parameter(
+            layer_scale_init_value * torch.ones((dim)), requires_grad=True)
+        self.norm_cfg = norm_cfg
+        self.dim = dim
+        self.deploy = deploy
 
+    def forward(self, x):
+        if hasattr(self, 'norm_reparam'):
+            x = x + self.drop_path(
+                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) *
+                self.norm_reparam(x))
+            x = x + self.drop_path(
+                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) *
+                self.mlp(self.norm2(x)))
+        else:
+            x = x + self.drop_path(
+                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) *
+                self.token_mixer(self.norm1(x)))
+            x = x + self.drop_path(
+                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) *
+                self.mlp(self.norm2(x)))
         return x
 
+    def fuse_affine(self, norm, token_mixer):
+        gamma_affn = token_mixer.affine.weight.reshape(-1)
+        gamma_affn = gamma_affn - torch.ones_like(gamma_affn)
+        beta_affn = token_mixer.affine.bias
+        gamma_ln = norm.weight
+        beta_ln = norm.bias
+        return (gamma_ln * gamma_affn), (beta_ln * gamma_affn + beta_affn)
+
+    def get_equivalent_scale_bias(self):
+        eq_s, eq_b = self.fuse_affine(self.norm1, self.token_mixer)
+        return eq_s, eq_b
+
+    def switch_to_deploy(self):
+        if self.deploy:
+            return
+        eq_s, eq_b = self.get_equivalent_scale_bias()
+        self.norm_reparam = build_norm_layer(self.norm_cfg, self.dim)[1]
+        self.norm_reparam.weight.data = eq_s
+        self.norm_reparam.bias.data = eq_b
+        self.__delattr__('norm1')
+        if hasattr(self, 'token_mixer'):
+            self.__delattr__('token_mixer')
+        self.deploy = True
+
+
+def basic_blocks(dim,
+                 index,
+                 layers,
+                 mlp_ratio=4.,
+                 norm_cfg=dict(type='GN', num_groups=1),
+                 act_cfg=dict(type='GELU'),
+                 drop_rate=.0,
+                 drop_path_rate=0.,
+                 layer_scale_init_value=1e-5,
+                 deploy=False):
+    """generate RIFormer blocks for a stage."""
+    blocks = []
+    for block_idx in range(layers[index]):
+        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (
+            sum(layers) - 1)
+        blocks.append(
+            RIFormerBlock(
+                dim,
+                mlp_ratio=mlp_ratio,
+                norm_cfg=norm_cfg,
+                act_cfg=act_cfg,
+                drop=drop_rate,
+                drop_path=block_dpr,
+                layer_scale_init_value=layer_scale_init_value,
+                deploy=deploy,
+            ))
+    blocks = nn.Sequential(*blocks)
+
+    return blocks
 
-@MODELS.register_module()
-class EdgeNeXt(BaseBackbone):
-    """EdgeNeXt.
 
-    A PyTorch implementation of: `EdgeNeXt: Efficiently Amalgamated
-    CNN-Transformer Architecture for Mobile Vision Applications
-    <https://arxiv.org/abs/2206.10589>`_
+@MODELS.register_module()
+class RIFormer(BaseBackbone):
+    """RIFormer.
 
-    Inspiration from
-    https://github.com/mmaaz60/EdgeNeXt
+    A PyTorch implementation of RIFormer introduced by:
+    `RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer <https://arxiv.org/abs/xxxx.xxxxx>`_
 
     Args:
         arch (str | dict): The model's architecture. If string, it should be
-            one of architectures in ``EdgeNeXt.arch_settings``.
-            And if dict, it should include the following keys:
+            one of architecture in ``RIFormer.arch_settings``. And if dict, it
+            should include the following two keys:
 
-            - channels (list[int]): The number of channels at each stage.
-            - depths (list[int]): The number of blocks at each stage.
-            - num_heads (list[int]): The number of heads at each stage.
+            - layers (list[int]): Number of blocks at each stage.
+            - embed_dims (list[int]): The number of channels at each stage.
+            - mlp_ratios (list[int]): Expansion ratio of MLPs.
+            - layer_scale_init_value (float): Init value for Layer Scale.
 
-            Defaults to 'xxsmall'.
-        in_channels (int): The number of input channels.
-            Defaults to 3.
-        global_blocks (list[int]): The number of global blocks.
-            Defaults to [0, 1, 1, 1].
-        global_block_type (list[str]): The type of global blocks.
-            Defaults to ['None', 'SDTA', 'SDTA', 'SDTA'].
-        drop_path_rate (float): Stochastic depth dropout rate.
-            Defaults to 0.
-        layer_scale_init_value (float): Initial value of layer scale.
-            Defaults to 1e-6.
-        linear_pw_conv (bool): Whether to use linear layer to do pointwise
-            convolution. Defaults to False.
-        mlp_ratio (int): The number of channel ratio in MLP layers.
-            Defaults to 4.
-        conv_kernel_size (list[int]): The kernel size of convolutional layers
-            at each stage. Defaults to [3, 5, 7, 9].
-        use_pos_embd_csa (list[bool]): Whether to use positional embedding in
-            Channel Self-Attention. Defaults to [False, True, False, False].
-        use_pos_emebd_global (bool): Whether to use positional embedding for
-            whole network. Defaults to False.
-        d2_scales (list[int]): The number of channel groups used for SDTA at
-            each stage. Defaults to [2, 2, 3, 4].
-        norm_cfg (dict): The config of normalization layer.
+            Defaults to 'S12'.
+
+        norm_cfg (dict): The config dict for norm layers.
             Defaults to ``dict(type='LN2d', eps=1e-6)``.
-        out_indices (Sequence | int): Output from which stages.
+        act_cfg (dict): The config dict for activation between pointwise
+            convolution. Defaults to ``dict(type='GELU')``.
+        in_patch_size (int): The patch size of/? input image patch embedding.
+            Defaults to 7.
+        in_stride (int): The stride of input image patch embedding.
+            Defaults to 4.
+        in_pad (int): The padding of input image patch embedding.
+            Defaults to 2.
+        down_patch_size (int): The patch size of downsampling patch embedding.
+            Defaults to 3.
+        down_stride (int): The stride of downsampling patch embedding.
+            Defaults to 2.
+        down_pad (int): The padding of downsampling patch embedding.
+            Defaults to 1.
+        drop_rate (float): Dropout rate. Defaults to 0.
+        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
+        out_indices (Sequence | int): Output from which network position.
+            Index 0-6 respectively corresponds to
+            [stage1, downsampling, stage2, downsampling, stage3, downsampling, stage4]
             Defaults to -1, means the last stage.
         frozen_stages (int): Stages to be frozen (all param fixed).
-            Defaults to 0, which means not freezing any parameters.
-        gap_before_final_norm (bool): Whether to globally average the feature
-            map before the final norm layer. Defaults to True.
-        act_cfg (dict): The config of activation layer.
-            Defaults to ``dict(type='GELU')``.
-        init_cfg (dict, optional): Config for initialization.
-            Defaults to None.
-    """
+            Defaults to -1, which means not freezing any parameters.
+        deploy (bool): Whether to switch the model structure to
+            deployment mode. Default: False.
+        init_cfg (dict, optional): Initialization config dict
+    """  # noqa: E501
+
+    # --layers: [x,x,x,x], numbers of layers for the four stages
+    # --embed_dims, --mlp_ratios:
+    #     embedding dims and mlp ratios for the four stages
+    # --downsamples: flags to apply downsampling or not in four blocks
     arch_settings = {
-        'xxsmall': {  # parameters: 1.3M
-            'channels': [24, 48, 88, 168],
-            'depths': [2, 2, 6, 2],
-            'num_heads': [4, 4, 4, 4]
+        's12': {
+            'layers': [2, 2, 6, 2],
+            'embed_dims': [64, 128, 320, 512],
+            'mlp_ratios': [4, 4, 4, 4],
+            'layer_scale_init_value': 1e-5,
         },
-        'xsmall': {  # parameters: 2.3M
-            'channels': [32, 64, 100, 192],
-            'depths': [3, 3, 9, 3],
-            'num_heads': [4, 4, 4, 4]
+        's24': {
+            'layers': [4, 4, 12, 4],
+            'embed_dims': [64, 128, 320, 512],
+            'mlp_ratios': [4, 4, 4, 4],
+            'layer_scale_init_value': 1e-5,
         },
-        'small': {  # parameters: 5.6M
-            'channels': [48, 96, 160, 304],
-            'depths': [3, 3, 9, 3],
-            'num_heads': [8, 8, 8, 8]
+        's36': {
+            'layers': [6, 6, 18, 6],
+            'embed_dims': [64, 128, 320, 512],
+            'mlp_ratios': [4, 4, 4, 4],
+            'layer_scale_init_value': 1e-6,
         },
-        'base': {  # parameters: 18.51M
-            'channels': [80, 160, 288, 584],
-            'depths': [3, 3, 9, 3],
-            'num_heads': [8, 8, 8, 8]
+        'm36': {
+            'layers': [6, 6, 18, 6],
+            'embed_dims': [96, 192, 384, 768],
+            'mlp_ratios': [4, 4, 4, 4],
+            'layer_scale_init_value': 1e-6,
+        },
+        'm48': {
+            'layers': [8, 8, 24, 8],
+            'embed_dims': [96, 192, 384, 768],
+            'mlp_ratios': [4, 4, 4, 4],
+            'layer_scale_init_value': 1e-6,
         },
     }
 
     def __init__(self,
-                 arch='xxsmall',
+                 arch='s12',
                  in_channels=3,
-                 global_blocks=[0, 1, 1, 1],
-                 global_block_type=['None', 'SDTA', 'SDTA', 'SDTA'],
+                 norm_cfg=dict(type='GN', num_groups=1),
+                 act_cfg=dict(type='GELU'),
+                 in_patch_size=7,
+                 in_stride=4,
+                 in_pad=2,
+                 down_patch_size=3,
+                 down_stride=2,
+                 down_pad=1,
+                 drop_rate=0.,
                  drop_path_rate=0.,
-                 layer_scale_init_value=1e-6,
-                 linear_pw_conv=True,
-                 mlp_ratio=4,
-                 conv_kernel_sizes=[3, 5, 7, 9],
-                 use_pos_embd_csa=[False, True, False, False],
-                 use_pos_embd_global=False,
-                 d2_scales=[2, 2, 3, 4],
-                 norm_cfg=dict(type='LN2d', eps=1e-6),
                  out_indices=-1,
-                 frozen_stages=0,
-                 gap_before_final_norm=True,
-                 act_cfg=dict(type='GELU'),
-                 init_cfg=None):
-        super(EdgeNeXt, self).__init__(init_cfg=init_cfg)
+                 frozen_stages=-1,
+                 init_cfg=None,
+                 deploy=False):
+
+        super().__init__(init_cfg=init_cfg)
 
         if isinstance(arch, str):
-            arch = arch.lower()
             assert arch in self.arch_settings, \
-                f'Arch {arch} is not in default archs ' \
-                f'{set(self.arch_settings)}'
-            self.arch_settings = self.arch_settings[arch]
+                f'Unavailable arch, please choose from ' \
+                f'({set(self.arch_settings)}) or pass a dict.'
+            arch = self.arch_settings[arch]
         elif isinstance(arch, dict):
-            essential_keys = {'channels', 'depths', 'num_heads'}
-            assert isinstance(arch, dict) and set(arch) == essential_keys, \
-                f'Custom arch needs a dict with keys {essential_keys}'
-            self.arch_settings = arch
-
-        self.channels = self.arch_settings['channels']
-        self.depths = self.arch_settings['depths']
-        self.num_heads = self.arch_settings['num_heads']
-        self.num_layers = len(self.depths)
-        self.use_pos_embd_global = use_pos_embd_global
-
-        for g in global_block_type:
-            assert g in ['None',
-                         'SDTA'], f'Global block type {g} is not supported'
+            assert 'layers' in arch and 'embed_dims' in arch, \
+                f'The arch dict must have "layers" and "embed_dims", ' \
+                f'but got {list(arch.keys())}.'
+
+        layers = arch['layers']
+        embed_dims = arch['embed_dims']
+        mlp_ratios = arch['mlp_ratios'] \
+            if 'mlp_ratios' in arch else [4, 4, 4, 4]
+        layer_scale_init_value = arch['layer_scale_init_value'] \
+            if 'layer_scale_init_value' in arch else 1e-5
+
+        self.patch_embed = PatchEmbed(
+            patch_size=in_patch_size,
+            stride=in_stride,
+            padding=in_pad,
+            in_chans=in_channels,
+            embed_dim=embed_dims[0])
+
+        # set the main block in network
+        network = []
+        for i in range(len(layers)):
+            stage = basic_blocks(
+                embed_dims[i],
+                i,
+                layers,
+                mlp_ratio=mlp_ratios[i],
+                norm_cfg=norm_cfg,
+                act_cfg=act_cfg,
+                drop_rate=drop_rate,
+                drop_path_rate=drop_path_rate,
+                layer_scale_init_value=layer_scale_init_value,
+                deploy=deploy)
+            network.append(stage)
+            if i >= len(layers) - 1:
+                break
+            if embed_dims[i] != embed_dims[i + 1]:
+                # downsampling between two stages
+                network.append(
+                    PatchEmbed(
+                        patch_size=down_patch_size,
+                        stride=down_stride,
+                        padding=down_pad,
+                        in_chans=embed_dims[i],
+                        embed_dim=embed_dims[i + 1]))
 
-        self.num_stages = len(self.depths)
+        self.network = nn.ModuleList(network)
 
         if isinstance(out_indices, int):
             out_indices = [out_indices]
         assert isinstance(out_indices, Sequence), \
             f'"out_indices" must by a sequence or int, ' \
             f'get {type(out_indices)} instead.'
         for i, index in enumerate(out_indices):
             if index < 0:
-                out_indices[i] = 4 + index
+                out_indices[i] = 7 + index
                 assert out_indices[i] >= 0, f'Invalid out_indices {index}'
         self.out_indices = out_indices
+        if self.out_indices:
+            for i_layer in self.out_indices:
+                layer = build_norm_layer(norm_cfg,
+                                         embed_dims[(i_layer + 1) // 2])[1]
+                layer_name = f'norm{i_layer}'
+                self.add_module(layer_name, layer)
 
         self.frozen_stages = frozen_stages
-        self.gap_before_final_norm = gap_before_final_norm
-
-        if self.use_pos_embd_global:
-            self.pos_embed = PositionEncodingFourier(
-                embed_dims=self.channels[0])
-        else:
-            self.pos_embed = None
-
-        # stochastic depth decay rule
-        dpr = [
-            x.item()
-            for x in torch.linspace(0, drop_path_rate, sum(self.depths))
-        ]
-
-        self.downsample_layers = ModuleList()
-        stem = nn.Sequential(
-            nn.Conv2d(in_channels, self.channels[0], kernel_size=4, stride=4),
-            build_norm_layer(norm_cfg, self.channels[0]),
-        )
-        self.downsample_layers.append(stem)
-
-        self.stages = ModuleList()
-        block_idx = 0
-        for i in range(self.num_stages):
-            depth = self.depths[i]
-            channels = self.channels[i]
-
-            if i >= 1:
-                downsample_layer = nn.Sequential(
-                    build_norm_layer(norm_cfg, self.channels[i - 1]),
-                    nn.Conv2d(
-                        self.channels[i - 1],
-                        channels,
-                        kernel_size=2,
-                        stride=2,
-                    ))
-                self.downsample_layers.append(downsample_layer)
-
-            stage_blocks = []
-            for j in range(depth):
-                if j > depth - global_blocks[i] - 1:
-                    stage_blocks.append(
-                        SDTAEncoder(
-                            in_channel=channels,
-                            drop_path_rate=dpr[block_idx + j],
-                            mlp_ratio=mlp_ratio,
-                            scales=d2_scales[i],
-                            use_pos_emb=use_pos_embd_csa[i],
-                            num_heads=self.num_heads[i],
-                        ))
-                else:
-                    dw_conv_cfg = dict(
-                        kernel_size=conv_kernel_sizes[i],
-                        padding=conv_kernel_sizes[i] // 2,
-                    )
-                    stage_blocks.append(
-                        ConvNeXtBlock(
-                            in_channels=channels,
-                            dw_conv_cfg=dw_conv_cfg,
-                            norm_cfg=norm_cfg,
-                            act_cfg=act_cfg,
-                            linear_pw_conv=linear_pw_conv,
-                            drop_path_rate=dpr[block_idx + j],
-                            layer_scale_init_value=layer_scale_init_value,
-                        ))
-            block_idx += depth
-
-            stage_blocks = Sequential(*stage_blocks)
-            self.stages.append(stage_blocks)
+        self._freeze_stages()
+        self.deploy = deploy
 
-            if i in self.out_indices:
-                out_norm_cfg = dict(type='LN') if self.gap_before_final_norm \
-                    else norm_cfg
-                norm_layer = build_norm_layer(out_norm_cfg, channels)
-                self.add_module(f'norm{i}', norm_layer)
-
-    def init_weights(self) -> None:
-        # TODO: need to be implemented in the future
-        return super().init_weights()
+    def forward_embeddings(self, x):
+        x = self.patch_embed(x)
+        return x
 
-    def forward(self, x):
+    def forward_tokens(self, x):
         outs = []
-        for i, stage in enumerate(self.stages):
-            x = self.downsample_layers[i](x)
-            x = stage(x)
-            if self.pos_embed and i == 0:
-                B, _, H, W = x.shape
-                x += self.pos_embed((B, H, W))
-
-            if i in self.out_indices:
-                norm_layer = getattr(self, f'norm{i}')
-                if self.gap_before_final_norm:
-                    gap = x.mean([-2, -1], keepdim=True)
-                    outs.append(norm_layer(gap.flatten(1)))
-                else:
-                    # The output of LayerNorm2d may be discontiguous, which
-                    # may cause some problem in the downstream tasks
-                    outs.append(norm_layer(x).contiguous())
-
+        for idx, block in enumerate(self.network):
+            x = block(x)
+            if idx in self.out_indices:
+                norm_layer = getattr(self, f'norm{idx}')
+                x_out = norm_layer(x)
+                outs.append(x_out)
         return tuple(outs)
 
+    def forward(self, x):
+        # input embedding
+        x = self.forward_embeddings(x)
+        # through backbone
+        x = self.forward_tokens(x)
+        return x
+
     def _freeze_stages(self):
-        for i in range(self.frozen_stages):
-            downsample_layer = self.downsample_layers[i]
-            stage = self.stages[i]
-            downsample_layer.eval()
-            stage.eval()
-            for param in chain(downsample_layer.parameters(),
-                               stage.parameters()):
+        if self.frozen_stages >= 0:
+            self.patch_embed.eval()
+            for param in self.patch_embed.parameters():
+                param.requires_grad = False
+
+        for i in range(0, self.frozen_stages + 1):
+            # Include both block and downsample layer.
+            module = self.network[i]
+            module.eval()
+            for param in module.parameters():
                 param.requires_grad = False
+            if i in self.out_indices:
+                norm_layer = getattr(self, f'norm{i}')
+                norm_layer.eval()
+                for param in norm_layer.parameters():
+                    param.requires_grad = False
 
     def train(self, mode=True):
-        super(EdgeNeXt, self).train(mode)
+        super(RIFormer, self).train(mode)
         self._freeze_stages()
+        return self
+
+    def switch_to_deploy(self):
+        for m in self.modules():
+            if isinstance(m, RIFormerBlock):
+                m.switch_to_deploy()
+        self.deploy = True
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/efficientformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/efficientnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/efficientnet_v2.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/efficientnet_v2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/hornet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/hornet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/hrnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/hrnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/inception_v3.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/inception_v3.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/lenet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/lenet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/levit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/levit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mixmim.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mixmim.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mlp_mixer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mlp_mixer.py`

 * *Files 0% similar despite different names*

```diff
@@ -24,15 +24,15 @@
         channels_mlp_dims (int): The hidden dimension for channels FFNs
         drop_rate (float): Probability of an element to be zeroed
             after the feed forward layer. Defaults to 0.
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         init_cfg (dict, optional): Initialization config dict.
             Defaults to None.
     """
 
     def __init__(self,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilenet_v2.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mobilenet_v2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilenet_v3.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mobilenet_v3.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobileone.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mobileone.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mobilevit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mobilevit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/mvit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/mvit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/poolformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/poolformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/regnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/regnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/replknet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/replknet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/repmlp.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/repmlp.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/repvgg.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/repvgg.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/res2net.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/res2net.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnest.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/resnest.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/resnet.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+import math
+
 import torch
 import torch.nn as nn
 import torch.utils.checkpoint as cp
 from mmcv.cnn import (ConvModule, build_activation_layer, build_conv_layer,
                       build_norm_layer)
 from mmcv.cnn.bricks import DropPath
 from mmengine.model import BaseModule
@@ -670,14 +672,72 @@
         self._freeze_stages()
         if mode and self.norm_eval:
             for m in self.modules():
                 # trick: eval have effect on BatchNorm only
                 if isinstance(m, _BatchNorm):
                     m.eval()
 
+    def get_layer_depth(self, param_name: str, prefix: str = ''):
+        """Get the layer id to set the different learning rates for ResNet.
+
+        ResNet stages:
+        50  :    [3, 4, 6, 3]
+        101 :    [3, 4, 23, 3]
+        152 :    [3, 8, 36, 3]
+        200 :    [3, 24, 36, 3]
+        eca269d: [3, 30, 48, 8]
+
+        Args:
+            param_name (str): The name of the parameter.
+            prefix (str): The prefix for the parameter.
+                Defaults to an empty string.
+
+        Returns:
+            Tuple[int, int]: The layer-wise depth and the num of layers.
+        """
+        depths = self.stage_blocks
+        if depths[1] == 4 and depths[2] == 6:
+            blk2, blk3 = 2, 3
+        elif depths[1] == 4 and depths[2] == 23:
+            blk2, blk3 = 2, 3
+        elif depths[1] == 8 and depths[2] == 36:
+            blk2, blk3 = 4, 4
+        elif depths[1] == 24 and depths[2] == 36:
+            blk2, blk3 = 4, 4
+        elif depths[1] == 30 and depths[2] == 48:
+            blk2, blk3 = 5, 6
+        else:
+            raise NotImplementedError
+
+        N2, N3 = math.ceil(depths[1] / blk2 -
+                           1e-5), math.ceil(depths[2] / blk3 - 1e-5)
+        N = 2 + N2 + N3  # r50: 2 + 2 + 2 = 6
+        max_layer_id = N + 1  # r50: 2 + 2 + 2 + 1(like head) = 7
+
+        if not param_name.startswith(prefix):
+            # For subsequent module like head
+            return max_layer_id, max_layer_id + 1
+
+        if param_name.startswith('backbone.layer'):
+            stage_id = int(param_name.split('.')[1][5:])
+            block_id = int(param_name.split('.')[2])
+
+            if stage_id == 1:
+                layer_id = 1
+            elif stage_id == 2:
+                layer_id = 2 + block_id // blk2  # r50: 2, 3
+            elif stage_id == 3:
+                layer_id = 2 + N2 + block_id // blk3  # r50: 4, 5
+            else:  # stage_id == 4
+                layer_id = N  # r50: 6
+            return layer_id, max_layer_id + 1
+
+        else:
+            return 0, max_layer_id + 1
+
 
 @MODELS.register_module()
 class ResNetV1c(ResNet):
     """ResNetV1c backbone.
 
     This variant is described in `Bag of Tricks.
     <https://arxiv.org/pdf/1812.01187.pdf>`_.
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnet_cifar.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/resnet_cifar.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/resnext.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/resnext.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/revvit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/revvit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/riformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/van.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,390 +1,434 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Sequence
-
 import torch
 import torch.nn as nn
-from mmcv.cnn.bricks import DropPath, build_norm_layer
-from mmengine.model import BaseModule
+from mmcv.cnn import Conv2d, build_activation_layer, build_norm_layer
+from mmcv.cnn.bricks import DropPath
+from mmcv.cnn.bricks.transformer import PatchEmbed
+from mmengine.model import BaseModule, ModuleList
+from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm
 
 from mmpretrain.registry import MODELS
 from .base_backbone import BaseBackbone
-from .poolformer import Mlp, PatchEmbed
 
 
-class Affine(nn.Module):
-    """Affine Transformation module.
+class MixFFN(BaseModule):
+    """An implementation of MixFFN of VAN. Refer to
+    mmdetection/mmdet/models/backbones/pvt.py.
+
+    The differences between MixFFN & FFN:
+        1. Use 1X1 Conv to replace Linear layer.
+        2. Introduce 3X3 Depth-wise Conv to encode positional information.
 
     Args:
-        in_features (int): Input dimension.
+        embed_dims (int): The feature dimension. Same as
+            `MultiheadAttention`.
+        feedforward_channels (int): The hidden dimension of FFNs.
+        act_cfg (dict, optional): The activation config for FFNs.
+            Default: dict(type='GELU').
+        ffn_drop (float, optional): Probability of an element to be
+            zeroed in FFN. Default 0.0.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
     """
 
-    def __init__(self, in_features):
-        super().__init__()
-        self.affine = nn.Conv2d(
-            in_features,
-            in_features,
-            kernel_size=1,
+    def __init__(self,
+                 embed_dims,
+                 feedforward_channels,
+                 act_cfg=dict(type='GELU'),
+                 ffn_drop=0.,
+                 init_cfg=None):
+        super(MixFFN, self).__init__(init_cfg=init_cfg)
+
+        self.embed_dims = embed_dims
+        self.feedforward_channels = feedforward_channels
+        self.act_cfg = act_cfg
+
+        self.fc1 = Conv2d(
+            in_channels=embed_dims,
+            out_channels=feedforward_channels,
+            kernel_size=1)
+        self.dwconv = Conv2d(
+            in_channels=feedforward_channels,
+            out_channels=feedforward_channels,
+            kernel_size=3,
             stride=1,
-            padding=0,
-            groups=in_features,
-            bias=True)
+            padding=1,
+            bias=True,
+            groups=feedforward_channels)
+        self.act = build_activation_layer(act_cfg)
+        self.fc2 = Conv2d(
+            in_channels=feedforward_channels,
+            out_channels=embed_dims,
+            kernel_size=1)
+        self.drop = nn.Dropout(ffn_drop)
+
+    def forward(self, x):
+        x = self.fc1(x)
+        x = self.dwconv(x)
+        x = self.act(x)
+        x = self.drop(x)
+        x = self.fc2(x)
+        x = self.drop(x)
+        return x
+
+
+class LKA(BaseModule):
+    """Large Kernel Attention(LKA) of VAN.
+
+    .. code:: text
+            DW_conv (depth-wise convolution)
+                            |
+                            |
+        DW_D_conv (depth-wise dilation convolution)
+                            |
+                            |
+        Transition Convolution (11 convolution)
+
+    Args:
+        embed_dims (int): Number of input channels.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self, embed_dims, init_cfg=None):
+        super(LKA, self).__init__(init_cfg=init_cfg)
+
+        # a spatial local convolution (depth-wise convolution)
+        self.DW_conv = Conv2d(
+            in_channels=embed_dims,
+            out_channels=embed_dims,
+            kernel_size=5,
+            padding=2,
+            groups=embed_dims)
+
+        # a spatial long-range convolution (depth-wise dilation convolution)
+        self.DW_D_conv = Conv2d(
+            in_channels=embed_dims,
+            out_channels=embed_dims,
+            kernel_size=7,
+            stride=1,
+            padding=9,
+            groups=embed_dims,
+            dilation=3)
+
+        self.conv1 = Conv2d(
+            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
+
+    def forward(self, x):
+        u = x.clone()
+        attn = self.DW_conv(x)
+        attn = self.DW_D_conv(attn)
+        attn = self.conv1(attn)
+
+        return u * attn
+
+
+class SpatialAttention(BaseModule):
+    """Basic attention module in VANBloack.
+
+    Args:
+        embed_dims (int): Number of input channels.
+        act_cfg (dict, optional): The activation config for FFNs.
+            Default: dict(type='GELU').
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self, embed_dims, act_cfg=dict(type='GELU'), init_cfg=None):
+        super(SpatialAttention, self).__init__(init_cfg=init_cfg)
+
+        self.proj_1 = Conv2d(
+            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
+        self.activation = build_activation_layer(act_cfg)
+        self.spatial_gating_unit = LKA(embed_dims)
+        self.proj_2 = Conv2d(
+            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
 
     def forward(self, x):
-        return self.affine(x) - x
+        shorcut = x.clone()
+        x = self.proj_1(x)
+        x = self.activation(x)
+        x = self.spatial_gating_unit(x)
+        x = self.proj_2(x)
+        x = x + shorcut
+        return x
 
 
-class RIFormerBlock(BaseModule):
-    """RIFormer Block.
+class VANBlock(BaseModule):
+    """A block of VAN.
 
     Args:
-        dim (int): Embedding dim.
-        mlp_ratio (float): Mlp expansion ratio. Defaults to 4.
-        norm_cfg (dict): The config dict for norm layers.
-            Defaults to ``dict(type='GN', num_groups=1)``.
-        act_cfg (dict): The config dict for activation between pointwise
-            convolution. Defaults to ``dict(type='GELU')``.
-        drop (float): Dropout rate. Defaults to 0.
-        drop_path (float): Stochastic depth rate. Defaults to 0.
+        embed_dims (int): Number of input channels.
+        ffn_ratio (float): The expansion ratio of feedforward network hidden
+            layer channels. Defaults to 4.
+        drop_rate (float): Dropout rate after embedding. Defaults to 0.
+        drop_path_rate (float): Stochastic depth rate. Defaults to 0.1.
+        act_cfg (dict, optional): The activation config for FFNs.
+            Default: dict(type='GELU').
         layer_scale_init_value (float): Init value for Layer Scale.
-            Defaults to 1e-5.
-        deploy (bool): Whether to switch the model structure to
-            deployment mode. Default: False.
+            Defaults to 1e-2.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
     """
 
     def __init__(self,
-                 dim,
-                 mlp_ratio=4.,
-                 norm_cfg=dict(type='GN', num_groups=1),
+                 embed_dims,
+                 ffn_ratio=4.,
+                 drop_rate=0.,
+                 drop_path_rate=0.,
                  act_cfg=dict(type='GELU'),
-                 drop=0.,
-                 drop_path=0.,
-                 layer_scale_init_value=1e-5,
-                 deploy=False):
-
-        super().__init__()
-
-        if deploy:
-            self.norm_reparam = build_norm_layer(norm_cfg, dim)[1]
-        else:
-            self.norm1 = build_norm_layer(norm_cfg, dim)[1]
-            self.token_mixer = Affine(in_features=dim)
-        self.norm2 = build_norm_layer(norm_cfg, dim)[1]
-        mlp_hidden_dim = int(dim * mlp_ratio)
-        self.mlp = Mlp(
-            in_features=dim,
-            hidden_features=mlp_hidden_dim,
+                 norm_cfg=dict(type='BN', eps=1e-5),
+                 layer_scale_init_value=1e-2,
+                 init_cfg=None):
+        super(VANBlock, self).__init__(init_cfg=init_cfg)
+        self.out_channels = embed_dims
+
+        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]
+        self.attn = SpatialAttention(embed_dims, act_cfg=act_cfg)
+        self.drop_path = DropPath(
+            drop_path_rate) if drop_path_rate > 0. else nn.Identity()
+
+        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]
+        mlp_hidden_dim = int(embed_dims * ffn_ratio)
+        self.mlp = MixFFN(
+            embed_dims=embed_dims,
+            feedforward_channels=mlp_hidden_dim,
             act_cfg=act_cfg,
-            drop=drop)
-
-        # The following two techniques are useful to train deep RIFormers.
-        self.drop_path = DropPath(drop_path) if drop_path > 0. \
-            else nn.Identity()
+            ffn_drop=drop_rate)
         self.layer_scale_1 = nn.Parameter(
-            layer_scale_init_value * torch.ones((dim)), requires_grad=True)
+            layer_scale_init_value * torch.ones((embed_dims)),
+            requires_grad=True) if layer_scale_init_value > 0 else None
         self.layer_scale_2 = nn.Parameter(
-            layer_scale_init_value * torch.ones((dim)), requires_grad=True)
-        self.norm_cfg = norm_cfg
-        self.dim = dim
-        self.deploy = deploy
+            layer_scale_init_value * torch.ones((embed_dims)),
+            requires_grad=True) if layer_scale_init_value > 0 else None
 
     def forward(self, x):
-        if hasattr(self, 'norm_reparam'):
-            x = x + self.drop_path(
-                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) *
-                self.norm_reparam(x))
-            x = x + self.drop_path(
-                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) *
-                self.mlp(self.norm2(x)))
-        else:
-            x = x + self.drop_path(
-                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) *
-                self.token_mixer(self.norm1(x)))
-            x = x + self.drop_path(
-                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) *
-                self.mlp(self.norm2(x)))
+        identity = x
+        x = self.norm1(x)
+        x = self.attn(x)
+        if self.layer_scale_1 is not None:
+            x = self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * x
+        x = identity + self.drop_path(x)
+
+        identity = x
+        x = self.norm2(x)
+        x = self.mlp(x)
+        if self.layer_scale_2 is not None:
+            x = self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * x
+        x = identity + self.drop_path(x)
+
         return x
 
-    def fuse_affine(self, norm, token_mixer):
-        gamma_affn = token_mixer.affine.weight.reshape(-1)
-        gamma_affn = gamma_affn - torch.ones_like(gamma_affn)
-        beta_affn = token_mixer.affine.bias
-        gamma_ln = norm.weight
-        beta_ln = norm.bias
-        return (gamma_ln * gamma_affn), (beta_ln * gamma_affn + beta_affn)
-
-    def get_equivalent_scale_bias(self):
-        eq_s, eq_b = self.fuse_affine(self.norm1, self.token_mixer)
-        return eq_s, eq_b
-
-    def switch_to_deploy(self):
-        if self.deploy:
-            return
-        eq_s, eq_b = self.get_equivalent_scale_bias()
-        self.norm_reparam = build_norm_layer(self.norm_cfg, self.dim)[1]
-        self.norm_reparam.weight.data = eq_s
-        self.norm_reparam.bias.data = eq_b
-        self.__delattr__('norm1')
-        if hasattr(self, 'token_mixer'):
-            self.__delattr__('token_mixer')
-        self.deploy = True
-
-
-def basic_blocks(dim,
-                 index,
-                 layers,
-                 mlp_ratio=4.,
-                 norm_cfg=dict(type='GN', num_groups=1),
-                 act_cfg=dict(type='GELU'),
-                 drop_rate=.0,
-                 drop_path_rate=0.,
-                 layer_scale_init_value=1e-5,
-                 deploy=False):
-    """generate RIFormer blocks for a stage."""
-    blocks = []
-    for block_idx in range(layers[index]):
-        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (
-            sum(layers) - 1)
-        blocks.append(
-            RIFormerBlock(
-                dim,
-                mlp_ratio=mlp_ratio,
-                norm_cfg=norm_cfg,
-                act_cfg=act_cfg,
-                drop=drop_rate,
-                drop_path=block_dpr,
-                layer_scale_init_value=layer_scale_init_value,
-                deploy=deploy,
-            ))
-    blocks = nn.Sequential(*blocks)
 
-    return blocks
+class VANPatchEmbed(PatchEmbed):
+    """Image to Patch Embedding of VAN.
+
+    The differences between VANPatchEmbed & PatchEmbed:
+        1. Use BN.
+        2. Do not use 'flatten' and 'transpose'.
+    """
+
+    def __init__(self, *args, norm_cfg=dict(type='BN'), **kwargs):
+        super(VANPatchEmbed, self).__init__(*args, norm_cfg=norm_cfg, **kwargs)
+
+    def forward(self, x):
+        """
+        Args:
+            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.
+        Returns:
+            tuple: Contains merged results and its spatial shape.
+            - x (Tensor): Has shape (B, out_h * out_w, embed_dims)
+            - out_size (tuple[int]): Spatial shape of x, arrange as
+              (out_h, out_w).
+        """
+
+        if self.adaptive_padding:
+            x = self.adaptive_padding(x)
+
+        x = self.projection(x)
+        out_size = (x.shape[2], x.shape[3])
+        if self.norm is not None:
+            x = self.norm(x)
+        return x, out_size
 
 
 @MODELS.register_module()
-class RIFormer(BaseBackbone):
-    """RIFormer.
+class VAN(BaseBackbone):
+    """Visual Attention Network.
+
+    A PyTorch implement of : `Visual Attention Network
+    <https://arxiv.org/pdf/2202.09741v2.pdf>`_
 
-    A PyTorch implementation of RIFormer introduced by:
-    `RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer <https://arxiv.org/abs/xxxx.xxxxx>`_
+    Inspiration from
+    https://github.com/Visual-Attention-Network/VAN-Classification
 
     Args:
-        arch (str | dict): The model's architecture. If string, it should be
-            one of architecture in ``RIFormer.arch_settings``. And if dict, it
-            should include the following two keys:
-
-            - layers (list[int]): Number of blocks at each stage.
-            - embed_dims (list[int]): The number of channels at each stage.
-            - mlp_ratios (list[int]): Expansion ratio of MLPs.
-            - layer_scale_init_value (float): Init value for Layer Scale.
-
-            Defaults to 'S12'.
-
-        norm_cfg (dict): The config dict for norm layers.
-            Defaults to ``dict(type='LN2d', eps=1e-6)``.
-        act_cfg (dict): The config dict for activation between pointwise
-            convolution. Defaults to ``dict(type='GELU')``.
-        in_patch_size (int): The patch size of/? input image patch embedding.
-            Defaults to 7.
-        in_stride (int): The stride of input image patch embedding.
-            Defaults to 4.
-        in_pad (int): The padding of input image patch embedding.
-            Defaults to 2.
-        down_patch_size (int): The patch size of downsampling patch embedding.
-            Defaults to 3.
-        down_stride (int): The stride of downsampling patch embedding.
-            Defaults to 2.
-        down_pad (int): The padding of downsampling patch embedding.
-            Defaults to 1.
-        drop_rate (float): Dropout rate. Defaults to 0.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
-        out_indices (Sequence | int): Output from which network position.
-            Index 0-6 respectively corresponds to
-            [stage1, downsampling, stage2, downsampling, stage3, downsampling, stage4]
-            Defaults to -1, means the last stage.
-        frozen_stages (int): Stages to be frozen (all param fixed).
-            Defaults to -1, which means not freezing any parameters.
-        deploy (bool): Whether to switch the model structure to
-            deployment mode. Default: False.
-        init_cfg (dict, optional): Initialization config dict
-    """  # noqa: E501
-
-    # --layers: [x,x,x,x], numbers of layers for the four stages
-    # --embed_dims, --mlp_ratios:
-    #     embedding dims and mlp ratios for the four stages
-    # --downsamples: flags to apply downsampling or not in four blocks
-    arch_settings = {
-        's12': {
-            'layers': [2, 2, 6, 2],
-            'embed_dims': [64, 128, 320, 512],
-            'mlp_ratios': [4, 4, 4, 4],
-            'layer_scale_init_value': 1e-5,
-        },
-        's24': {
-            'layers': [4, 4, 12, 4],
-            'embed_dims': [64, 128, 320, 512],
-            'mlp_ratios': [4, 4, 4, 4],
-            'layer_scale_init_value': 1e-5,
-        },
-        's36': {
-            'layers': [6, 6, 18, 6],
-            'embed_dims': [64, 128, 320, 512],
-            'mlp_ratios': [4, 4, 4, 4],
-            'layer_scale_init_value': 1e-6,
-        },
-        'm36': {
-            'layers': [6, 6, 18, 6],
-            'embed_dims': [96, 192, 384, 768],
-            'mlp_ratios': [4, 4, 4, 4],
-            'layer_scale_init_value': 1e-6,
-        },
-        'm48': {
-            'layers': [8, 8, 24, 8],
-            'embed_dims': [96, 192, 384, 768],
-            'mlp_ratios': [4, 4, 4, 4],
-            'layer_scale_init_value': 1e-6,
-        },
-    }
+        arch (str | dict): Visual Attention Network architecture.
+            If use string, choose from 'tiny', 'small', 'base' and 'large'.
+            If use dict, it should have below keys:
+
+            - **embed_dims** (List[int]): The dimensions of embedding.
+            - **depths** (List[int]): The number of blocks in each stage.
+            - **ffn_ratios** (List[int]): The number of expansion ratio of
+              feedforward network hidden layer channels.
+
+            Defaults to 'tiny'.
+        patch_sizes (List[int | tuple]): The patch size in patch embeddings.
+            Defaults to [7, 3, 3, 3].
+        in_channels (int): The num of input channels. Defaults to 3.
+        drop_rate (float): Dropout rate after embedding. Defaults to 0.
+        drop_path_rate (float): Stochastic depth rate. Defaults to 0.1.
+        out_indices (Sequence[int]): Output from which stages.
+            Default: ``(3, )``.
+        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
+            -1 means not freezing any parameters. Defaults to -1.
+        norm_eval (bool): Whether to set norm layers to eval mode, namely,
+            freeze running stats (mean and var). Note: Effect on Batch Norm
+            and its variants only. Defaults to False.
+        norm_cfg (dict): Config dict for normalization layer for all output
+            features. Defaults to ``dict(type='LN')``
+        block_cfgs (Sequence[dict] | dict): The extra config of each block.
+            Defaults to empty dicts.
+        init_cfg (dict, optional): The Config for initialization.
+            Defaults to None.
+
+    Examples:
+        >>> from mmpretrain.models import VAN
+        >>> import torch
+        >>> cfg = dict(arch='tiny')
+        >>> model = VAN(**cfg)
+        >>> inputs = torch.rand(1, 3, 224, 224)
+        >>> outputs = model(inputs)
+        >>> for out in outputs:
+        >>>     print(out.size())
+        (1, 256, 7, 7)
+    """
+    arch_zoo = {
+        **dict.fromkeys(['t', 'tiny'],
+                        {'embed_dims': [32, 64, 160, 256],
+                         'depths': [3, 3, 5, 2],
+                         'ffn_ratios': [8, 8, 4, 4]}),
+        **dict.fromkeys(['s', 'small'],
+                        {'embed_dims': [64, 128, 320, 512],
+                         'depths': [2, 2, 4, 2],
+                         'ffn_ratios': [8, 8, 4, 4]}),
+        **dict.fromkeys(['b', 'base'],
+                        {'embed_dims': [64, 128, 320, 512],
+                         'depths': [3, 3, 12, 3],
+                         'ffn_ratios': [8, 8, 4, 4]}),
+        **dict.fromkeys(['l', 'large'],
+                        {'embed_dims': [64, 128, 320, 512],
+                         'depths': [3, 5, 27, 3],
+                         'ffn_ratios': [8, 8, 4, 4]}),
+    }  # yapf: disable
 
     def __init__(self,
-                 arch='s12',
+                 arch='tiny',
+                 patch_sizes=[7, 3, 3, 3],
                  in_channels=3,
-                 norm_cfg=dict(type='GN', num_groups=1),
-                 act_cfg=dict(type='GELU'),
-                 in_patch_size=7,
-                 in_stride=4,
-                 in_pad=2,
-                 down_patch_size=3,
-                 down_stride=2,
-                 down_pad=1,
                  drop_rate=0.,
                  drop_path_rate=0.,
-                 out_indices=-1,
+                 out_indices=(3, ),
                  frozen_stages=-1,
-                 init_cfg=None,
-                 deploy=False):
-
-        super().__init__(init_cfg=init_cfg)
+                 norm_eval=False,
+                 norm_cfg=dict(type='LN'),
+                 block_cfgs=dict(),
+                 init_cfg=None):
+        super(VAN, self).__init__(init_cfg=init_cfg)
 
         if isinstance(arch, str):
-            assert arch in self.arch_settings, \
-                f'Unavailable arch, please choose from ' \
-                f'({set(self.arch_settings)}) or pass a dict.'
-            arch = self.arch_settings[arch]
-        elif isinstance(arch, dict):
-            assert 'layers' in arch and 'embed_dims' in arch, \
-                f'The arch dict must have "layers" and "embed_dims", ' \
-                f'but got {list(arch.keys())}.'
-
-        layers = arch['layers']
-        embed_dims = arch['embed_dims']
-        mlp_ratios = arch['mlp_ratios'] \
-            if 'mlp_ratios' in arch else [4, 4, 4, 4]
-        layer_scale_init_value = arch['layer_scale_init_value'] \
-            if 'layer_scale_init_value' in arch else 1e-5
-
-        self.patch_embed = PatchEmbed(
-            patch_size=in_patch_size,
-            stride=in_stride,
-            padding=in_pad,
-            in_chans=in_channels,
-            embed_dim=embed_dims[0])
-
-        # set the main block in network
-        network = []
-        for i in range(len(layers)):
-            stage = basic_blocks(
-                embed_dims[i],
-                i,
-                layers,
-                mlp_ratio=mlp_ratios[i],
-                norm_cfg=norm_cfg,
-                act_cfg=act_cfg,
-                drop_rate=drop_rate,
-                drop_path_rate=drop_path_rate,
-                layer_scale_init_value=layer_scale_init_value,
-                deploy=deploy)
-            network.append(stage)
-            if i >= len(layers) - 1:
-                break
-            if embed_dims[i] != embed_dims[i + 1]:
-                # downsampling between two stages
-                network.append(
-                    PatchEmbed(
-                        patch_size=down_patch_size,
-                        stride=down_stride,
-                        padding=down_pad,
-                        in_chans=embed_dims[i],
-                        embed_dim=embed_dims[i + 1]))
-
-        self.network = nn.ModuleList(network)
-
-        if isinstance(out_indices, int):
-            out_indices = [out_indices]
-        assert isinstance(out_indices, Sequence), \
-            f'"out_indices" must by a sequence or int, ' \
-            f'get {type(out_indices)} instead.'
-        for i, index in enumerate(out_indices):
-            if index < 0:
-                out_indices[i] = 7 + index
-                assert out_indices[i] >= 0, f'Invalid out_indices {index}'
+            arch = arch.lower()
+            assert arch in set(self.arch_zoo), \
+                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'
+            self.arch_settings = self.arch_zoo[arch]
+        else:
+            essential_keys = {'embed_dims', 'depths', 'ffn_ratios'}
+            assert isinstance(arch, dict) and set(arch) == essential_keys, \
+                f'Custom arch needs a dict with keys {essential_keys}'
+            self.arch_settings = arch
+
+        self.embed_dims = self.arch_settings['embed_dims']
+        self.depths = self.arch_settings['depths']
+        self.ffn_ratios = self.arch_settings['ffn_ratios']
+        self.num_stages = len(self.depths)
         self.out_indices = out_indices
-        if self.out_indices:
-            for i_layer in self.out_indices:
-                layer = build_norm_layer(norm_cfg,
-                                         embed_dims[(i_layer + 1) // 2])[1]
-                layer_name = f'norm{i_layer}'
-                self.add_module(layer_name, layer)
-
         self.frozen_stages = frozen_stages
-        self._freeze_stages()
-        self.deploy = deploy
-
-    def forward_embeddings(self, x):
-        x = self.patch_embed(x)
-        return x
+        self.norm_eval = norm_eval
 
-    def forward_tokens(self, x):
-        outs = []
-        for idx, block in enumerate(self.network):
-            x = block(x)
-            if idx in self.out_indices:
-                norm_layer = getattr(self, f'norm{idx}')
-                x_out = norm_layer(x)
-                outs.append(x_out)
-        return tuple(outs)
+        total_depth = sum(self.depths)
+        dpr = [
+            x.item() for x in torch.linspace(0, drop_path_rate, total_depth)
+        ]  # stochastic depth decay rule
+
+        cur_block_idx = 0
+        for i, depth in enumerate(self.depths):
+            patch_embed = VANPatchEmbed(
+                in_channels=in_channels if i == 0 else self.embed_dims[i - 1],
+                input_size=None,
+                embed_dims=self.embed_dims[i],
+                kernel_size=patch_sizes[i],
+                stride=patch_sizes[i] // 2 + 1,
+                padding=(patch_sizes[i] // 2, patch_sizes[i] // 2),
+                norm_cfg=dict(type='BN'))
+
+            blocks = ModuleList([
+                VANBlock(
+                    embed_dims=self.embed_dims[i],
+                    ffn_ratio=self.ffn_ratios[i],
+                    drop_rate=drop_rate,
+                    drop_path_rate=dpr[cur_block_idx + j],
+                    **block_cfgs) for j in range(depth)
+            ])
+            cur_block_idx += depth
+            norm = build_norm_layer(norm_cfg, self.embed_dims[i])[1]
+
+            self.add_module(f'patch_embed{i + 1}', patch_embed)
+            self.add_module(f'blocks{i + 1}', blocks)
+            self.add_module(f'norm{i + 1}', norm)
 
-    def forward(self, x):
-        # input embedding
-        x = self.forward_embeddings(x)
-        # through backbone
-        x = self.forward_tokens(x)
-        return x
+    def train(self, mode=True):
+        super(VAN, self).train(mode)
+        self._freeze_stages()
+        if mode and self.norm_eval:
+            for m in self.modules():
+                # trick: eval have effect on BatchNorm only
+                if isinstance(m, _BatchNorm):
+                    m.eval()
 
     def _freeze_stages(self):
-        if self.frozen_stages >= 0:
-            self.patch_embed.eval()
-            for param in self.patch_embed.parameters():
+        for i in range(0, self.frozen_stages + 1):
+            # freeze patch embed
+            m = getattr(self, f'patch_embed{i + 1}')
+            m.eval()
+            for param in m.parameters():
                 param.requires_grad = False
 
-        for i in range(0, self.frozen_stages + 1):
-            # Include both block and downsample layer.
-            module = self.network[i]
-            module.eval()
-            for param in module.parameters():
+            # freeze blocks
+            m = getattr(self, f'blocks{i + 1}')
+            m.eval()
+            for param in m.parameters():
                 param.requires_grad = False
-            if i in self.out_indices:
-                norm_layer = getattr(self, f'norm{i}')
-                norm_layer.eval()
-                for param in norm_layer.parameters():
-                    param.requires_grad = False
 
-    def train(self, mode=True):
-        super(RIFormer, self).train(mode)
-        self._freeze_stages()
-        return self
+            # freeze norm
+            m = getattr(self, f'norm{i + 1}')
+            m.eval()
+            for param in m.parameters():
+                param.requires_grad = False
+
+    def forward(self, x):
+        outs = []
+        for i in range(self.num_stages):
+            patch_embed = getattr(self, f'patch_embed{i + 1}')
+            blocks = getattr(self, f'blocks{i + 1}')
+            norm = getattr(self, f'norm{i + 1}')
+            x, hw_shape = patch_embed(x)
+            for block in blocks:
+                x = block(x)
+            x = x.flatten(2).transpose(1, 2)
+            x = norm(x)
+            x = x.reshape(-1, *hw_shape,
+                          block.out_channels).permute(0, 3, 1, 2).contiguous()
+            if i in self.out_indices:
+                outs.append(x)
 
-    def switch_to_deploy(self):
-        for m in self.modules():
-            if isinstance(m, RIFormerBlock):
-                m.switch_to_deploy()
-        self.deploy = True
+        return tuple(outs)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/seresnet.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/seresnet.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/seresnext.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/seresnext.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/shufflenet_v1.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/shufflenet_v1.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/shufflenet_v2.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/shufflenet_v2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/swin_transformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/swin_transformer_v2.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/swin_transformer_v2.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/t2t_vit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/t2t_vit.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,15 +34,15 @@
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         qkv_bias (bool): enable bias for qkv if True. Defaults to True.
         qk_scale (float, optional): Override default qk scale of
             ``(input_dims // num_heads) ** -0.5`` if set. Defaults to None.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         init_cfg (dict, optional): Initialization config dict.
             Defaults to None.
 
     Notes:
         In general, ``qk_scale`` should be ``head_dims ** -0.5``, i.e.
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/timm_backbone.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/timm_backbone.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/tinyvit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/tinyvit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/tnt.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/tnt.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/twins.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/twins.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/van.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/itpn_neck.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,434 +1,388 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+import math
+from typing import List, Optional, Union
+
 import torch
 import torch.nn as nn
-from mmcv.cnn import Conv2d, build_activation_layer, build_norm_layer
-from mmcv.cnn.bricks import DropPath
-from mmcv.cnn.bricks.transformer import PatchEmbed
-from mmengine.model import BaseModule, ModuleList
-from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm
+import torch.nn.functional as F
+from mmcv.cnn import build_norm_layer
+from mmengine.model import BaseModule
 
+from mmpretrain.models.backbones.hivit import BlockWithRPE
 from mmpretrain.registry import MODELS
-from .base_backbone import BaseBackbone
-
+from ..backbones.vision_transformer import TransformerEncoderLayer
+from ..utils import build_2d_sincos_position_embedding
 
-class MixFFN(BaseModule):
-    """An implementation of MixFFN of VAN. Refer to
-    mmdetection/mmdet/models/backbones/pvt.py.
 
-    The differences between MixFFN & FFN:
-        1. Use 1X1 Conv to replace Linear layer.
-        2. Introduce 3X3 Depth-wise Conv to encode positional information.
+class PatchSplit(nn.Module):
+    """The up-sample module used in neck (transformer pyramid network)
 
     Args:
-        embed_dims (int): The feature dimension. Same as
-            `MultiheadAttention`.
-        feedforward_channels (int): The hidden dimension of FFNs.
-        act_cfg (dict, optional): The activation config for FFNs.
-            Default: dict(type='GELU').
-        ffn_drop (float, optional): Probability of an element to be
-            zeroed in FFN. Default 0.0.
-        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
-            Default: None.
+        dim (int): the input dimension (channel number).
+        fpn_dim (int): the fpn dimension (channel number).
+        norm_cfg (dict): Config dict for normalization layer.
+                Defaults to ``dict(type='LN')``.
     """
 
-    def __init__(self,
-                 embed_dims,
-                 feedforward_channels,
-                 act_cfg=dict(type='GELU'),
-                 ffn_drop=0.,
-                 init_cfg=None):
-        super(MixFFN, self).__init__(init_cfg=init_cfg)
-
-        self.embed_dims = embed_dims
-        self.feedforward_channels = feedforward_channels
-        self.act_cfg = act_cfg
-
-        self.fc1 = Conv2d(
-            in_channels=embed_dims,
-            out_channels=feedforward_channels,
-            kernel_size=1)
-        self.dwconv = Conv2d(
-            in_channels=feedforward_channels,
-            out_channels=feedforward_channels,
-            kernel_size=3,
-            stride=1,
-            padding=1,
-            bias=True,
-            groups=feedforward_channels)
-        self.act = build_activation_layer(act_cfg)
-        self.fc2 = Conv2d(
-            in_channels=feedforward_channels,
-            out_channels=embed_dims,
-            kernel_size=1)
-        self.drop = nn.Dropout(ffn_drop)
+    def __init__(self, dim, fpn_dim, norm_cfg):
+        super().__init__()
+        _, self.norm = build_norm_layer(norm_cfg, dim)
+        self.reduction = nn.Linear(dim, fpn_dim * 4, bias=False)
+        self.fpn_dim = fpn_dim
 
     def forward(self, x):
-        x = self.fc1(x)
-        x = self.dwconv(x)
-        x = self.act(x)
-        x = self.drop(x)
-        x = self.fc2(x)
-        x = self.drop(x)
+        B, N, H, W, C = x.shape
+        x = self.norm(x)
+        x = self.reduction(x)
+        x = x.reshape(B, N, H, W, 2, 2,
+                      self.fpn_dim).permute(0, 1, 2, 4, 3, 5,
+                                            6).reshape(B, N, 2 * H, 2 * W,
+                                                       self.fpn_dim)
         return x
 
 
-class LKA(BaseModule):
-    """Large Kernel Attention(LKA) of VAN.
-
-    .. code:: text
-            DW_conv (depth-wise convolution)
-                            |
-                            |
-        DW_D_conv (depth-wise dilation convolution)
-                            |
-                            |
-        Transition Convolution (11 convolution)
-
-    Args:
-        embed_dims (int): Number of input channels.
-        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
-            Default: None.
-    """
-
-    def __init__(self, embed_dims, init_cfg=None):
-        super(LKA, self).__init__(init_cfg=init_cfg)
-
-        # a spatial local convolution (depth-wise convolution)
-        self.DW_conv = Conv2d(
-            in_channels=embed_dims,
-            out_channels=embed_dims,
-            kernel_size=5,
-            padding=2,
-            groups=embed_dims)
-
-        # a spatial long-range convolution (depth-wise dilation convolution)
-        self.DW_D_conv = Conv2d(
-            in_channels=embed_dims,
-            out_channels=embed_dims,
-            kernel_size=7,
-            stride=1,
-            padding=9,
-            groups=embed_dims,
-            dilation=3)
-
-        self.conv1 = Conv2d(
-            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
-
-    def forward(self, x):
-        u = x.clone()
-        attn = self.DW_conv(x)
-        attn = self.DW_D_conv(attn)
-        attn = self.conv1(attn)
-
-        return u * attn
-
-
-class SpatialAttention(BaseModule):
-    """Basic attention module in VANBloack.
-
-    Args:
-        embed_dims (int): Number of input channels.
-        act_cfg (dict, optional): The activation config for FFNs.
-            Default: dict(type='GELU').
-        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
-            Default: None.
-    """
-
-    def __init__(self, embed_dims, act_cfg=dict(type='GELU'), init_cfg=None):
-        super(SpatialAttention, self).__init__(init_cfg=init_cfg)
-
-        self.proj_1 = Conv2d(
-            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
-        self.activation = build_activation_layer(act_cfg)
-        self.spatial_gating_unit = LKA(embed_dims)
-        self.proj_2 = Conv2d(
-            in_channels=embed_dims, out_channels=embed_dims, kernel_size=1)
-
-    def forward(self, x):
-        shorcut = x.clone()
-        x = self.proj_1(x)
-        x = self.activation(x)
-        x = self.spatial_gating_unit(x)
-        x = self.proj_2(x)
-        x = x + shorcut
-        return x
-
-
-class VANBlock(BaseModule):
-    """A block of VAN.
+@MODELS.register_module()
+class iTPNPretrainDecoder(BaseModule):
+    """The neck module of iTPN (transformer pyramid network).
 
     Args:
-        embed_dims (int): Number of input channels.
-        ffn_ratio (float): The expansion ratio of feedforward network hidden
-            layer channels. Defaults to 4.
-        drop_rate (float): Dropout rate after embedding. Defaults to 0.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.1.
-        act_cfg (dict, optional): The activation config for FFNs.
-            Default: dict(type='GELU').
-        layer_scale_init_value (float): Init value for Layer Scale.
-            Defaults to 1e-2.
-        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
-            Default: None.
+        num_patches (int): The number of total patches. Defaults to 196.
+        patch_size (int): Image patch size. Defaults to 16.
+        in_chans (int): The channel of input image. Defaults to 3.
+        embed_dim (int): Encoder's embedding dimension. Defaults to 512.
+        fpn_dim (int): The fpn dimension (channel number).
+        fpn_depth (int): The layer number of feature pyramid.
+        decoder_embed_dim (int): Decoder's embedding dimension.
+            Defaults to 512.
+        decoder_depth (int): The depth of decoder. Defaults to 8.
+        decoder_num_heads (int): Number of attention heads of decoder.
+            Defaults to 16.
+        mlp_ratio (int): Ratio of mlp hidden dim to decoder's embedding dim.
+            Defaults to 4.
+        norm_cfg (dict): Normalization layer. Defaults to LayerNorm.
+        reconstruction_type (str): The itpn supports 2 kinds of supervisions.
+            Defaults to 'pixel'.
+        num_outs (int): The output number of neck (transformer pyramid
+            network). Defaults to 3.
+        predict_feature_dim (int): The output dimension to supervision.
+            Defaults to None.
+        init_cfg (Union[List[dict], dict], optional): Initialization config
+            dict. Defaults to None.
     """
 
     def __init__(self,
-                 embed_dims,
-                 ffn_ratio=4.,
-                 drop_rate=0.,
-                 drop_path_rate=0.,
-                 act_cfg=dict(type='GELU'),
-                 norm_cfg=dict(type='BN', eps=1e-5),
-                 layer_scale_init_value=1e-2,
-                 init_cfg=None):
-        super(VANBlock, self).__init__(init_cfg=init_cfg)
-        self.out_channels = embed_dims
-
-        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]
-        self.attn = SpatialAttention(embed_dims, act_cfg=act_cfg)
-        self.drop_path = DropPath(
-            drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-
-        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]
-        mlp_hidden_dim = int(embed_dims * ffn_ratio)
-        self.mlp = MixFFN(
-            embed_dims=embed_dims,
-            feedforward_channels=mlp_hidden_dim,
-            act_cfg=act_cfg,
-            ffn_drop=drop_rate)
-        self.layer_scale_1 = nn.Parameter(
-            layer_scale_init_value * torch.ones((embed_dims)),
-            requires_grad=True) if layer_scale_init_value > 0 else None
-        self.layer_scale_2 = nn.Parameter(
-            layer_scale_init_value * torch.ones((embed_dims)),
-            requires_grad=True) if layer_scale_init_value > 0 else None
+                 num_patches: int = 196,
+                 patch_size: int = 16,
+                 in_chans: int = 3,
+                 embed_dim: int = 512,
+                 fpn_dim: int = 256,
+                 fpn_depth: int = 2,
+                 decoder_embed_dim: int = 512,
+                 decoder_depth: int = 6,
+                 decoder_num_heads: int = 16,
+                 mlp_ratio: int = 4,
+                 norm_cfg: dict = dict(type='LN', eps=1e-6),
+                 reconstruction_type: str = 'pixel',
+                 num_outs: int = 3,
+                 qkv_bias: bool = True,
+                 qk_scale: Optional[bool] = None,
+                 drop_rate: float = 0.0,
+                 attn_drop_rate: float = 0.0,
+                 predict_feature_dim: Optional[float] = None,
+                 init_cfg: Optional[Union[List[dict], dict]] = None) -> None:
+        super().__init__(init_cfg=init_cfg)
+        self.num_patches = num_patches
+        assert reconstruction_type in ['pixel', 'clip'], \
+            'iTPN method only support `pixel` and `clip`, ' \
+            f'but got `{reconstruction_type}`.'
+        self.reconstruction_type = reconstruction_type
+        self.num_outs = num_outs
+
+        self.build_transformer_pyramid(
+            num_outs=num_outs,
+            embed_dim=embed_dim,
+            fpn_dim=fpn_dim,
+            fpn_depth=fpn_depth,
+            mlp_ratio=mlp_ratio,
+            qkv_bias=qkv_bias,
+            qk_scale=qk_scale,
+            drop_rate=drop_rate,
+            attn_drop_rate=attn_drop_rate,
+            rpe=False,
+            norm_cfg=norm_cfg,
+        )
+
+        # merge the output
+        self.decoder_embed = nn.ModuleList()
+        self.decoder_embed.append(
+            nn.Sequential(
+                nn.LayerNorm(fpn_dim),
+                nn.Linear(fpn_dim, decoder_embed_dim, bias=True),
+            ))
+
+        if self.num_outs >= 2:
+            self.decoder_embed.append(
+                nn.Sequential(
+                    nn.LayerNorm(fpn_dim),
+                    nn.Linear(fpn_dim, decoder_embed_dim // 4, bias=True),
+                ))
+        if self.num_outs >= 3:
+            self.decoder_embed.append(
+                nn.Sequential(
+                    nn.LayerNorm(fpn_dim),
+                    nn.Linear(fpn_dim, decoder_embed_dim // 16, bias=True),
+                ))
+
+        if reconstruction_type == 'pixel':
+            self.mask_token = nn.Parameter(
+                torch.zeros(1, 1, decoder_embed_dim))
+
+            # create new position embedding, different from that in encoder
+            # and is not learnable
+            self.decoder_pos_embed = nn.Parameter(
+                torch.zeros(1, self.num_patches, decoder_embed_dim),
+                requires_grad=False)
+
+            self.decoder_blocks = nn.ModuleList([
+                TransformerEncoderLayer(
+                    decoder_embed_dim,
+                    decoder_num_heads,
+                    int(mlp_ratio * decoder_embed_dim),
+                    qkv_bias=True,
+                    norm_cfg=norm_cfg) for _ in range(decoder_depth)
+            ])
 
-    def forward(self, x):
-        identity = x
-        x = self.norm1(x)
-        x = self.attn(x)
-        if self.layer_scale_1 is not None:
-            x = self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * x
-        x = identity + self.drop_path(x)
-
-        identity = x
-        x = self.norm2(x)
-        x = self.mlp(x)
-        if self.layer_scale_2 is not None:
-            x = self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * x
-        x = identity + self.drop_path(x)
+            self.decoder_norm_name, decoder_norm = build_norm_layer(
+                norm_cfg, decoder_embed_dim, postfix=1)
+            self.add_module(self.decoder_norm_name, decoder_norm)
+
+            # Used to map features to pixels
+            if predict_feature_dim is None:
+                predict_feature_dim = patch_size**2 * in_chans
+            self.decoder_pred = nn.Linear(
+                decoder_embed_dim, predict_feature_dim, bias=True)
+        else:
+            _, norm = build_norm_layer(norm_cfg, embed_dim)
+            self.add_module('norm', norm)
 
-        return x
+    def build_transformer_pyramid(self,
+                                  num_outs=3,
+                                  embed_dim=512,
+                                  fpn_dim=256,
+                                  fpn_depth=2,
+                                  mlp_ratio=4.0,
+                                  qkv_bias=True,
+                                  qk_scale=None,
+                                  drop_rate=0.0,
+                                  attn_drop_rate=0.0,
+                                  rpe=False,
+                                  norm_cfg=None):
+        Hp = None
+        mlvl_dims = {'4': embed_dim // 4, '8': embed_dim // 2, '16': embed_dim}
+        if num_outs > 1:
+            if embed_dim != fpn_dim:
+                self.align_dim_16tofpn = nn.Linear(embed_dim, fpn_dim)
+            else:
+                self.align_dim_16tofpn = None
+            self.fpn_modules = nn.ModuleList()
+            self.fpn_modules.append(
+                BlockWithRPE(
+                    Hp,
+                    fpn_dim,
+                    0,
+                    mlp_ratio,
+                    qkv_bias,
+                    qk_scale,
+                    drop=drop_rate,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    rpe=rpe,
+                    norm_cfg=norm_cfg))
+            self.fpn_modules.append(
+                BlockWithRPE(
+                    Hp,
+                    fpn_dim,
+                    0,
+                    mlp_ratio,
+                    qkv_bias,
+                    qk_scale,
+                    drop=drop_rate,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    rpe=False,
+                    norm_cfg=norm_cfg,
+                ))
+
+            self.align_dim_16to8 = nn.Linear(
+                mlvl_dims['8'], fpn_dim, bias=False)
+            self.split_16to8 = PatchSplit(mlvl_dims['16'], fpn_dim, norm_cfg)
+            self.block_16to8 = nn.Sequential(*[
+                BlockWithRPE(
+                    Hp,
+                    fpn_dim,
+                    0,
+                    mlp_ratio,
+                    qkv_bias,
+                    qk_scale,
+                    drop=drop_rate,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    rpe=rpe,
+                    norm_cfg=norm_cfg,
+                ) for _ in range(fpn_depth)
+            ])
 
+        if num_outs > 2:
+            self.align_dim_8to4 = nn.Linear(
+                mlvl_dims['4'], fpn_dim, bias=False)
+            self.split_8to4 = PatchSplit(fpn_dim, fpn_dim, norm_cfg)
+            self.block_8to4 = nn.Sequential(*[
+                BlockWithRPE(
+                    Hp,
+                    fpn_dim,
+                    0,
+                    mlp_ratio,
+                    qkv_bias,
+                    qk_scale,
+                    drop=drop_rate,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    rpe=rpe,
+                    norm_cfg=norm_cfg,
+                ) for _ in range(fpn_depth)
+            ])
+            self.fpn_modules.append(
+                BlockWithRPE(
+                    Hp,
+                    fpn_dim,
+                    0,
+                    mlp_ratio,
+                    qkv_bias,
+                    qk_scale,
+                    drop=drop_rate,
+                    attn_drop=attn_drop_rate,
+                    drop_path=0.,
+                    rpe=rpe,
+                    norm_cfg=norm_cfg))
+
+    def init_weights(self) -> None:
+        """Initialize position embedding and mask token of MAE decoder."""
+        super().init_weights()
+
+        if self.reconstruction_type == 'pixel':
+            decoder_pos_embed = build_2d_sincos_position_embedding(
+                int(self.num_patches**.5),
+                self.decoder_pos_embed.shape[-1],
+                cls_token=False)
+            self.decoder_pos_embed.data.copy_(decoder_pos_embed.float())
 
-class VANPatchEmbed(PatchEmbed):
-    """Image to Patch Embedding of VAN.
+            torch.nn.init.normal_(self.mask_token, std=.02)
+        else:
+            self.rescale_init_weight()
 
-    The differences between VANPatchEmbed & PatchEmbed:
-        1. Use BN.
-        2. Do not use 'flatten' and 'transpose'.
-    """
+    def rescale_init_weight(self) -> None:
+        """Rescale the initialized weights."""
 
-    def __init__(self, *args, norm_cfg=dict(type='BN'), **kwargs):
-        super(VANPatchEmbed, self).__init__(*args, norm_cfg=norm_cfg, **kwargs)
+        def rescale(param, layer_id):
+            param.div_(math.sqrt(2.0 * layer_id))
+
+        for layer_id, layer in enumerate(self.fpn_modules):
+            if isinstance(layer, BlockWithRPE):
+                if layer.attn is not None:
+                    rescale(layer.attn.proj.weight.data, layer_id + 1)
+                rescale(layer.mlp.fc2.weight.data, layer_id + 1)
+
+    @property
+    def decoder_norm(self):
+        """The normalization layer of decoder."""
+        return getattr(self, self.decoder_norm_name)
+
+    def forward(self,
+                x: torch.Tensor,
+                ids_restore: torch.Tensor = None) -> torch.Tensor:
+        """The forward function.
+
+        The process computes the visible patches' features vectors and the mask
+        tokens to output feature vectors, which will be used for
+        reconstruction.
 
-    def forward(self, x):
-        """
         Args:
-            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.
+            x (torch.Tensor): hidden features, which is of shape
+                    B x (L * mask_ratio) x C.
+            ids_restore (torch.Tensor): ids to restore original image.
+
         Returns:
-            tuple: Contains merged results and its spatial shape.
-            - x (Tensor): Has shape (B, out_h * out_w, embed_dims)
-            - out_size (tuple[int]): Spatial shape of x, arrange as
-              (out_h, out_w).
+            torch.Tensor: The reconstructed feature vectors, which is of
+            shape B x (num_patches) x C.
         """
 
-        if self.adaptive_padding:
-            x = self.adaptive_padding(x)
-
-        x = self.projection(x)
-        out_size = (x.shape[2], x.shape[3])
-        if self.norm is not None:
-            x = self.norm(x)
-        return x, out_size
-
-
-@MODELS.register_module()
-class VAN(BaseBackbone):
-    """Visual Attention Network.
-
-    A PyTorch implement of : `Visual Attention Network
-    <https://arxiv.org/pdf/2202.09741v2.pdf>`_
-
-    Inspiration from
-    https://github.com/Visual-Attention-Network/VAN-Classification
-
-    Args:
-        arch (str | dict): Visual Attention Network architecture.
-            If use string, choose from 'tiny', 'small', 'base' and 'large'.
-            If use dict, it should have below keys:
-
-            - **embed_dims** (List[int]): The dimensions of embedding.
-            - **depths** (List[int]): The number of blocks in each stage.
-            - **ffn_ratios** (List[int]): The number of expansion ratio of
-              feedforward network hidden layer channels.
-
-            Defaults to 'tiny'.
-        patch_sizes (List[int | tuple]): The patch size in patch embeddings.
-            Defaults to [7, 3, 3, 3].
-        in_channels (int): The num of input channels. Defaults to 3.
-        drop_rate (float): Dropout rate after embedding. Defaults to 0.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.1.
-        out_indices (Sequence[int]): Output from which stages.
-            Default: ``(3, )``.
-        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
-            -1 means not freezing any parameters. Defaults to -1.
-        norm_eval (bool): Whether to set norm layers to eval mode, namely,
-            freeze running stats (mean and var). Note: Effect on Batch Norm
-            and its variants only. Defaults to False.
-        norm_cfg (dict): Config dict for normalization layer for all output
-            features. Defaults to ``dict(type='LN')``
-        block_cfgs (Sequence[dict] | dict): The extra config of each block.
-            Defaults to empty dicts.
-        init_cfg (dict, optional): The Config for initialization.
-            Defaults to None.
-
-    Examples:
-        >>> from mmpretrain.models import VAN
-        >>> import torch
-        >>> cfg = dict(arch='tiny')
-        >>> model = VAN(**cfg)
-        >>> inputs = torch.rand(1, 3, 224, 224)
-        >>> outputs = model(inputs)
-        >>> for out in outputs:
-        >>>     print(out.size())
-        (1, 256, 7, 7)
-    """
-    arch_zoo = {
-        **dict.fromkeys(['t', 'tiny'],
-                        {'embed_dims': [32, 64, 160, 256],
-                         'depths': [3, 3, 5, 2],
-                         'ffn_ratios': [8, 8, 4, 4]}),
-        **dict.fromkeys(['s', 'small'],
-                        {'embed_dims': [64, 128, 320, 512],
-                         'depths': [2, 2, 4, 2],
-                         'ffn_ratios': [8, 8, 4, 4]}),
-        **dict.fromkeys(['b', 'base'],
-                        {'embed_dims': [64, 128, 320, 512],
-                         'depths': [3, 3, 12, 3],
-                         'ffn_ratios': [8, 8, 4, 4]}),
-        **dict.fromkeys(['l', 'large'],
-                        {'embed_dims': [64, 128, 320, 512],
-                         'depths': [3, 5, 27, 3],
-                         'ffn_ratios': [8, 8, 4, 4]}),
-    }  # yapf: disable
-
-    def __init__(self,
-                 arch='tiny',
-                 patch_sizes=[7, 3, 3, 3],
-                 in_channels=3,
-                 drop_rate=0.,
-                 drop_path_rate=0.,
-                 out_indices=(3, ),
-                 frozen_stages=-1,
-                 norm_eval=False,
-                 norm_cfg=dict(type='LN'),
-                 block_cfgs=dict(),
-                 init_cfg=None):
-        super(VAN, self).__init__(init_cfg=init_cfg)
-
-        if isinstance(arch, str):
-            arch = arch.lower()
-            assert arch in set(self.arch_zoo), \
-                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'
-            self.arch_settings = self.arch_zoo[arch]
+        features = x[:2]
+        x = x[-1]
+        B, L, _ = x.shape
+        x = x[..., None, None, :]
+        Hp = Wp = math.sqrt(L)
+
+        outs = [x] if self.align_dim_16tofpn is None else [
+            self.align_dim_16tofpn(x)
+        ]
+        if self.num_outs >= 2:
+            x = self.block_16to8(
+                self.split_16to8(x) + self.align_dim_16to8(features[1]))
+            outs.append(x)
+        if self.num_outs >= 3:
+            x = self.block_8to4(
+                self.split_8to4(x) + self.align_dim_8to4(features[0]))
+            outs.append(x)
+        if self.num_outs > 3:
+            outs = [
+                out.reshape(B, Hp, Wp, *out.shape[-3:]).permute(
+                    0, 5, 1, 3, 2, 4).reshape(B, -1, Hp * out.shape[-3],
+                                              Wp * out.shape[-2]).contiguous()
+                for out in outs
+            ]
+            if self.num_outs >= 4:
+                outs.insert(0, F.avg_pool2d(outs[0], kernel_size=2, stride=2))
+            if self.num_outs >= 5:
+                outs.insert(0, F.avg_pool2d(outs[0], kernel_size=2, stride=2))
+
+        for i, out in enumerate(outs):
+            out = self.fpn_modules[i](out)
+            outs[i] = out
+
+        if self.reconstruction_type == 'pixel':
+            feats = []
+            for feat, layer in zip(outs, self.decoder_embed):
+                x = layer(feat).reshape(B, L, -1)
+                # append mask tokens to sequence
+                mask_tokens = self.mask_token.repeat(
+                    x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)
+                x = torch.cat([x, mask_tokens], dim=1)
+                x = torch.gather(
+                    x,
+                    dim=1,
+                    index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))
+                feats.append(x)
+            x = feats.pop(0)
+            # add pos embed
+            x = x + self.decoder_pos_embed
+
+            for i, feat in enumerate(feats):
+                x = x + feats[i]
+            # apply Transformer blocks
+            for i, blk in enumerate(self.decoder_blocks):
+                x = blk(x)
+            x = self.decoder_norm(x)
+            x = self.decoder_pred(x)
+            return x
         else:
-            essential_keys = {'embed_dims', 'depths', 'ffn_ratios'}
-            assert isinstance(arch, dict) and set(arch) == essential_keys, \
-                f'Custom arch needs a dict with keys {essential_keys}'
-            self.arch_settings = arch
-
-        self.embed_dims = self.arch_settings['embed_dims']
-        self.depths = self.arch_settings['depths']
-        self.ffn_ratios = self.arch_settings['ffn_ratios']
-        self.num_stages = len(self.depths)
-        self.out_indices = out_indices
-        self.frozen_stages = frozen_stages
-        self.norm_eval = norm_eval
-
-        total_depth = sum(self.depths)
-        dpr = [
-            x.item() for x in torch.linspace(0, drop_path_rate, total_depth)
-        ]  # stochastic depth decay rule
-
-        cur_block_idx = 0
-        for i, depth in enumerate(self.depths):
-            patch_embed = VANPatchEmbed(
-                in_channels=in_channels if i == 0 else self.embed_dims[i - 1],
-                input_size=None,
-                embed_dims=self.embed_dims[i],
-                kernel_size=patch_sizes[i],
-                stride=patch_sizes[i] // 2 + 1,
-                padding=(patch_sizes[i] // 2, patch_sizes[i] // 2),
-                norm_cfg=dict(type='BN'))
-
-            blocks = ModuleList([
-                VANBlock(
-                    embed_dims=self.embed_dims[i],
-                    ffn_ratio=self.ffn_ratios[i],
-                    drop_rate=drop_rate,
-                    drop_path_rate=dpr[cur_block_idx + j],
-                    **block_cfgs) for j in range(depth)
-            ])
-            cur_block_idx += depth
-            norm = build_norm_layer(norm_cfg, self.embed_dims[i])[1]
+            feats = []
+            for feat, layer in zip(outs, self.decoder_embed):
+                x = layer(feat).reshape(B, L, -1)
+                feats.append(x)
+            x = feats.pop(0)
+            for i, feat in enumerate(feats):
+                x = x + feats[i]
 
-            self.add_module(f'patch_embed{i + 1}', patch_embed)
-            self.add_module(f'blocks{i + 1}', blocks)
-            self.add_module(f'norm{i + 1}', norm)
-
-    def train(self, mode=True):
-        super(VAN, self).train(mode)
-        self._freeze_stages()
-        if mode and self.norm_eval:
-            for m in self.modules():
-                # trick: eval have effect on BatchNorm only
-                if isinstance(m, _BatchNorm):
-                    m.eval()
-
-    def _freeze_stages(self):
-        for i in range(0, self.frozen_stages + 1):
-            # freeze patch embed
-            m = getattr(self, f'patch_embed{i + 1}')
-            m.eval()
-            for param in m.parameters():
-                param.requires_grad = False
-
-            # freeze blocks
-            m = getattr(self, f'blocks{i + 1}')
-            m.eval()
-            for param in m.parameters():
-                param.requires_grad = False
-
-            # freeze norm
-            m = getattr(self, f'norm{i + 1}')
-            m.eval()
-            for param in m.parameters():
-                param.requires_grad = False
-
-    def forward(self, x):
-        outs = []
-        for i in range(self.num_stages):
-            patch_embed = getattr(self, f'patch_embed{i + 1}')
-            blocks = getattr(self, f'blocks{i + 1}')
-            norm = getattr(self, f'norm{i + 1}')
-            x, hw_shape = patch_embed(x)
-            for block in blocks:
-                x = block(x)
-            x = x.flatten(2).transpose(1, 2)
-            x = norm(x)
-            x = x.reshape(-1, *hw_shape,
-                          block.out_channels).permute(0, 3, 1, 2).contiguous()
-            if i in self.out_indices:
-                outs.append(x)
+            x = self.norm(x)
 
-        return tuple(outs)
+            return x
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vgg.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/vgg.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vig.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/vig.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vision_transformer.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/vision_transformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,15 @@
             Defaults to 0.
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         qkv_bias (bool): enable bias for qkv if True. Defaults to True.
         ffn_type (str): Select the type of ffn layers. Defaults to 'origin'.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         init_cfg (dict, optional): Initialization config dict.
             Defaults to None.
     """
 
     def __init__(self,
@@ -301,14 +301,15 @@
         # Set out type
         if out_type not in self.OUT_TYPES:
             raise ValueError(f'Unsupported `out_type` {out_type}, please '
                              f'choose from {self.OUT_TYPES}')
         self.out_type = out_type
 
         # Set cls token
+        self.with_cls_token = with_cls_token
         if with_cls_token:
             self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))
         elif out_type != 'cls_token':
             self.cls_token = None
             self.num_extra_tokens = 0
         else:
             raise ValueError(
@@ -389,14 +390,20 @@
 
     def _prepare_pos_embed(self, state_dict, prefix, *args, **kwargs):
         name = prefix + 'pos_embed'
         if name not in state_dict.keys():
             return
 
         ckpt_pos_embed_shape = state_dict[name].shape
+        if (not self.with_cls_token
+                and ckpt_pos_embed_shape[1] == self.pos_embed.shape[1] + 1):
+            # Remove cls token from state dict if it's not used.
+            state_dict[name] = state_dict[name][:, 1:]
+            ckpt_pos_embed_shape = state_dict[name].shape
+
         if self.pos_embed.shape != ckpt_pos_embed_shape:
             from mmengine.logging import MMLogger
             logger = MMLogger.get_current_instance()
             logger.info(
                 f'Resize the pos_embed shape from {ckpt_pos_embed_shape} '
                 f'to {self.pos_embed.shape}.')
 
@@ -421,27 +428,37 @@
             self.pos_embed.requires_grad = False
         # set dropout to eval model
         self.drop_after_pos.eval()
         # freeze patch embedding
         self.patch_embed.eval()
         for param in self.patch_embed.parameters():
             param.requires_grad = False
+        # freeze pre-norm
+        for param in self.pre_norm.parameters():
+            param.requires_grad = False
         # freeze cls_token
-        self.cls_token.requires_grad = False
+        if self.cls_token:
+            self.cls_token.requires_grad = False
         # freeze layers
         for i in range(1, self.frozen_stages + 1):
             m = self.layers[i - 1]
             m.eval()
             for param in m.parameters():
                 param.requires_grad = False
         # freeze the last layer norm
-        if self.frozen_stages == len(self.layers) and self.final_norm:
-            self.ln1.eval()
-            for param in self.ln1.parameters():
-                param.requires_grad = False
+        if self.frozen_stages == len(self.layers):
+            if self.final_norm:
+                self.ln1.eval()
+                for param in self.ln1.parameters():
+                    param.requires_grad = False
+
+            if self.out_type == 'avg_featmap':
+                self.ln2.eval()
+                for param in self.ln2.parameters():
+                    param.requires_grad = False
 
     def forward(self, x):
         B = x.shape[0]
         x, patch_resolution = self.patch_embed(x)
 
         if self.cls_token is not None:
             # stole cls_tokens impl from Phil Wang, thanks
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vit_eva02.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/vit_eva02.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/vit_sam.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/vit_sam.py`

 * *Files 0% similar despite different names*

```diff
@@ -231,15 +231,15 @@
         drop_rate (float): Probability of an element to be zeroed
             after the feed forward layer. Defaults to 0.
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         qkv_bias (bool): enable bias for qkv if True. Defaults to True.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         use_rel_pos (bool):Whether to use relative position embedding.
             Defaults to False.
         window_size (int): Window size for window attention. Defaults to 0.
         input_size (int, optional): Input resolution for calculating the
             relative positional parameter size. Defaults to None.
@@ -589,19 +589,19 @@
 
         outs = []
         for i, layer in enumerate(self.layers):
             x = layer(x)
 
             if i in self.out_indices:
                 # (B, H, W, C) -> (B, C, H, W)
-                x = x.permute(0, 3, 1, 2)
+                x_reshape = x.permute(0, 3, 1, 2)
 
                 if self.out_channels > 0:
-                    x = self.channel_reduction(x)
-                outs.append(self._format_output(x))
+                    x_reshape = self.channel_reduction(x_reshape)
+                outs.append(self._format_output(x_reshape))
 
         return tuple(outs)
 
     def _format_output(self, x) -> torch.Tensor:
         if self.out_type == 'raw' or self.out_type == 'featmap':
             return x
         elif self.out_type == 'avg_featmap':
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/backbones/xcit.py` & `mmpretrain-1.0.1/mmpretrain/models/backbones/xcit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/builder.py` & `mmpretrain-1.0.1/mmpretrain/models/builder.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/base.py` & `mmpretrain-1.0.1/mmpretrain/models/classifiers/base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/hugging_face.py` & `mmpretrain-1.0.1/mmpretrain/models/classifiers/hugging_face.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/image.py` & `mmpretrain-1.0.1/mmpretrain/models/classifiers/image.py`

 * *Files 3% similar despite different names*

```diff
@@ -74,14 +74,20 @@
         if head is not None and not isinstance(head, nn.Module):
             head = MODELS.build(head)
 
         self.backbone = backbone
         self.neck = neck
         self.head = head
 
+        # If the model needs to load pretrain weights from a third party,
+        # the key can be modified with this hook
+        if hasattr(self.backbone, '_checkpoint_filter'):
+            self._register_load_state_dict_pre_hook(
+                self.backbone._checkpoint_filter)
+
     def forward(self,
                 inputs: torch.Tensor,
                 data_samples: Optional[List[DataSample]] = None,
                 mode: str = 'tensor'):
         """The unified entry for a forward process in both training and test.
 
         The method should accept three modes: "tensor", "predict" and "loss":
@@ -251,9 +257,9 @@
         Returns:
             Tuple[int, int]: The layer-wise depth and the max depth.
         """
         if hasattr(self.backbone, 'get_layer_depth'):
             return self.backbone.get_layer_depth(param_name, 'backbone.')
         else:
             raise NotImplementedError(
-                f"The babckone {type(self.backbone)} doesn't "
+                f"The backbone {type(self.backbone)} doesn't "
                 'support `get_layer_depth` by now.')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/classifiers/timm.py` & `mmpretrain-1.0.1/mmpretrain/models/classifiers/timm.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,28 +6,30 @@
 from .conformer_head import ConformerHead
 from .contrastive_head import ContrastiveHead
 from .deit_head import DeiTClsHead
 from .efficientformer_head import EfficientFormerClsHead
 from .grounding_head import GroundingHead
 from .itc_head import ITCHead
 from .itm_head import ITMHead
+from .itpn_clip_head import iTPNClipHead
 from .latent_heads import LatentCrossCorrelationHead, LatentPredictHead
 from .levit_head import LeViTClsHead
 from .linear_head import LinearClsHead
 from .mae_head import MAEPretrainHead
 from .margin_head import ArcFaceClsHead
 from .mim_head import MIMHead
 from .mixmim_head import MixMIMPretrainHead
 from .mocov3_head import MoCoV3Head
 from .multi_label_cls_head import MultiLabelClsHead
 from .multi_label_csra_head import CSRAClsHead
 from .multi_label_linear_head import MultiLabelLinearClsHead
 from .multi_task_head import MultiTaskHead
 from .seq_gen_head import SeqGenerationHead
 from .simmim_head import SimMIMHead
+from .spark_head import SparKPretrainHead
 from .stacked_head import StackedLinearClsHead
 from .swav_head import SwAVHead
 from .vig_head import VigClsHead
 from .vision_transformer_head import VisionTransformerClsHead
 from .vqa_head import VQAGenerationHead
 
 __all__ = [
@@ -58,8 +60,10 @@
     'MIMHead',
     'SimMIMHead',
     'SeqGenerationHead',
     'VQAGenerationHead',
     'ITCHead',
     'ITMHead',
     'GroundingHead',
+    'iTPNClipHead',
+    'SparKPretrainHead',
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/beitv1_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/beitv1_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/beitv2_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/beitv2_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/cae_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/cae_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/cls_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/cls_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/conformer_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/conformer_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/contrastive_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/contrastive_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/deit_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/deit_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/efficientformer_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/efficientformer_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/grounding_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/grounding_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/itc_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/itc_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/itm_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/itm_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/latent_heads.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/latent_heads.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/levit_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/levit_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/linear_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/linear_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/mae_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/mae_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/margin_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/margin_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/mim_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/mim_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/mixmim_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/mixmim_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/mocov3_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/mocov3_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_cls_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_cls_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_csra_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_csra_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_label_linear_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/multi_label_linear_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/multi_task_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/multi_task_head.py`

 * *Files 15% similar despite different names*

```diff
@@ -115,20 +115,32 @@
         Returns:
             List[MultiTaskDataSample]: A list of data samples which contains
             the predicted results.
         """
         predictions_dict = dict()
 
         for task_name, head in self.task_heads.items():
-            task_samples = head.predict(feats)
+            task_samples = None
+            if data_samples is not None:
+                task_samples = [
+                    data_sample.get(task_name, None) if data_sample else None
+                    for data_sample in data_samples
+                ]
+
+            task_samples = head.predict(feats, task_samples)
             batch_size = len(task_samples)
             predictions_dict[task_name] = task_samples
 
         if data_samples is None:
             data_samples = [MultiTaskDataSample() for _ in range(batch_size)]
+        else:
+            data_samples = [
+                MultiTaskDataSample() if data_sample is None else data_sample
+                for data_sample in data_samples
+            ]
 
         for task_name, task_samples in predictions_dict.items():
             for data_sample, task_sample in zip(data_samples, task_samples):
                 task_sample.set_field(
                     task_name in data_sample.tasks,
                     'eval_mask',
                     field_type='metainfo')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/seq_gen_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/seq_gen_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/simmim_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/simmim_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/stacked_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/stacked_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/swav_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/swav_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/vig_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/vig_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/vision_transformer_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/vision_transformer_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/heads/vqa_head.py` & `mmpretrain-1.0.1/mmpretrain/models/heads/vqa_head.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/asymmetric_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/asymmetric_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/cae_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/cae_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/cosine_similarity_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/cosine_similarity_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/cross_correlation_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/cross_correlation_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/cross_entropy_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/cross_entropy_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/focal_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/focal_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/label_smooth_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/label_smooth_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/reconstruction_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/reconstruction_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/seesaw_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/seesaw_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/swav_loss.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/swav_loss.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/losses/utils.py` & `mmpretrain-1.0.1/mmpretrain/models/losses/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -2,17 +2,20 @@
 from mmpretrain.utils.dependency import WITH_MULTIMODAL
 
 if WITH_MULTIMODAL:
     from .blip import *  # noqa: F401,F403
     from .blip2 import *  # noqa: F401,F403
     from .chinese_clip import *  # noqa: F401, F403
     from .flamingo import *  # noqa: F401, F403
+    from .llava import *  # noqa: F401, F403
+    from .minigpt4 import *  # noqa: F401, F403
     from .ofa import *  # noqa: F401, F403
+    from .otter import *  # noqa: F401, F403
 else:
     from mmpretrain.registry import MODELS
     from mmpretrain.utils.dependency import register_multimodal_placeholder
 
     register_multimodal_placeholder([
         'Blip2Caption', 'Blip2Retrieval', 'Blip2VQA', 'BlipCaption',
         'BlipNLVR', 'BlipRetrieval', 'BlipGrounding', 'BlipVQA', 'Flamingo',
-        'OFA', 'ChineseCLIP'
+        'OFA', 'ChineseCLIP', 'MiniGPT4', 'Llava', 'Otter'
     ], MODELS)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_caption.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_grounding.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_grounding.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_nlvr.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_nlvr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_retrieval.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_retrieval.py`

 * *Files 0% similar despite different names*

```diff
@@ -159,15 +159,15 @@
             [self.vision_backbone, self.vision_backbone_m],
             [self.text_backbone, self.text_backbone_m],
             [self.vision_neck, self.vision_neck_m],
             [self.text_neck, self.text_neck_m],
         ]
         self.copy_params()
 
-        # multimodal backone shares weights with text backbone in BLIP
+        # multimodal backbone shares weights with text backbone in BLIP
         # No need to set up
 
         # Notice that this topk is used for select k candidate to compute
         # image-text score, but not the final metric topk in evaluation.
         self.fast_match = fast_match
         self.topk = topk
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/blip_vqa.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/blip_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip/language_model.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip/language_model.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/Qformer.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/Qformer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_caption.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_caption.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_opt_vqa.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_opt_vqa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/blip2_retriever.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/blip2_retriever.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/blip2/modeling_opt.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/blip2/modeling_opt.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/bert.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/bert.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/chinese_clip.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/chinese_clip.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/chinese_clip/utils.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/chinese_clip/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/adapter.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/adapter.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     @classmethod
     def extend_init(
         cls,
         base: object,
         vis_hidden_size: int,
         cross_attn_every_n_layers: int,
         use_media_placement_augmentation: bool,
+        only_attend_previous: bool = False,
     ):
         """Initialize Flamingo by adding a new gated cross attn to the decoder.
 
         Store the media token id for computing the media locations.
 
         Args:
             base (object): Base module could be any object that represent
@@ -44,14 +45,15 @@
             nn.ModuleList([
                 FlamingoLayer(gated_cross_attn_layer, decoder_layer)
                 for gated_cross_attn_layer, decoder_layer in zip(
                     gated_cross_attn_layers, base._get_decoder_layers())
             ]))
         base.use_media_placement_augmentation = use_media_placement_augmentation  # noqa
         base.initialized_flamingo = True
+        base.only_attend_previous = only_attend_previous
         return base
 
     def set_decoder_layers_attr_name(self, decoder_layers_attr_name):
         """Set decoder layers attribute name."""
         self.decoder_layers_attr_name = decoder_layers_attr_name
 
     def _get_decoder_layers(self):
@@ -63,16 +65,20 @@
         setattr_recursive(self, self.decoder_layers_attr_name, value)
 
     def forward(self, *input, **kwargs):
         """Condition the Flamingo layers on the media locations before forward
         function."""
         input_ids = kwargs['input_ids'] if 'input_ids' in kwargs else input[0]
         media_locations = input_ids == self.media_token_id
-        attend_previous = ((random.random() < 0.5)
-                           if self.use_media_placement_augmentation else False)
+        if self.only_attend_previous:
+            attend_previous = True
+        elif self.use_media_placement_augmentation:
+            attend_previous = (random.random() < 0.5)
+        else:
+            attend_previous = False
 
         for layer in self.get_decoder().layers:
             layer.condition_media_locations(media_locations)
             layer.condition_attend_previous(attend_previous)
 
         return super().forward(
             *input, **kwargs)  # Call the other parent's forward method
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/flamingo.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/flamingo.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,15 +19,15 @@
         vision_encoder (dict): The config of the vision encoder.
         lang_encoder (dict): The config of the language encoder.
         tokenizer (dict): The tokenizer to encode the text.
         task (int): The task to perform prediction.
         zeroshot_prompt (str): Prompt used for zero-shot inference.
             Defaults to '<image>Output:'.
         shot_prompt_tmpl (str): Prompt used for few-shot inference.
-            Defaults to '<image>Output:{caption}<|endofchunk|>'.
+            Defaults to ``<image>Output:{caption}<|endofchunk|>``.
         final_prompt_tmpl (str): Final part of prompt used for inference.
             Defaults to '<image>Output:'.
         generation_cfg (dict): The extra generation config, accept the keyword
             arguments of [~`transformers.GenerationConfig`].
             Defaults to an empty dict.
         data_preprocessor (Optional[dict]): The config for preprocessing input
             data. If None or no specified type, it will use
@@ -92,14 +92,15 @@
             from mmengine.runner.checkpoint import load_checkpoint
             load_checkpoint(
                 self.vision_encoder,
                 vision_encoder_weight,
                 map_location='cpu',
                 revise_keys=[(r'^backbone\.', '')],
             )
+            self.vision_encoder.is_init = True
 
         self.perceiver = PerceiverResampler(dim=self.vision_encoder.embed_dims)
 
         # init language encoder related modules
         self.lang_encoder = ExtendModule(**lang_encoder)
         self.lang_encoder.resize_token_embeddings(len(self.tokenizer))
         self.lang_encoder.media_token_id = self.tokenizer.encode('<image>')[-1]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/modules.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/modules.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/flamingo/utils.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/flamingo/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/ofa.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/ofa.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/multimodal/ofa/ofa_modules.py` & `mmpretrain-1.0.1/mmpretrain/models/multimodal/ofa/ofa_modules.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .beitv2_neck import BEiTV2Neck
 from .cae_neck import CAENeck
 from .densecl_neck import DenseCLNeck
 from .gap import GlobalAveragePooling
 from .gem import GeneralizedMeanPooling
 from .hr_fuse import HRFuseScales
+from .itpn_neck import iTPNPretrainDecoder
 from .linear_neck import LinearNeck
 from .mae_neck import ClsBatchNormNeck, MAEPretrainDecoder
 from .milan_neck import MILANPretrainDecoder
 from .mixmim_neck import MixMIMPretrainDecoder
 from .mocov2_neck import MoCoV2Neck
 from .nonlinear_neck import NonLinearNeck
 from .simmim_neck import SimMIMLinearDecoder
+from .spark_neck import SparKLightDecoder
 from .swav_neck import SwAVNeck
 
 __all__ = [
     'GlobalAveragePooling',
     'GeneralizedMeanPooling',
     'HRFuseScales',
     'LinearNeck',
@@ -26,8 +28,10 @@
     'ClsBatchNormNeck',
     'MILANPretrainDecoder',
     'MixMIMPretrainDecoder',
     'MoCoV2Neck',
     'NonLinearNeck',
     'SimMIMLinearDecoder',
     'SwAVNeck',
+    'iTPNPretrainDecoder',
+    'SparKLightDecoder',
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/beitv2_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/beitv2_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/cae_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/cae_neck.py`

 * *Files 0% similar despite different names*

```diff
@@ -35,15 +35,15 @@
         drop_rate (float): The dropout rate. Defaults to 0.0.
         attn_drop_rate (float): The drop out rate for attention output weights.
             Defaults to 0.
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.
         layer_scale_init_value (float): The init value of gamma.
             Defaults to 0.0.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
     """
 
     def __init__(
         self,
         embed_dims: int,
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/densecl_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/densecl_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/gap.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/gap.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/gem.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/gem.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/hr_fuse.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/hr_fuse.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/linear_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/linear_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/mae_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/mae_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/milan_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/milan_neck.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,15 +25,15 @@
         attn_drop_rate (float): The drop out rate for attention layer.
             Defaults to 0.0.
         drop_path_rate (float): Stochastic depth rate. Defaults to 0.0.
         num_fcs (int): The number of fully-connected layers for FFNs.
             Defaults to 2.
         qkv_bias (bool): Enable bias for qkv if True. Defaults to True.
         act_cfg (dict): The activation config for FFNs.
-            Defaluts to ``dict(type='GELU')``.
+            Defaults to ``dict(type='GELU')``.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
         batch_first (bool): Key, Query and Value are shape of
             (batch, n, embed_dim)
             or (n, batch, embed_dim). Defaults to False.
         init_cfg (dict, optional): The Config for initialization.
             Defaults to None.
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/mixmim_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/mixmim_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/mocov2_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/mocov2_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/nonlinear_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/nonlinear_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/simmim_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/simmim_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/necks/swav_neck.py` & `mmpretrain-1.0.1/mmpretrain/models/necks/swav_neck.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/base.py` & `mmpretrain-1.0.1/mmpretrain/models/retrievers/base.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/retrievers/image2image.py` & `mmpretrain-1.0.1/mmpretrain/models/retrievers/image2image.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -2,32 +2,37 @@
 from .barlowtwins import BarlowTwins
 from .base import BaseSelfSupervisor
 from .beit import VQKD, BEiT, BEiTPretrainViT
 from .byol import BYOL
 from .cae import CAE, CAEPretrainViT, DALLEEncoder
 from .densecl import DenseCL
 from .eva import EVA
-from .mae import MAE, MAEViT
+from .itpn import iTPN, iTPNHiViT
+from .mae import MAE, MAEHiViT, MAEViT
 from .maskfeat import HOGGenerator, MaskFeat, MaskFeatViT
 from .milan import MILAN, CLIPGenerator, MILANViT
 from .mixmim import MixMIM, MixMIMPretrainTransformer
 from .moco import MoCo
 from .mocov3 import MoCoV3, MoCoV3ViT
 from .simclr import SimCLR
 from .simmim import SimMIM, SimMIMSwinTransformer
 from .simsiam import SimSiam
+from .spark import SparK
 from .swav import SwAV
 
 __all__ = [
     'BaseSelfSupervisor',
     'BEiTPretrainViT',
     'VQKD',
     'CAEPretrainViT',
     'DALLEEncoder',
     'MAEViT',
+    'MAEHiViT',
+    'iTPNHiViT',
+    'iTPN',
     'HOGGenerator',
     'MaskFeatViT',
     'CLIPGenerator',
     'MILANViT',
     'MixMIMPretrainTransformer',
     'MoCoV3ViT',
     'SimMIMSwinTransformer',
@@ -43,8 +48,9 @@
     'MILAN',
     'MixMIM',
     'SimMIM',
     'EVA',
     'DenseCL',
     'BarlowTwins',
     'SwAV',
+    'SparK',
 ]
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/barlowtwins.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/barlowtwins.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/base.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -171,9 +171,9 @@
         Returns:
             Tuple[int, int]: The layer-wise depth and the max depth.
         """
         if hasattr(self.backbone, 'get_layer_depth'):
             return self.backbone.get_layer_depth(param_name, 'backbone.')
         else:
             raise NotImplementedError(
-                f"The babckone {type(self.backbone)} doesn't "
+                f"The backbone {type(self.backbone)} doesn't "
                 'support `get_layer_depth` by now.')
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/beit.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/beit.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/byol.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/byol.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/cae.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/cae.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/densecl.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/densecl.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/eva.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/eva.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mae.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/mixmim.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,221 +1,250 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Dict, List, Optional, Sequence, Tuple, Union
+import random
+from typing import Dict, List, Optional, Tuple, Union
 
 import torch
+from torch import nn
+from torch.nn import functional as F
 
-from mmpretrain.models import VisionTransformer
+from mmpretrain.models.backbones import MixMIMTransformer
 from mmpretrain.registry import MODELS
 from mmpretrain.structures import DataSample
 from ..utils import build_2d_sincos_position_embedding
 from .base import BaseSelfSupervisor
 
 
 @MODELS.register_module()
-class MAEViT(VisionTransformer):
-    """Vision Transformer for MAE pre-training.
+class MixMIMPretrainTransformer(MixMIMTransformer):
+    """MixMIM backbone for MixMIM pre-training.
 
-    A PyTorch implement of: `An Image is Worth 16x16 Words: Transformers
-    for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.
-    This module implements the patch masking in MAE and initialize the
-    position embedding with sine-cosine position embedding.
+    A PyTorch implement of : ` MixMIM: Mixed and Masked Image
+    Modeling for Efficient Visual Representation Learning
+    <https://arxiv.org/abs/2205.13137>`_
 
     Args:
-        arch (str | dict): Vision Transformer architecture
-            Default: 'b'
-        img_size (int | tuple): Input image size
-        patch_size (int | tuple): The patch size
-        out_indices (Sequence | int): Output from which stages.
-            Defaults to -1, means the last stage.
-        drop_rate (float): Probability of an element to be zeroed.
-            Defaults to 0.
-        drop_path_rate (float): stochastic depth rate. Defaults to 0.
+        arch (str | dict): MixMIM architecture. If use string,
+            choose from 'base','large' and 'huge'.
+            If use dict, it should have below keys:
+
+            - **embed_dims** (int): The dimensions of embedding.
+            - **depths** (int): The number of transformer encoder layers.
+            - **num_heads** (int): The number of heads in attention modules.
+
+            Defaults to 'base'.
+        mlp_ratio (int): The mlp ratio in FFN.  Defaults to 4.
+        img_size (int | tuple): The expected input image shape. Because we
+            support dynamic input shape, just set the argument to mlp_ratio
+            the most common input image shape. Defaults to 224.
+        patch_size (int | tuple): The patch size in patch embedding.
+            Defaults to 16.
+        in_channels (int): The num of input channels. Defaults to 3.
+        window_size (list): The height and width of the window.
+        qkv_bias (bool): Whether to add bias for qkv in attention modules.
+            Defaults to True.
+        patch_cfg (dict): Extra config dict for patch embedding.
+            Defaults to an empty dict.
         norm_cfg (dict): Config dict for normalization layer.
             Defaults to ``dict(type='LN')``.
-        final_norm (bool): Whether to add a additional layer to normalize
-            final feature map. Defaults to True.
-        out_type (str): The type of output features. Please choose from
-
-            - ``"cls_token"``: The class token tensor with shape (B, C).
-            - ``"featmap"``: The feature map tensor from the patch tokens
-              with shape (B, C, H, W).
-            - ``"avg_featmap"``: The global averaged feature map tensor
-              with shape (B, C).
-            - ``"raw"``: The raw feature tensor includes patch tokens and
-              class tokens with shape (B, L, C).
-
-            It only works without input mask. Defaults to ``"avg_featmap"``.
-        interpolate_mode (str): Select the interpolate mode for position
-            embeding vector resize. Defaults to "bicubic".
-        patch_cfg (dict): Configs of patch embeding. Defaults to an empty dict.
-        layer_cfgs (Sequence | dict): Configs of each transformer layer in
-            encoder. Defaults to an empty dict.
-        mask_ratio (bool): The ratio of total number of patches to be masked.
-            Defaults to 0.75.
-        init_cfg (Union[List[dict], dict], optional): Initialization config
-            dict. Defaults to None.
+        drop_rate (float): Probability of an element to be zeroed.
+            Defaults to 0.
+        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
+        attn_drop_rate (float): Attention drop rate. Defaults to 0.
+        use_checkpoint (bool): Whether use the checkpoint to reduce GPU memory
+            cost. Defaults to False.
+        mask_ratio (bool): The base ratio of total number of patches to be
+            masked. Defaults to 0.5.
+        range_mask_ratio (float): The range of mask ratio.
+            Defaults to 0.
+        init_cfg (dict, optional): Initialization config dict.
+            Defaults to None.
     """
 
     def __init__(self,
-                 arch: Union[str, dict] = 'b',
+                 arch: Union[str, dict] = 'base',
+                 mlp_ratio: float = 4,
                  img_size: int = 224,
-                 patch_size: int = 16,
-                 out_indices: Union[Sequence, int] = -1,
-                 drop_rate: float = 0,
-                 drop_path_rate: float = 0,
-                 norm_cfg: dict = dict(type='LN', eps=1e-6),
-                 final_norm: bool = True,
-                 out_type: str = 'raw',
-                 interpolate_mode: str = 'bicubic',
+                 patch_size: int = 4,
+                 in_channels: int = 3,
+                 window_size: List = [14, 14, 14, 7],
+                 qkv_bias: bool = True,
                  patch_cfg: dict = dict(),
-                 layer_cfgs: dict = dict(),
-                 mask_ratio: float = 0.75,
-                 init_cfg: Optional[Union[List[dict], dict]] = None) -> None:
+                 norm_cfg: dict = dict(type='LN'),
+                 drop_rate: float = 0.0,
+                 drop_path_rate: float = 0.0,
+                 attn_drop_rate: float = 0.0,
+                 use_checkpoint: bool = False,
+                 mask_ratio: float = 0.5,
+                 range_mask_ratio: float = 0.0,
+                 init_cfg: Optional[dict] = None) -> None:
+
         super().__init__(
             arch=arch,
+            mlp_ratio=mlp_ratio,
             img_size=img_size,
             patch_size=patch_size,
-            out_indices=out_indices,
+            in_channels=in_channels,
+            window_size=window_size,
+            qkv_bias=qkv_bias,
+            patch_cfg=patch_cfg,
+            norm_cfg=norm_cfg,
             drop_rate=drop_rate,
             drop_path_rate=drop_path_rate,
-            norm_cfg=norm_cfg,
-            final_norm=final_norm,
-            out_type=out_type,
-            with_cls_token=True,
-            interpolate_mode=interpolate_mode,
-            patch_cfg=patch_cfg,
-            layer_cfgs=layer_cfgs,
+            attn_drop_rate=attn_drop_rate,
+            use_checkpoint=use_checkpoint,
             init_cfg=init_cfg)
 
-        # position embedding is not learnable during pretraining
-        self.pos_embed.requires_grad = False
         self.mask_ratio = mask_ratio
-        self.num_patches = self.patch_resolution[0] * self.patch_resolution[1]
+        self.range_mask_ratio = range_mask_ratio
+
+    def init_weights(self):
+        """Initialize position embedding, patch embedding."""
+        super(MixMIMTransformer, self).init_weights()
 
-    def init_weights(self) -> None:
-        """Initialize position embedding, patch embedding and cls token."""
-        super().init_weights()
         pos_embed = build_2d_sincos_position_embedding(
             int(self.num_patches**.5),
-            self.pos_embed.shape[-1],
-            cls_token=True)
-        self.pos_embed.data.copy_(pos_embed.float())
-
-        w = self.patch_embed.projection.weight.data
-        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
-
-        torch.nn.init.normal_(self.cls_token, std=.02)
-
-    def random_masking(
-        self,
-        x: torch.Tensor,
-        mask_ratio: float = 0.75
-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        """Generate the mask for MAE Pre-training.
+            self.absolute_pos_embed.shape[-1],
+            cls_token=False)
+        self.absolute_pos_embed.data.copy_(pos_embed.float())
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            # we use xavier_uniform following official JAX ViT:
+            torch.nn.init.xavier_uniform_(m.weight)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                nn.init.constant_(m.bias, 0)
+        elif isinstance(m, nn.LayerNorm):
+            nn.init.constant_(m.bias, 0)
+            nn.init.constant_(m.weight, 1.0)
+
+    def random_masking(self,
+                       x: torch.Tensor,
+                       mask_ratio: float = 0.5) -> Tuple[torch.Tensor]:
+        """Generate the mask for MixMIM Pretraining.
 
         Args:
             x (torch.Tensor): Image with data augmentation applied, which is
                 of shape B x L x C.
             mask_ratio (float): The mask ratio of total patches.
-                Defaults to 0.75.
+                Defaults to 0.5.
 
         Returns:
-            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: masked image, mask
-            and the ids to restore original image.
-
-            - ``x_masked`` (torch.Tensor): masked image.
-            - ``mask`` (torch.Tensor): mask used to mask image.
-            - ``ids_restore`` (torch.Tensor): ids to restore original image.
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
+                - mask_s1 (torch.Tensor): mask with stride of
+                  self.encoder_stride // 8.
+                - mask_s2 (torch.Tensor): mask with stride of
+                  self.encoder_stride // 4.
+                - mask_s3 (torch.Tensor): mask with stride of
+                  self.encoder_stride // 2.
+                - mask (torch.Tensor): mask with stride of
+                  self.encoder_stride.
         """
-        N, L, D = x.shape  # batch, length, dim
-        len_keep = int(L * (1 - mask_ratio))
 
-        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]
-
-        # sort noise for each sample
-        ids_shuffle = torch.argsort(
-            noise, dim=1)  # ascend: small is keep, large is remove
-        ids_restore = torch.argsort(ids_shuffle, dim=1)
-
-        # keep the first subset
-        ids_keep = ids_shuffle[:, :len_keep]
-        x_masked = torch.gather(
-            x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
-
-        # generate the binary mask: 0 is keep, 1 is remove
-        mask = torch.ones([N, L], device=x.device)
-        mask[:, :len_keep] = 0
-        # unshuffle to get the binary mask
-        mask = torch.gather(mask, dim=1, index=ids_restore)
-
-        return x_masked, mask, ids_restore
-
-    def forward(
-        self,
-        x: torch.Tensor,
-        mask: Optional[bool] = True
-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        B, C, H, W = x.shape
+        out_H = H // self.encoder_stride
+        out_W = W // self.encoder_stride
+        s3_H, s3_W = out_H * 2, out_W * 2
+        s2_H, s2_W = out_H * 4, out_W * 4
+        s1_H, s1_W = out_H * 8, out_W * 8
+
+        seq_l = out_H * out_W
+        # use a shared mask for a batch images
+        mask = torch.zeros([1, 1, seq_l], device=x.device)
+
+        mask_ratio = mask_ratio + random.uniform(0.0, self.range_mask_ratio)
+        noise = torch.rand(1, 1, seq_l, device=x.device)  # noise in [0, 1]
+        # ascend: small is keep, large is removed
+        mask_idx = torch.argsort(noise, dim=2)[:, :, :int(seq_l * mask_ratio)]
+        mask.scatter_(2, mask_idx, 1)
+        mask = mask.reshape(1, 1, out_H, out_W)
+        mask_s1 = F.interpolate(mask, size=(s1_H, s1_W), mode='nearest')
+        mask_s2 = F.interpolate(mask, size=(s2_H, s2_W), mode='nearest')
+        mask_s3 = F.interpolate(mask, size=(s3_H, s3_W), mode='nearest')
+
+        mask = mask.reshape(1, out_H * out_W, 1).contiguous()
+        mask_s1 = mask_s1.reshape(1, s1_H * s1_W, 1).contiguous()
+        mask_s2 = mask_s2.reshape(1, s2_H * s2_W, 1).contiguous()
+        mask_s3 = mask_s3.reshape(1, s3_H * s3_W, 1).contiguous()
+
+        return mask_s1, mask_s2, mask_s3, mask
+
+    def forward(self,
+                x: torch.Tensor,
+                mask: Optional[bool] = True) -> Tuple[torch.Tensor]:
         """Generate features for masked images.
 
-        The function supports two kind of forward behaviors. If the ``mask`` is
-        ``True``, the function will generate mask to masking some patches
-        randomly and get the hidden features for visible patches, which means
-        the function will be executed as masked imagemodeling pre-training;
-        if the ``mask`` is ``None`` or ``False``, the forward function will
-        call ``super().forward()``, which extract features from images without
-        mask.
-
+        This function generates mask and masks some patches randomly and get
+        the hidden features for visible patches.
 
         Args:
             x (torch.Tensor): Input images, which is of shape B x C x H x W.
-            mask (bool, optional): To indicate whether the forward function
-                generating ``mask`` or not.
+            mask (bool, optional): To indicate whether the forward containing
+                ``mask`` or not.
 
         Returns:
-            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Hidden features,
-            mask and the ids to restore original image.
-
-            - ``x`` (torch.Tensor): hidden features, which is of shape
-              B x (L * mask_ratio) x C.
-            - ``mask`` (torch.Tensor): mask used to mask image.
-            - ``ids_restore`` (torch.Tensor): ids to restore original image.
+            Tuple[torch.Tensor, torch.Tensor]:
+              - x (torch.Tensor): hidden features, which is of shape
+                B x L x C.
+              - mask_s4 (torch.Tensor): the mask tensor for the last layer.
         """
         if mask is None or False:
             return super().forward(x)
 
         else:
-            B = x.shape[0]
-            x = self.patch_embed(x)[0]
-            # add pos embed w/o cls token
-            x = x + self.pos_embed[:, 1:, :]
-
-            # masking: length -> length * mask_ratio
-            x, mask, ids_restore = self.random_masking(x, self.mask_ratio)
-
-            # append cls token
-            cls_token = self.cls_token + self.pos_embed[:, :1, :]
-            cls_tokens = cls_token.expand(B, -1, -1)
-            x = torch.cat((cls_tokens, x), dim=1)
-
-            for _, layer in enumerate(self.layers):
-                x = layer(x)
-            # Use final norm
-            x = self.norm1(x)
+            mask_s1, mask_s2, mask_s3, mask_s4 = self.random_masking(
+                x, self.mask_ratio)
+
+            x, _ = self.patch_embed(x)
+
+            x = x * (1. - mask_s1) + x.flip(0) * mask_s1
+            x = x + self.absolute_pos_embed
+            x = self.drop_after_pos(x)
+
+            for idx, layer in enumerate(self.layers):
+                if idx == 0:
+                    x = layer(x, attn_mask=mask_s1)
+                elif idx == 1:
+                    x = layer(x, attn_mask=mask_s2)
+                elif idx == 2:
+                    x = layer(x, attn_mask=mask_s3)
+                elif idx == 3:
+                    x = layer(x, attn_mask=mask_s4)
+
+            x = self.norm(x)
 
-            return (x, mask, ids_restore)
+            return x, mask_s4
 
 
 @MODELS.register_module()
-class MAE(BaseSelfSupervisor):
-    """MAE.
+class MixMIM(BaseSelfSupervisor):
+    """MixMIM.
 
-    Implementation of `Masked Autoencoders Are Scalable Vision Learners
-    <https://arxiv.org/abs/2111.06377>`_.
+    Implementation of `MixMIM: Mixed and Masked Image Modeling for Efficient
+    Visual Representation Learning. <https://arxiv.org/abs/2205.13137>`_.
     """
 
+    def __init__(self,
+                 backbone: dict,
+                 neck: Optional[dict] = None,
+                 head: Optional[dict] = None,
+                 pretrained: Optional[str] = None,
+                 data_preprocessor: Optional[Union[dict, nn.Module]] = None,
+                 init_cfg: Optional[dict] = None):
+
+        head.update(dict(patch_size=neck['encoder_stride']))
+        super().__init__(
+            backbone=backbone,
+            neck=neck,
+            head=head,
+            pretrained=pretrained,
+            data_preprocessor=data_preprocessor,
+            init_cfg=init_cfg)
+
     def extract_feat(self, inputs: torch.Tensor):
         return self.backbone(inputs, mask=None)
 
     def loss(self, inputs: torch.Tensor, data_samples: List[DataSample],
              **kwargs) -> Dict[str, torch.Tensor]:
         """The forward function in training.
 
@@ -223,14 +252,12 @@
             inputs (torch.Tensor): The input images.
             data_samples (List[DataSample]): All elements required
                 during the forward function.
 
         Returns:
             Dict[str, torch.Tensor]: A dictionary of loss components.
         """
-        # ids_restore: the same as that in original repo, which is used
-        # to recover the original order of tokens in decoder.
-        latent, mask, ids_restore = self.backbone(inputs)
-        pred = self.neck(latent, ids_restore)
-        loss = self.head.loss(pred, inputs, mask)
+        latent, mask = self.backbone(inputs)
+        x_rec = self.neck(latent, mask)
+        loss = self.head.loss(x_rec, inputs, mask)
         losses = dict(loss=loss)
         return losses
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/maskfeat.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/maskfeat.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/milan.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/milan.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mixmim.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/itpn.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,249 +1,324 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import random
-from typing import Dict, List, Optional, Tuple, Union
+import math
+from typing import Dict, List, Optional, Tuple
 
 import torch
-from torch import nn
-from torch.nn import functional as F
+import torch.nn as nn
+from mmengine.model.weight_init import trunc_normal_
 
-from mmpretrain.models.backbones import MixMIMTransformer
+from mmpretrain.models.backbones.hivit import BlockWithRPE, HiViT, PatchMerge
 from mmpretrain.registry import MODELS
 from mmpretrain.structures import DataSample
 from ..utils import build_2d_sincos_position_embedding
 from .base import BaseSelfSupervisor
 
 
 @MODELS.register_module()
-class MixMIMPretrainTransformer(MixMIMTransformer):
-    """MixMIM backbone for MixMIM pre-training.
-
-    A PyTorch implement of : ` MixMIM: Mixed and Masked Image
-    Modeling for Efficient Visual Representation Learning
-    <https://arxiv.org/abs/2205.13137>`_
+class iTPNHiViT(HiViT):
+    """HiViT for iTPN pre-training.
 
     Args:
-        arch (str | dict): MixMIM architecture. If use string,
-            choose from 'base','large' and 'huge'.
-            If use dict, it should have below keys:
-
-            - **embed_dims** (int): The dimensions of embedding.
-            - **depths** (int): The number of transformer encoder layers.
-            - **num_heads** (int): The number of heads in attention modules.
-
-            Defaults to 'base'.
-        mlp_ratio (int): The mlp ratio in FFN.  Defaults to 4.
-        img_size (int | tuple): The expected input image shape. Because we
-            support dynamic input shape, just set the argument to mlp_ratio
-            the most common input image shape. Defaults to 224.
-        patch_size (int | tuple): The patch size in patch embedding.
-            Defaults to 16.
-        in_channels (int): The num of input channels. Defaults to 3.
-        window_size (list): The height and width of the window.
-        qkv_bias (bool): Whether to add bias for qkv in attention modules.
-            Defaults to True.
-        patch_cfg (dict): Extra config dict for patch embedding.
-            Defaults to an empty dict.
-        norm_cfg (dict): Config dict for normalization layer.
-            Defaults to ``dict(type='LN')``.
+        img_size (int | tuple): Input image size. Defaults to 224.
+        patch_size (int | tuple): The patch size. Defaults to 16.
+        inner_patches (int): Inner patch. Defaults to 4.
+        stem_mlp_ratio (int): Ratio of MLP hidden dim to embedding dim
+            in the first two stages. Defaults to 3.
+        mlp_ratio (int): Ratio of MLP hidden dim to embedding dim in
+            the last stage. Defaults to 4.
+        qkv_bias (bool): Enable bias for qkv projections if True.
+        qk_scale (float): The number of divider after q@k. Default to None.
         drop_rate (float): Probability of an element to be zeroed.
             Defaults to 0.
-        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
-        attn_drop_rate (float): Attention drop rate. Defaults to 0.
-        use_checkpoint (bool): Whether use the checkpoint to reduce GPU memory
-            cost. Defaults to False.
-        mask_ratio (bool): The base ratio of total number of patches to be
-            masked. Defaults to 0.5.
-        range_mask_ratio (float): The range of mask ratio.
+        attn_drop_rate (float): The drop out rate for attention output weights.
             Defaults to 0.
-        init_cfg (dict, optional): Initialization config dict.
-            Defaults to None.
+        drop_path_rate (float): stochastic depth rate. Defaults to 0.
+        norm_cfg (dict): Config dict for normalization layer.
+            Defaults to ``dict(type='LN')``.
+        ape (bool): If True, add absolute position embedding to
+            the patch embedding.
+        rpe (bool): If True, add relative position embedding to
+            the patch embedding.
+        layer_scale_init_value (float): Layer-scale init values. Defaults to 0.
+        mask_ratio (bool): The ratio of total number of patches to be masked.
+            Defaults to 0.75.
+        reconstruction_type (str): The reconstruction of self-supervised
+            learning. Defaults to 'pixel'.
     """
 
-    def __init__(self,
-                 arch: Union[str, dict] = 'base',
-                 mlp_ratio: float = 4,
-                 img_size: int = 224,
-                 patch_size: int = 4,
-                 in_channels: int = 3,
-                 window_size: List = [14, 14, 14, 7],
-                 qkv_bias: bool = True,
-                 patch_cfg: dict = dict(),
-                 norm_cfg: dict = dict(type='LN'),
-                 drop_rate: float = 0.0,
-                 drop_path_rate: float = 0.0,
-                 attn_drop_rate: float = 0.0,
-                 use_checkpoint: bool = False,
-                 mask_ratio: float = 0.5,
-                 range_mask_ratio: float = 0.0,
-                 init_cfg: Optional[dict] = None) -> None:
-
+    def __init__(
+        self,
+        arch='base',
+        img_size: int = 224,
+        patch_size: int = 16,
+        inner_patches: int = 4,
+        stem_mlp_ratio: int = 3.,
+        mlp_ratio: int = 4.,
+        qkv_bias: bool = True,
+        qk_scale: Optional[bool] = None,
+        drop_rate: float = 0.0,
+        attn_drop_rate: float = 0.0,
+        drop_path_rate: float = 0.0,
+        norm_cfg: dict = dict(type='LN', eps=1e-6),
+        ape: bool = True,
+        rpe: bool = False,
+        layer_scale_init_value: float = 0.0,
+        mask_ratio: float = 0.75,
+        reconstruction_type: str = 'pixel',
+    ):
         super().__init__(
             arch=arch,
-            mlp_ratio=mlp_ratio,
             img_size=img_size,
             patch_size=patch_size,
-            in_channels=in_channels,
-            window_size=window_size,
+            inner_patches=inner_patches,
+            stem_mlp_ratio=stem_mlp_ratio,
+            mlp_ratio=mlp_ratio,
             qkv_bias=qkv_bias,
-            patch_cfg=patch_cfg,
-            norm_cfg=norm_cfg,
+            qk_scale=qk_scale,
             drop_rate=drop_rate,
-            drop_path_rate=drop_path_rate,
             attn_drop_rate=attn_drop_rate,
-            use_checkpoint=use_checkpoint,
-            init_cfg=init_cfg)
+            drop_path_rate=drop_path_rate,
+            norm_cfg=norm_cfg,
+            ape=ape,
+            rpe=rpe,
+            layer_scale_init_value=layer_scale_init_value)
 
+        self.pos_embed.requires_grad = False
         self.mask_ratio = mask_ratio
-        self.range_mask_ratio = range_mask_ratio
 
-    def init_weights(self):
-        """Initialize position embedding, patch embedding."""
-        super(MixMIMTransformer, self).init_weights()
-
-        pos_embed = build_2d_sincos_position_embedding(
-            int(self.num_patches**.5),
-            self.absolute_pos_embed.shape[-1],
-            cls_token=False)
-        self.absolute_pos_embed.data.copy_(pos_embed.float())
-
-        self.apply(self._init_weights)
-
-    def _init_weights(self, m):
-        if isinstance(m, nn.Linear):
-            # we use xavier_uniform following official JAX ViT:
-            torch.nn.init.xavier_uniform_(m.weight)
-            if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-        elif isinstance(m, nn.LayerNorm):
-            nn.init.constant_(m.bias, 0)
-            nn.init.constant_(m.weight, 1.0)
-
-    def random_masking(self,
-                       x: torch.Tensor,
-                       mask_ratio: float = 0.5) -> Tuple[torch.Tensor]:
-        """Generate the mask for MixMIM Pretraining.
+        assert reconstruction_type in ['pixel', 'clip'], \
+            'iTPN method only support `pixel` and `clip`, ' \
+            f'but got `{reconstruction_type}`.'
+        self.reconstruction_type = reconstruction_type
+        self.num_patches = self.patch_embed.num_patches
+
+        if reconstruction_type == 'clip':
+            self.mask_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))
+
+    def init_weights(self) -> None:
+        """Initialize position embedding, patch embedding and cls token."""
+        super().apply(self._init_weights)
+
+        if self.reconstruction_type == 'clip':
+            trunc_normal_(self.mask_token, std=0.02)
+            self.rescale_init_weight()
+        else:
+            pos_embed = build_2d_sincos_position_embedding(
+                int(self.num_patches**.5),
+                self.pos_embed.shape[-1],
+                cls_token=False)
+            self.pos_embed.data.copy_(pos_embed.float())
+
+            w = self.patch_embed.proj.weight.data
+            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
+
+    def rescale_init_weight(self) -> None:
+        """Rescale the initialized weights."""
+
+        def rescale(param, layer_id):
+            param.div_(math.sqrt(2.0 * layer_id))
+
+        for layer_id, layer in enumerate(self.blocks):
+            if isinstance(layer, BlockWithRPE):
+                if layer.attn is not None:
+                    rescale(layer.attn.proj.weight.data, layer_id + 1)
+                rescale(layer.mlp.fc2.weight.data, layer_id + 1)
+
+    def masking_id(self, batch_size, mask_ratio):
+        N, L = batch_size, self.pos_embed.size(1)
+        len_keep = int(L * (1 - mask_ratio))
+
+        noise = torch.rand(
+            N, L, device=self.pos_embed.device)  # noise in [0, 1]
+
+        # sort noise for each sample
+        ids_shuffle = torch.argsort(
+            noise, dim=1)  # ascend: small is keep, large is remove
+        ids_restore = torch.argsort(ids_shuffle, dim=1)
+
+        # keep the first subset
+        ids_keep = ids_shuffle[:, :len_keep]
+        # generate the binary mask: 0 is keep, 1 is remove
+        mask = torch.ones([N, L], device=self.pos_embed.device)
+        mask[:, :ids_keep.size(1)] = 0
+        # unshuffle to get the binary mask
+        mask = torch.gather(mask, dim=1, index=ids_restore)
+
+        return ids_keep, ids_restore, mask
+
+    def forward_pixel(
+        self,
+        x: torch.Tensor,
+        mask: Optional[bool] = True
+    ) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:
+        """Generate features for masked images.
+
+        The function supports two kind of forward behaviors. If the ``mask`` is
+        ``True``, the function will generate mask to masking some patches
+        randomly and get the hidden features for visible patches, which means
+        the function will be executed as masked imagemodeling pre-training;
+        if the ``mask`` is ``None`` or ``False``, the forward function will
+        call ``super().forward()``, which extract features from images without
+        mask.
+
 
         Args:
-            x (torch.Tensor): Image with data augmentation applied, which is
-                of shape B x L x C.
-            mask_ratio (float): The mask ratio of total patches.
-                Defaults to 0.5.
+            x (torch.Tensor): Input images, which is of shape B x C x H x W.
+            mask (bool, optional): To indicate whether the forward function
+                generating ``mask`` or not.
 
         Returns:
-            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
-                - mask_s1 (torch.Tensor): mask with stride of
-                  self.encoder_stride // 8.
-                - mask_s2 (torch.Tensor): mask with stride of
-                  self.encoder_stride // 4.
-                - mask_s3 (torch.Tensor): mask with stride of
-                  self.encoder_stride // 2.
-                - mask (torch.Tensor): mask with stride of
-                  self.encoder_stride.
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Hidden features,
+            mask and the ids to restore original image.
+
+            - ``x`` (torch.Tensor): hidden features, which is of shape
+              B x (L * mask_ratio) x C.
+            - ``mask`` (torch.Tensor): mask used to mask image.
+            - ``ids_restore`` (torch.Tensor): ids to restore original image.
         """
+        if mask is None or False:
+            return super().forward(x)
+
+        else:
+            B, C, H, W = x.shape
+            ids_keep, ids_restore, mask = self.masking_id(B, self.mask_ratio)
 
-        B, C, H, W = x.shape
-        out_H = H // self.encoder_stride
-        out_W = W // self.encoder_stride
-        s3_H, s3_W = out_H * 2, out_W * 2
-        s2_H, s2_W = out_H * 4, out_W * 4
-        s1_H, s1_W = out_H * 8, out_W * 8
-
-        seq_l = out_H * out_W
-        # use a shared mask for a batch images
-        mask = torch.zeros([1, 1, seq_l], device=x.device)
-
-        mask_ratio = mask_ratio + random.uniform(0.0, self.range_mask_ratio)
-        noise = torch.rand(1, 1, seq_l, device=x.device)  # noise in [0, 1]
-        # ascend: small is keep, large is removed
-        mask_idx = torch.argsort(noise, dim=2)[:, :, :int(seq_l * mask_ratio)]
-        mask.scatter_(2, mask_idx, 1)
-        mask = mask.reshape(1, 1, out_H, out_W)
-        mask_s1 = F.interpolate(mask, size=(s1_H, s1_W), mode='nearest')
-        mask_s2 = F.interpolate(mask, size=(s2_H, s2_W), mode='nearest')
-        mask_s3 = F.interpolate(mask, size=(s3_H, s3_W), mode='nearest')
-
-        mask = mask.reshape(1, out_H * out_W, 1).contiguous()
-        mask_s1 = mask_s1.reshape(1, s1_H * s1_W, 1).contiguous()
-        mask_s2 = mask_s2.reshape(1, s2_H * s2_W, 1).contiguous()
-        mask_s3 = mask_s3.reshape(1, s3_H * s3_W, 1).contiguous()
-
-        return mask_s1, mask_s2, mask_s3, mask
-
-    def forward(self,
-                x: torch.Tensor,
-                mask: Optional[bool] = True) -> Tuple[torch.Tensor]:
+            x = self.patch_embed(x)
+
+            x = torch.gather(
+                x,
+                dim=1,
+                index=ids_keep[:, :, None, None,
+                               None].expand(-1, -1, *x.shape[2:]))
+
+            outs = []
+            for blk in self.blocks[:-self.num_main_blocks]:
+                if isinstance(blk, PatchMerge):
+                    outs.append(x)
+                x = blk(x)
+
+            x = x[..., 0, 0, :]
+            if self.ape:
+                pos_embed = self.interpolate_pos_encoding(x, H, W)
+                pos_embed = torch.gather(
+                    pos_embed.expand(B, -1, -1),
+                    dim=1,
+                    index=ids_keep[:, :, None].expand(-1, -1,
+                                                      pos_embed.shape[2]),
+                )
+                x = x + pos_embed
+            x = self.pos_drop(x)
+
+            for blk in self.blocks[-self.num_main_blocks:]:
+                x = blk(x)
+
+            outs.append(x)
+
+            return (tuple(outs), mask, ids_restore)
+
+    def forward_clip(self,
+                     x: torch.Tensor,
+                     mask: Optional[bool] = True) -> Tuple:
         """Generate features for masked images.
 
-        This function generates mask and masks some patches randomly and get
-        the hidden features for visible patches.
+        The function supports two kind of forward behaviors. If the ``mask`` is
+        ``True``, the function will generate mask to masking some patches
+        randomly and get the hidden features for visible patches, which means
+        the function will be executed as masked imagemodeling pre-training;
+        if the ``mask`` is ``None`` or ``False``, the forward function will
+        call ``super().forward()``, which extract features from images without
+        mask.
+
 
         Args:
             x (torch.Tensor): Input images, which is of shape B x C x H x W.
-            mask (bool, optional): To indicate whether the forward containing
-                ``mask`` or not.
+            mask (bool, optional): To indicate whether the forward function
+                generating ``mask`` or not.
 
         Returns:
-            Tuple[torch.Tensor, torch.Tensor]:
-              - x (torch.Tensor): hidden features, which is of shape
-                B x L x C.
-              - mask_s4 (torch.Tensor): the mask tensor for the last layer.
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Hidden features,
+            mask and the ids to restore original image.
+
+            - ``x`` (torch.Tensor): hidden features, which is of shape
+              B x (L * mask_ratio) x C.
+            - ``mask`` (torch.Tensor): mask used to mask image.
+            - ``ids_restore`` (torch.Tensor): ids to restore original image.
         """
         if mask is None or False:
             return super().forward(x)
 
         else:
-            mask_s1, mask_s2, mask_s3, mask_s4 = self.random_masking(
-                x, self.mask_ratio)
+            B, C, H, W = x.shape
+            x = self.patch_embed(x)
 
-            x, _ = self.patch_embed(x)
+            outs = []
+            for blk in self.blocks[:-self.num_main_blocks]:
+                if isinstance(blk, PatchMerge):
+                    outs.append(x)
+                x = blk(x)
 
-            x = x * (1. - mask_s1) + x.flip(0) * mask_s1
-            x = x + self.absolute_pos_embed
-            x = self.drop_after_pos(x)
-
-            for idx, layer in enumerate(self.layers):
-                if idx == 0:
-                    x = layer(x, attn_mask=mask_s1)
-                elif idx == 1:
-                    x = layer(x, attn_mask=mask_s2)
-                elif idx == 2:
-                    x = layer(x, attn_mask=mask_s3)
-                elif idx == 3:
-                    x = layer(x, attn_mask=mask_s4)
+            x = x[..., 0, 0, :]
+            B, L, _ = x.shape
+            mask_token = self.mask_token.expand(B, L, -1)
+            w = mask.flatten(1).unsqueeze(-1).type_as(mask_token)
+            x = x * (1. - w) + mask_token * w
 
-            x = self.norm(x)
+            if self.ape:
+                pos_embed = self.interpolate_pos_encoding(x, H, W)
+                x = x + pos_embed
+            x = self.pos_drop(x)
 
-            return x, mask_s4
+            rpe_index = True if self.rpe else None
 
+            for blk in self.blocks[-self.num_main_blocks:]:
+                x = blk(x, rpe_index)
 
-@MODELS.register_module()
-class MixMIM(BaseSelfSupervisor):
-    """MixMIM.
+            outs.append(x)
 
-    Implementation of `MixMIM: Mixed and Masked Image Modeling for Efficient
-    Visual Representation Learning. <https://arxiv.org/abs/2205.13137>`_.
-    """
+            return tuple(outs)
+
+    def forward(self, x: torch.Tensor, mask: Optional[bool] = True) -> Tuple:
+        """Generate features for masked images.
 
-    def __init__(self,
-                 backbone: dict,
-                 neck: Optional[dict] = None,
-                 head: Optional[dict] = None,
-                 pretrained: Optional[str] = None,
-                 data_preprocessor: Optional[Union[dict, nn.Module]] = None,
-                 init_cfg: Optional[dict] = None):
+        The function supports two kind of forward behaviors. If the ``mask`` is
+        ``True``, the function will generate mask to masking some patches
+        randomly and get the hidden features for visible patches, which means
+        the function will be executed as masked imagemodeling pre-training;
+        if the ``mask`` is ``None`` or ``False``, the forward function will
+        call ``super().forward()``, which extract features from images without
+        mask.
 
-        head.update(dict(patch_size=neck['encoder_stride']))
-        super().__init__(
-            backbone=backbone,
-            neck=neck,
-            head=head,
-            pretrained=pretrained,
-            data_preprocessor=data_preprocessor,
-            init_cfg=init_cfg)
+
+        Args:
+            x (torch.Tensor): Input images, which is of shape B x C x H x W.
+            mask (bool, optional): To indicate whether the forward function
+                generating ``mask`` or not.
+
+        Returns:
+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Hidden features,
+            mask and the ids to restore original image.
+
+            - ``x`` (torch.Tensor): hidden features, which is of shape
+              B x (L * mask_ratio) x C.
+            - ``mask`` (torch.Tensor): mask used to mask image.
+            - ``ids_restore`` (torch.Tensor): ids to restore original image.
+        """
+
+        if self.reconstruction_type == 'pixel':
+            return self.forward_pixel(x, mask)
+        return self.forward_clip(x, mask)
+
+
+@MODELS.register_module()
+class iTPN(BaseSelfSupervisor):
+    """iTPN.
+
+    Implementation of `iTPN: Integrally Pre-Trained Transformer Pyramid
+    Networks <https://arxiv.org/abs/2211.12735>`_.
+    """
 
     def extract_feat(self, inputs: torch.Tensor):
         return self.backbone(inputs, mask=None)
 
     def loss(self, inputs: torch.Tensor, data_samples: List[DataSample],
              **kwargs) -> Dict[str, torch.Tensor]:
         """The forward function in training.
@@ -252,12 +327,30 @@
             inputs (torch.Tensor): The input images.
             data_samples (List[DataSample]): All elements required
                 during the forward function.
 
         Returns:
             Dict[str, torch.Tensor]: A dictionary of loss components.
         """
-        latent, mask = self.backbone(inputs)
-        x_rec = self.neck(latent, mask)
-        loss = self.head.loss(x_rec, inputs, mask)
+
+        if self.backbone.reconstruction_type == 'pixel':
+            latent, mask, ids_restore = self.backbone(inputs)
+            pred = self.neck(latent, ids_restore)
+
+            loss = self.head.loss(pred, inputs, mask)
+        else:
+            mask = torch.stack(
+                [data_sample.mask for data_sample in data_samples])
+
+            img_latent = self.backbone(inputs[0], mask)
+
+            # inputs[1] is the target image
+            with torch.no_grad():
+                target = self.target_generator(inputs[1])[0]
+                target = target.detach()
+
+            # iTPN contains a neck module
+            feats = self.neck(img_latent)
+            loss = self.head.loss(feats, target[:, 1:, :], mask)
+
         losses = dict(loss=loss)
         return losses
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/moco.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/moco.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/mocov3.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/mocov3.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simclr.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/simclr.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simmim.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/simmim.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,15 @@
             Defaults to False.
         frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
             -1 means not freezing any parameters. Defaults to -1.
         norm_eval (bool): Whether to set norm layers to eval mode, namely,
             freeze running stats (mean and var). Note: Effect on Batch Norm
             and its variants only. Defaults to False.
         norm_cfg (dict): Config dict for normalization layer at end
-            of backone. Defaults to dict(type='LN')
+            of backbone. Defaults to dict(type='LN')
         stage_cfgs (Sequence | dict): Extra config dict for each
             stage. Defaults to empty dict.
         patch_cfg (dict): Extra config dict for patch embedding.
             Defaults to empty dict.
         pad_small_map (bool): If True, pad the small feature map to the window
             size, which is common used in detection and segmentation. If False,
             avoid shifting window and shrink the window size to the size of
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/simsiam.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/simsiam.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/selfsup/swav.py` & `mmpretrain-1.0.1/mmpretrain/models/selfsup/swav.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/tta/score_tta.py` & `mmpretrain-1.0.1/mmpretrain/models/tta/score_tta.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/__init__.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -21,14 +21,17 @@
 from .make_divisible import make_divisible
 from .norm import GRN, LayerNorm2d, build_norm_layer
 from .position_encoding import (ConditionalPositionEncoding,
                                 PositionEncodingFourier, RotaryEmbeddingFast,
                                 build_2d_sincos_position_embedding)
 from .res_layer_extra_norm import ResLayerExtraNorm
 from .se_layer import SELayer
+from .sparse_modules import (SparseAvgPooling, SparseBatchNorm2d, SparseConv2d,
+                             SparseHelper, SparseLayerNorm2D, SparseMaxPooling,
+                             SparseSyncBatchNorm2d)
 from .swiglu_ffn import SwiGLUFFN, SwiGLUFFNFused
 from .vector_quantizer import NormEMAVectorQuantizer
 
 __all__ = [
     'channel_shuffle',
     'make_divisible',
     'InvertedResidual',
@@ -74,14 +77,21 @@
     'CosineEMA',
     'ResLayerExtraNorm',
     'MultiModalDataPreprocessor',
     'QuickGELU',
     'SwiGLUFFN',
     'SwiGLUFFNFused',
     'RotaryEmbeddingFast',
+    'SparseAvgPooling',
+    'SparseConv2d',
+    'SparseHelper',
+    'SparseMaxPooling',
+    'SparseBatchNorm2d',
+    'SparseLayerNorm2D',
+    'SparseSyncBatchNorm2d',
 ]
 
 if WITH_MULTIMODAL:
     from .huggingface import (no_load_hf_pretrained_model, register_hf_model,
                               register_hf_tokenizer)
     from .tokenizer import (Blip2Tokenizer, BlipTokenizer, FullTokenizer,
                             OFATokenizer)
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/attention.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/attention.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/cutmix.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/cutmix.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/mixup.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/mixup.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/resizemix.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/resizemix.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_augments/wrapper.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/batch_augments/wrapper.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/batch_shuffle.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/batch_shuffle.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/box_utils.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/box_utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/channel_shuffle.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/channel_shuffle.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/clip_generator_helper.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/clip_generator_helper.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/data_preprocessor.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/data_preprocessor.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/ema.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/ema.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/embed.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/embed.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/helpers.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/helpers.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/huggingface.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/huggingface.py`

 * *Files 4% similar despite different names*

```diff
@@ -82,15 +82,17 @@
                 '`pretrained_model_name_or_path` or `name_or_path`.')
         # `pretrained_model_name_or_path` is too long for config,
         # add an alias name `name_or_path` here.
         name_or_path = kwargs.pop('pretrained_model_name_or_path',
                                   kwargs.pop('name_or_path'))
 
         if kwargs.pop('load_pretrained', True) and _load_hf_pretrained_model:
-            return cls.from_pretrained(name_or_path, **kwargs)
+            model = cls.from_pretrained(name_or_path, **kwargs)
+            setattr(model, 'is_init', True)
+            return model
         else:
             cfg = get_config(name_or_path, **kwargs)
             return from_config(cfg)
 
     registry._register_module(module=build, module_name=cls.__name__)
     return cls
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/inverted_residual.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/inverted_residual.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/layer_scale.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/layer_scale.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/make_divisible.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/make_divisible.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/norm.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/norm.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/position_encoding.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/res_layer_extra_norm.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/res_layer_extra_norm.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/se_layer.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/se_layer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/swiglu_ffn.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/swiglu_ffn.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/tokenizer.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/tokenizer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/models/utils/vector_quantizer.py` & `mmpretrain-1.0.1/mmpretrain/models/utils/vector_quantizer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/registry.py` & `mmpretrain-1.0.1/mmpretrain/registry.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/structures/data_sample.py` & `mmpretrain-1.0.1/mmpretrain/structures/data_sample.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/structures/utils.py` & `mmpretrain-1.0.1/mmpretrain/structures/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/utils/analyze.py` & `mmpretrain-1.0.1/mmpretrain/utils/analyze.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/utils/collect_env.py` & `mmpretrain-1.0.1/mmpretrain/utils/collect_env.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/utils/dependency.py` & `mmpretrain-1.0.1/mmpretrain/utils/dependency.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/utils/progress.py` & `mmpretrain-1.0.1/mmpretrain/utils/progress.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/utils/setup_env.py` & `mmpretrain-1.0.1/mmpretrain/utils/setup_env.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/version.py` & `mmpretrain-1.0.1/mmpretrain/version.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved
 
-__version__ = '1.0.0rc8'
+__version__ = '1.0.1'
 
 
 def parse_version_info(version_str):
     """Parse a version string into a tuple.
 
     Args:
         version_str (str): The version string.
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/visualization/utils.py` & `mmpretrain-1.0.1/mmpretrain/visualization/utils.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain/visualization/visualizer.py` & `mmpretrain-1.0.1/mmpretrain/visualization/visualizer.py`

 * *Files identical despite different names*

### Comparing `mmpretrain-1.0.0rc8/mmpretrain.egg-info/PKG-INFO` & `mmpretrain-1.0.1/mmpretrain.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmpretrain
-Version: 1.0.0rc8
+Version: 1.0.1
 Summary: OpenMMLab Model Pretraining Toolbox and Benchmark
 Home-page: https://github.com/open-mmlab/mmpretrain
 Author: MMPretrain Contributors
 Author-email: openmmlab@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
         
@@ -90,36 +90,37 @@
           - Visual Grounding
           - Retrieval (Image-To-Image, Text-To-Image, Image-To-Text)
         
         https://github.com/open-mmlab/mmpretrain/assets/26739999/e4dcd3a2-f895-4d1b-a351-fbc74a04e904
         
         ## What's new
         
+         v1.0.1 was released in 28/07/2023
+        
+        Fix some bugs and enhance the codebase. Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details.
+        
+         v1.0.0 was released in 04/07/2023
+        
+        - Support inference of more **multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc.
+        - Support around **10 multi-modal** datasets!
+        - Add [**iTPN**](./configs/itpn/), [**SparK**](./configs/spark/) self-supervised learning algorithms.
+        - Provide examples of [New Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./configs/mae/benchmarks/). Here are the documentation links of [New Config](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner).
+        
          v1.0.0rc8 was released in 22/05/2023
         
         - Support multiple **multi-modal** algorithms and inferencers. You can explore these features by the [gradio demo](https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)!
         - Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones.
         - Register torchvision transforms into MMPretrain, you can now easily integrate torchvision's data augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/en/latest/api/data_process.html#torchvision-transforms)
         
-         v1.0.0rc7 was released in 07/04/2023
+        Update of previous versions
         
         - Integrated Self-supervised learning algorithms from **MMSelfSup**, such as **MAE**, **BEiT**, etc.
         - Support **RIFormer**, a simple but effective vision backbone by removing token mixer.
-        - Add t-SNE visualization.
         - Refactor dataset pipeline visualization.
-        
-        Update of previous versions
-        
         - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones.
-        - Reproduce the training accuracy of **ConvNeXt** and **RepVGG**.
-        - Support confusion matrix calculation and plot.
-        - Support **multi-task** training and testing.
-        - Support Test-time Augmentation.
-        - Upgrade API to get pre-defined models of MMPreTrain.
-        - Refactor BEiT backbone and support v1/v2 inference.
         
         This release introduced a brand new and flexible training & test engine, but it's still in progress. Welcome
         to try according to [the documentation](https://mmpretrain.readthedocs.io/en/latest/).
         
         And there are some BC-breaking changes. Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/latest/migration.html).
         
         Please refer to [changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and other release history.
@@ -228,14 +229,18 @@
                 <li><a href="configs/revvit">RevViT</a></li>
                 <li><a href="configs/convnext_v2">ConvNeXt V2</a></li>
                 <li><a href="configs/vig">ViG</a></li>
                 <li><a href="configs/xcit">XCiT</a></li>
                 <li><a href="configs/levit">LeViT</a></li>
                 <li><a href="configs/riformer">RIFormer</a></li>
                 <li><a href="configs/glip">GLIP</a></li>
+                <li><a href="configs/sam">ViT SAM</a></li>
+                <li><a href="configs/eva02">EVA02</a></li>
+                <li><a href="configs/dinov2">DINO V2</a></li>
+                <li><a href="configs/hivit">HiViT</a></li>
                 </ul>
               </td>
               <td>
                 <ul>
                 <li><a href="configs/mocov2">MoCo V1 (CVPR'2020)</a></li>
                 <li><a href="configs/simclr">SimCLR (ICML'2020)</a></li>
                 <li><a href="configs/mocov2">MoCo V2 (arXiv'2020)</a></li>
@@ -250,23 +255,28 @@
                 <li><a href="configs/simmim">SimMIM (CVPR'2022)</a></li>
                 <li><a href="configs/maskfeat">MaskFeat (CVPR'2022)</a></li>
                 <li><a href="configs/cae">CAE (arXiv'2022)</a></li>
                 <li><a href="configs/milan">MILAN (arXiv'2022)</a></li>
                 <li><a href="configs/beitv2">BEiT V2 (arXiv'2022)</a></li>
                 <li><a href="configs/eva">EVA (CVPR'2023)</a></li>
                 <li><a href="configs/mixmim">MixMIM (arXiv'2022)</a></li>
+                <li><a href="configs/itpn">iTPN (CVPR'2023)</a></li>
+                <li><a href="configs/spark">SparK (ICLR'2023)</a></li>
                 </ul>
               </td>
               <td>
                 <ul>
                 <li><a href="configs/blip">BLIP (arxiv'2022)</a></li>
                 <li><a href="configs/blip2">BLIP-2 (arxiv'2023)</a></li>
                 <li><a href="configs/ofa">OFA (CoRR'2022)</a></li>
                 <li><a href="configs/flamingo">Flamingo (NeurIPS'2022)</a></li>
                 <li><a href="configs/chinese_clip">Chinese CLIP (arxiv'2022)</a></li>
+                <li><a href="configs/minigpt4">MiniGPT-4 (arxiv'2023)</a></li>
+                <li><a href="configs/llava">LLaVA (arxiv'2023)</a></li>
+                <li><a href="configs/otter">Otter (arxiv'2023)</a></li>
                 </ul>
               </td>
               <td>
               Image Retrieval Task:
                 <ul>
                 <li><a href="configs/arcface">ArcFace (CVPR'2019)</a></li>
                 </ul>
```

#### html2text {}

```diff
@@ -1,11 +1,11 @@
-Metadata-Version: 2.1 Name: mmpretrain Version: 1.0.0rc8 Summary: OpenMMLab
-Model Pretraining Toolbox and Benchmark Home-page: https://github.com/open-
-mmlab/mmpretrain Author: MMPretrain Contributors Author-email:
-openmmlab@gmail.com License: Apache License 2.0 Description:
+Metadata-Version: 2.1 Name: mmpretrain Version: 1.0.1 Summary: OpenMMLab Model
+Pretraining Toolbox and Benchmark Home-page: https://github.com/open-mmlab/
+mmpretrain Author: MMPretrain Contributors Author-email: openmmlab@gmail.com
+License: Apache License 2.0 Description:
                            [resources/mmpt-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
  [![PyPI](https://img.shields.io/pypi/v/mmpretrain)](https://pypi.org/project/
  mmpretrain) [![Docs](https://img.shields.io/badge/docs-latest-blue)](https://
 mmpretrain.readthedocs.io/en/latest/) [![Build Status](https://github.com/open-
@@ -33,50 +33,58 @@
 backbones and pretrained models - Rich training strategies (supervised
 learning, self-supervised learning, multi-modality learning etc.) - Bag of
 training tricks - Large-scale training configs - High efficiency and
 extensibility - Powerful toolkits for model analysis and experiments - Various
 out-of-box inference tasks. - Image Classification - Image Caption - Visual
 Question Answering - Visual Grounding - Retrieval (Image-To-Image, Text-To-
 Image, Image-To-Text) https://github.com/open-mmlab/mmpretrain/assets/26739999/
-e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.0rc8 was released
-in 22/05/2023 - Support multiple **multi-modal** algorithms and inferencers.
-You can explore these features by the [gradio demo](https://github.com/open-
-mmlab/mmpretrain/tree/main/projects/gradio_demo)! - Add EVA-02, Dino-V2, ViT-
-SAM and GLIP backbones. - Register torchvision transforms into MMPretrain, you
-can now easily integrate torchvision's data augmentations in MMPretrain. See
-[the doc](https://mmpretrain.readthedocs.io/en/latest/api/
-data_process.html#torchvision-transforms)  v1.0.0rc7 was released in 07/04/
-2023 - Integrated Self-supervised learning algorithms from **MMSelfSup**, such
-as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
-vision backbone by removing token mixer. - Add t-SNE visualization. - Refactor
-dataset pipeline visualization. Update of previous versions - Support
-**LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**, **EVA**, **RevViT**,
-**EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM** backbones. - Reproduce
-the training accuracy of **ConvNeXt** and **RepVGG**. - Support confusion
-matrix calculation and plot. - Support **multi-task** training and testing. -
-Support Test-time Augmentation. - Upgrade API to get pre-defined models of
-MMPreTrain. - Refactor BEiT backbone and support v1/v2 inference. This release
-introduced a brand new and flexible training & test engine, but it's still in
-progress. Welcome to try according to [the documentation](https://
-mmpretrain.readthedocs.io/en/latest/). And there are some BC-breaking changes.
-Please check [the migration tutorial](https://mmpretrain.readthedocs.io/en/
-latest/migration.html). Please refer to [changelog](https://
-mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more details and
-other release history. ## Installation Below are quick steps for installation:
-```shell conda create -n open-mmlab python=3.8 pytorch==1.10.1
-torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda activate open-mmlab
-pip install openmim git clone https://github.com/open-mmlab/mmpretrain.git cd
-mmpretrain mim install -e . ``` Please refer to [installation documentation]
-(https://mmpretrain.readthedocs.io/en/latest/get_started.html) for more
-detailed installation and dataset preparation. For multi-modality models
-support, please install the extra dependencies by: ```shell mim install -e ".
-[multimodal]" ``` ## User Guides We provided a series of tutorials about the
-basic usage of MMPreTrain for new users: - [Learn about Configs](https://
-mmpretrain.readthedocs.io/en/latest/user_guides/config.html) - [Prepare
-Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
+e4dcd3a2-f895-4d1b-a351-fbc74a04e904 ## What's new  v1.0.1 was released in
+28/07/2023 Fix some bugs and enhance the codebase. Please refer to [changelog]
+(https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html) for more
+details.  v1.0.0 was released in 04/07/2023 - Support inference of more
+**multi-modal** algorithms, such as [**LLaVA**](./configs/llava/), [**MiniGPT-
+4**](./configs/minigpt4), [**Otter**](./configs/otter/), etc. - Support around
+**10 multi-modal** datasets! - Add [**iTPN**](./configs/itpn/), [**SparK**](./
+configs/spark/) self-supervised learning algorithms. - Provide examples of [New
+Config](./mmpretrain/configs/) and [DeepSpeed/FSDP with FlexibleRunner](./
+configs/mae/benchmarks/). Here are the documentation links of [New Config]
+(https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-
+pure-python-style-configuration-file-beta) and [DeepSpeed/FSDP with
+FlexibleRunner](https://mmengine.readthedocs.io/en/latest/api/generated/
+mmengine.runner.FlexibleRunner.html#mmengine.runner.FlexibleRunner). 
+v1.0.0rc8 was released in 22/05/2023 - Support multiple **multi-modal**
+algorithms and inferencers. You can explore these features by the [gradio demo]
+(https://github.com/open-mmlab/mmpretrain/tree/main/projects/gradio_demo)! -
+Add EVA-02, Dino-V2, ViT-SAM and GLIP backbones. - Register torchvision
+transforms into MMPretrain, you can now easily integrate torchvision's data
+augmentations in MMPretrain. See [the doc](https://mmpretrain.readthedocs.io/
+en/latest/api/data_process.html#torchvision-transforms) Update of previous
+versions - Integrated Self-supervised learning algorithms from **MMSelfSup**,
+such as **MAE**, **BEiT**, etc. - Support **RIFormer**, a simple but effective
+vision backbone by removing token mixer. - Refactor dataset pipeline
+visualization. - Support **LeViT**, **XCiT**, **ViG**, **ConvNeXt-V2**,
+**EVA**, **RevViT**, **EfficientnetV2**, **CLIP**, **TinyViT** and **MixMIM**
+backbones. This release introduced a brand new and flexible training & test
+engine, but it's still in progress. Welcome to try according to [the
+documentation](https://mmpretrain.readthedocs.io/en/latest/). And there are
+some BC-breaking changes. Please check [the migration tutorial](https://
+mmpretrain.readthedocs.io/en/latest/migration.html). Please refer to
+[changelog](https://mmpretrain.readthedocs.io/en/latest/notes/changelog.html)
+for more details and other release history. ## Installation Below are quick
+steps for installation: ```shell conda create -n open-mmlab python=3.8
+pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y conda
+activate open-mmlab pip install openmim git clone https://github.com/open-
+mmlab/mmpretrain.git cd mmpretrain mim install -e . ``` Please refer to
+[installation documentation](https://mmpretrain.readthedocs.io/en/latest/
+get_started.html) for more detailed installation and dataset preparation. For
+multi-modality models support, please install the extra dependencies by:
+```shell mim install -e ".[multimodal]" ``` ## User Guides We provided a series
+of tutorials about the basic usage of MMPreTrain for new users: - [Learn about
+Configs](https://mmpretrain.readthedocs.io/en/latest/user_guides/config.html) -
+[Prepare Dataset](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 dataset_prepare.html) - [Inference with existing models](https://
 mmpretrain.readthedocs.io/en/latest/user_guides/inference.html) - [Train]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/train.html) - [Test]
 (https://mmpretrain.readthedocs.io/en/latest/user_guides/test.html) -
 [Downstream tasks](https://mmpretrain.readthedocs.io/en/latest/user_guides/
 downstream.html) For more information, please refer to [our documentation]
 (https://mmpretrain.readthedocs.io/en/latest/). ## Model zoo Results and models
@@ -91,20 +99,20 @@
     * SE-ResNet             (ICML'2020)          (arxiv'2023)   Training&Test Tips:
     * SE-ResNeXt          * MoCo_V2_           * OFA_               * RandAug
     * RegNet                (arXiv'2020)         (CoRR'2022)        * AutoAug
     * ShuffleNet_V1       * BYOL_              * Flamingo_          * RepeatAugSampler
     * ShuffleNet_V2         (NeurIPS'2020)       (NeurIPS'2022)     * TTA
     * MobileNet_V2        * SwAV_              * Chinese_CLIP_      * ...
     * MobileNet_V3          (NeurIPS'2020)       (arxiv'2022)
-    * Swin-               * DenseCL_
-      Transformer           (CVPR'2021)
-    * Swin-               * SimSiam_
-      Transformer_V2        (CVPR'2021)
-    * RepVGG              * Barlow_Twins_
-    * Vision-               (ICML'2021)
+    * Swin-               * DenseCL_           * MiniGPT-4_
+      Transformer           (CVPR'2021)          (arxiv'2023)
+    * Swin-               * SimSiam_           * LLaVA_
+      Transformer_V2        (CVPR'2021)          (arxiv'2023)
+    * RepVGG              * Barlow_Twins_      * Otter_
+    * Vision-               (ICML'2021)          (arxiv'2023)
       Transformer         * MoCo_V3_
     * Transformer-in-       (ICCV'2021)
       Transformer         * BEiT_
     * Res2Net               (ICLR'2022)
     * MLP-Mixer           * MAE_
     * DeiT                  (CVPR'2022)
     * DeiT-3              * SimMIM_
@@ -117,30 +125,34 @@
     * HRNet                 (arXiv'2022)
     * VAN                 * BEiT_V2_
     * ConvMixer             (arXiv'2022)
     * CSPNet              * EVA_
     * PoolFormer            (CVPR'2023)
     * Inception_V3        * MixMIM_
     * MobileOne             (arXiv'2022)
-    * EfficientFormer
-    * MViT
-    * HorNet
-    * MobileViT
+    * EfficientFormer     * iTPN_
+    * MViT                  (CVPR'2023)
+    * HorNet              * SparK_
+    * MobileViT             (ICLR'2023)
     * DaViT
     * RepLKNet
     * BEiT
     * MixMIM
     * EfficientNet_V2
     * RevViT
     * ConvNeXt_V2
     * ViG
     * XCiT
     * LeViT
     * RIFormer
     * GLIP
+    * ViT_SAM
+    * EVA02
+    * DINO_V2
+    * HiViT
 ## Contributing We appreciate all contributions to improve MMPreTrain. Please
 refer to [CONTRUBUTING](https://mmpretrain.readthedocs.io/en/latest/notes/
 contribution_guide.html) for the contributing guideline. ## Acknowledgement
 MMPreTrain is an open source project that is contributed by researchers and
 engineers from various colleges and companies. We appreciate all the
 contributors who implement their methods or add new features, as well as users
 who give valuable feedbacks. We wish that the toolbox and benchmark could serve
```

### Comparing `mmpretrain-1.0.0rc8/mmpretrain.egg-info/SOURCES.txt` & `mmpretrain-1.0.1/mmpretrain.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -7,24 +7,29 @@
 mmpretrain/version.py
 mmpretrain.egg-info/PKG-INFO
 mmpretrain.egg-info/SOURCES.txt
 mmpretrain.egg-info/dependency_links.txt
 mmpretrain.egg-info/not-zip-safe
 mmpretrain.egg-info/requires.txt
 mmpretrain.egg-info/top_level.txt
+mmpretrain/.mim/dataset-index.yml
 mmpretrain/.mim/model-index.yml
 mmpretrain/.mim/configs/_base_/default_runtime.py
 mmpretrain/.mim/configs/_base_/datasets/cifar100_bs16.py
 mmpretrain/.mim/configs/_base_/datasets/cifar10_bs16.py
 mmpretrain/.mim/configs/_base_/datasets/coco_caption.py
+mmpretrain/.mim/configs/_base_/datasets/coco_okvqa.py
 mmpretrain/.mim/configs/_base_/datasets/coco_retrieval.py
 mmpretrain/.mim/configs/_base_/datasets/coco_vg_vqa.py
 mmpretrain/.mim/configs/_base_/datasets/coco_vqa.py
 mmpretrain/.mim/configs/_base_/datasets/cub_bs8_384.py
 mmpretrain/.mim/configs/_base_/datasets/cub_bs8_448.py
+mmpretrain/.mim/configs/_base_/datasets/flickr30k_caption.py
+mmpretrain/.mim/configs/_base_/datasets/flickr30k_retrieval.py
+mmpretrain/.mim/configs/_base_/datasets/gqa.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet21k_bs128.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_mbv3.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_medium_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_poolformer_small_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_revvit_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_medium_384.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs128_riformer_small_384.py
@@ -32,14 +37,15 @@
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_196.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_336.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_448.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_eva_560.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs16_pil_bicubic_384.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_beitv2.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_davit_224.py
+mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_itpn.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_levit_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a12.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_rsb_a3.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_simmim_192.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs256_swin_192.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs32_byol.py
@@ -54,26 +60,31 @@
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_384.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_clip_448.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_convmixer_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_deit3_384.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_edgenext_256.py
+mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_hivit_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_mixer_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_pil_resize_autoaug.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_256.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_swin_384.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs64_t2t_224.py
 mmpretrain/.mim/configs/_base_/datasets/imagenet_bs8_pil_bicubic_320.py
 mmpretrain/.mim/configs/_base_/datasets/inshop_bs32_448.py
 mmpretrain/.mim/configs/_base_/datasets/nlvr2.py
+mmpretrain/.mim/configs/_base_/datasets/nocaps.py
+mmpretrain/.mim/configs/_base_/datasets/ocrvqa.py
 mmpretrain/.mim/configs/_base_/datasets/refcoco.py
+mmpretrain/.mim/configs/_base_/datasets/vizwiz.py
 mmpretrain/.mim/configs/_base_/datasets/voc_bs16.py
+mmpretrain/.mim/configs/_base_/datasets/vsr.py
 mmpretrain/.mim/configs/_base_/datasets/pipelines/auto_aug.py
 mmpretrain/.mim/configs/_base_/datasets/pipelines/rand_aug.py
 mmpretrain/.mim/configs/_base_/models/efficientformer-l1.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b0.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b1.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b2.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b3.py
@@ -82,15 +93,17 @@
 mmpretrain/.mim/configs/_base_/models/efficientnet_b6.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b7.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_b8.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_em.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_es.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_l2.py
 mmpretrain/.mim/configs/_base_/models/inception_v3.py
+mmpretrain/.mim/configs/_base_/models/itpn_hivit-base-p16.py
 mmpretrain/.mim/configs/_base_/models/levit-256-p16.py
+mmpretrain/.mim/configs/_base_/models/mae_hivit-base-p16.py
 mmpretrain/.mim/configs/_base_/models/mae_vit-base-p16.py
 mmpretrain/.mim/configs/_base_/models/mlp_mixer_base_patch16.py
 mmpretrain/.mim/configs/_base_/models/mlp_mixer_large_patch16.py
 mmpretrain/.mim/configs/_base_/models/mobilenet_v2_1x.py
 mmpretrain/.mim/configs/_base_/models/replknet-31B_in1k.py
 mmpretrain/.mim/configs/_base_/models/replknet-31L_in1k.py
 mmpretrain/.mim/configs/_base_/models/replknet-XL_in1k.py
@@ -201,14 +214,17 @@
 mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_b3.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_l.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_m.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_s.py
 mmpretrain/.mim/configs/_base_/models/efficientnet_v2/efficientnetv2_xl.py
 mmpretrain/.mim/configs/_base_/models/eva/eva-g.py
 mmpretrain/.mim/configs/_base_/models/eva/eva-l.py
+mmpretrain/.mim/configs/_base_/models/hivit/base_224.py
+mmpretrain/.mim/configs/_base_/models/hivit/small_224.py
+mmpretrain/.mim/configs/_base_/models/hivit/tiny_224.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-base-gf.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-base.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-large-gf384.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-large.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-small-gf.py
 mmpretrain/.mim/configs/_base_/models/hornet/hornet-small.py
@@ -279,14 +295,15 @@
 mmpretrain/.mim/configs/_base_/models/vig/pyramid_vig_tiny.py
 mmpretrain/.mim/configs/_base_/models/vig/vig_base.py
 mmpretrain/.mim/configs/_base_/models/vig/vig_small.py
 mmpretrain/.mim/configs/_base_/models/vig/vig_tiny.py
 mmpretrain/.mim/configs/_base_/schedules/cifar10_bs128.py
 mmpretrain/.mim/configs/_base_/schedules/cub_bs64.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_conformer.py
+mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_hivit.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_revvit.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_adamw_swin.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_coslr.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs1024_linearlr_bn_nowd.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_AdamW.py
 mmpretrain/.mim/configs/_base_/schedules/imagenet_bs2048_adamw_levit.py
@@ -317,18 +334,24 @@
 mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-1600e_in1k.py
 mmpretrain/.mim/configs/beitv2/beitv2_beit-base-p16_8xb256-amp-coslr-300e_in1k.py
 mmpretrain/.mim/configs/beitv2/metafile.yml
 mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb128-coslr-100e_in1k.py
 mmpretrain/.mim/configs/beitv2/benchmarks/beit-base-p16_8xb64_in1k.py
 mmpretrain/.mim/configs/blip/blip-base_8xb16_refcoco.py
 mmpretrain/.mim/configs/blip/blip-base_8xb32_caption.py
+mmpretrain/.mim/configs/blip/blip-base_8xb32_caption_flickr30k.py
 mmpretrain/.mim/configs/blip/blip-base_8xb32_nlvr.py
+mmpretrain/.mim/configs/blip/blip-base_8xb32_nocaps.py
+mmpretrain/.mim/configs/blip/blip-base_8xb32_ocrvqa.py
+mmpretrain/.mim/configs/blip/blip-base_8xb32_okvqa.py
 mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval.py
+mmpretrain/.mim/configs/blip/blip-base_8xb32_retrieval_flickr30k.py
 mmpretrain/.mim/configs/blip/blip-base_8xb32_vqa.py
 mmpretrain/.mim/configs/blip/metafile.yml
+mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_gqa.py
 mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb16_vqa.py
 mmpretrain/.mim/configs/blip2/blip2-opt2.7b_8xb32_caption.py
 mmpretrain/.mim/configs/blip2/blip2_8xb32_retrieval.py
 mmpretrain/.mim/configs/blip2/metafile.yml
 mmpretrain/.mim/configs/byol/byol_resnet50_16xb256-coslr-200e_in1k.py
 mmpretrain/.mim/configs/byol/metafile.yml
 mmpretrain/.mim/configs/byol/benchmarks/mask-rcnn_r50-c4_ms-1x_coco.py
@@ -500,14 +523,18 @@
 mmpretrain/.mim/configs/flamingo/flamingo_fewshot_vqa.py
 mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_caption.py
 mmpretrain/.mim/configs/flamingo/flamingo_zeroshot_vqa.py
 mmpretrain/.mim/configs/flamingo/metafile.yml
 mmpretrain/.mim/configs/glip/glip-l_headless.py
 mmpretrain/.mim/configs/glip/glip-t_headless.py
 mmpretrain/.mim/configs/glip/metafile.yml
+mmpretrain/.mim/configs/hivit/hivit-base-p16_16xb64_in1k.py
+mmpretrain/.mim/configs/hivit/hivit-small-p16_16xb64_in1k.py
+mmpretrain/.mim/configs/hivit/hivit-tiny-p16_16xb64_in1k.py
+mmpretrain/.mim/configs/hivit/metafile.yml
 mmpretrain/.mim/configs/hornet/hornet-base-gf_8xb64_in1k.py
 mmpretrain/.mim/configs/hornet/hornet-base_8xb64_in1k.py
 mmpretrain/.mim/configs/hornet/hornet-small-gf_8xb64_in1k.py
 mmpretrain/.mim/configs/hornet/hornet-small_8xb64_in1k.py
 mmpretrain/.mim/configs/hornet/hornet-tiny-gf_8xb128_in1k.py
 mmpretrain/.mim/configs/hornet/hornet-tiny_8xb128_in1k.py
 mmpretrain/.mim/configs/hornet/metafile.yml
@@ -517,49 +544,72 @@
 mmpretrain/.mim/configs/hrnet/hrnet-w40_4xb32_in1k.py
 mmpretrain/.mim/configs/hrnet/hrnet-w44_4xb32_in1k.py
 mmpretrain/.mim/configs/hrnet/hrnet-w48_4xb32_in1k.py
 mmpretrain/.mim/configs/hrnet/hrnet-w64_4xb32_in1k.py
 mmpretrain/.mim/configs/hrnet/metafile.yml
 mmpretrain/.mim/configs/inception_v3/inception-v3_8xb32_in1k.py
 mmpretrain/.mim/configs/inception_v3/metafile.yml
+mmpretrain/.mim/configs/itpn/itpn-clip-b_hivit-base-p16_8xb256-amp-coslr-300e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-clip-b_hivit-base-p16_8xb256-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-1600e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-400e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-base-p16_8xb512-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-1600e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-400e_in1k.py
+mmpretrain/.mim/configs/itpn/itpn-pixel_hivit-large-p16_8xb512-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/itpn/metafile.yml
 mmpretrain/.mim/configs/lenet/lenet5_mnist.py
 mmpretrain/.mim/configs/levit/levit-128_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/levit-128s_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/levit-192_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/levit-256_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/levit-384_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/metafile.yml
 mmpretrain/.mim/configs/levit/deploy/levit-128_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/deploy/levit-128s_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/deploy/levit-192_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/deploy/levit-256_8xb256_in1k.py
 mmpretrain/.mim/configs/levit/deploy/levit-384_8xb256_in1k.py
+mmpretrain/.mim/configs/llava/llava-7b-v1_caption.py
+mmpretrain/.mim/configs/llava/metafile.yml
+mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-1600e_in1k.py
+mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-400e_in1k.py
+mmpretrain/.mim/configs/mae/mae_hivit-base-p16_8xb512-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-1600e_in1k.py
+mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-400e_in1k.py
+mmpretrain/.mim/configs/mae/mae_hivit-large-p16_8xb512-amp-coslr-800e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-1600e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-300e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-400e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-base-p16_8xb512-amp-coslr-800e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-huge-p14_8xb512-amp-coslr-1600e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-1600e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-300e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-400e_in1k.py
 mmpretrain/.mim/configs/mae/mae_vit-large-p16_8xb512-amp-coslr-800e_in1k.py
 mmpretrain/.mim/configs/mae/metafile.yml
 mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
 mmpretrain/.mim/configs/mae/benchmarks/vit-base-p16_8xb2048-linear-coslr-90e_in1k.py
 mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_32xb8-coslr-50e_in1k-448px.py
 mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-coslr-50e_in1k.py
+mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-ds-coslr-50e_in1k.py
+mmpretrain/.mim/configs/mae/benchmarks/vit-huge-p14_8xb128-fsdp-coslr-50e_in1k.py
 mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-coslr-50e_in1k.py
+mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-ds-coslr-50e_in1k.py
+mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb128-fsdp-coslr-50e_in1k.py
 mmpretrain/.mim/configs/mae/benchmarks/vit-large-p16_8xb2048-linear-coslr-90e_in1k.py
 mmpretrain/.mim/configs/maskfeat/maskfeat_vit-base-p16_8xb256-amp-coslr-300e_in1k.py
 mmpretrain/.mim/configs/maskfeat/metafile.yml
 mmpretrain/.mim/configs/maskfeat/benchmarks/vit-base-p16_8xb256-coslr-100e_in1k.py
 mmpretrain/.mim/configs/milan/metafile.yml
 mmpretrain/.mim/configs/milan/milan_vit-base-p16_16xb256-amp-coslr-400e_in1k.py
 mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb128-coslr-100e_in1k.py
 mmpretrain/.mim/configs/milan/benchmarks/vit-base-p16_8xb2048-linear-coslr-100e_in1k.py
+mmpretrain/.mim/configs/minigpt4/metafile.yml
+mmpretrain/.mim/configs/minigpt4/minigpt-4_vicuna-7b_caption.py
 mmpretrain/.mim/configs/mixmim/metafile.yml
 mmpretrain/.mim/configs/mixmim/mixmim_mixmim-base_16xb128-coslr-300e_in1k.py
 mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb128-coslr-100e_in1k.py
 mmpretrain/.mim/configs/mixmim/benchmarks/mixmim-base_8xb64_in1k.py
 mmpretrain/.mim/configs/mlp_mixer/metafile.yml
 mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-base-p16_64xb64_in1k.py
 mmpretrain/.mim/configs/mlp_mixer/mlp-mixer-large-p16_64xb64_in1k.py
@@ -608,14 +658,17 @@
 mmpretrain/.mim/configs/mvit/mvitv2-tiny_8xb256_in1k.py
 mmpretrain/.mim/configs/ofa/metafile.yml
 mmpretrain/.mim/configs/ofa/ofa-base_finetuned_caption.py
 mmpretrain/.mim/configs/ofa/ofa-base_finetuned_refcoco.py
 mmpretrain/.mim/configs/ofa/ofa-base_finetuned_vqa.py
 mmpretrain/.mim/configs/ofa/ofa-base_zeroshot_vqa.py
 mmpretrain/.mim/configs/ofa/ofa-large_zeroshot_vqa.py
+mmpretrain/.mim/configs/otter/metafile.yml
+mmpretrain/.mim/configs/otter/otter-9b_caption.py
+mmpretrain/.mim/configs/otter/otter-9b_vqa.py
 mmpretrain/.mim/configs/poolformer/metafile.yml
 mmpretrain/.mim/configs/poolformer/poolformer-m36_32xb128_in1k.py
 mmpretrain/.mim/configs/poolformer/poolformer-m48_32xb128_in1k.py
 mmpretrain/.mim/configs/poolformer/poolformer-s12_32xb128_in1k.py
 mmpretrain/.mim/configs/poolformer/poolformer-s24_32xb128_in1k.py
 mmpretrain/.mim/configs/poolformer/poolformer-s36_32xb128_in1k.py
 mmpretrain/.mim/configs/regnet/metafile.yml
@@ -753,14 +806,21 @@
 mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w6_8xb256-coslr-100e_in1k-192px.py
 mmpretrain/.mim/configs/simmim/benchmarks/swin-base-w7_8xb256-coslr-100e_in1k.py
 mmpretrain/.mim/configs/simmim/benchmarks/swin-large-w14_8xb256-coslr-100e_in1k.py
 mmpretrain/.mim/configs/simsiam/metafile.yml
 mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-100e_in1k.py
 mmpretrain/.mim/configs/simsiam/simsiam_resnet50_8xb32-coslr-200e_in1k.py
 mmpretrain/.mim/configs/simsiam/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
+mmpretrain/.mim/configs/spark/metafile.yml
+mmpretrain/.mim/configs/spark/spark_sparse-convnext-small_16xb256-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/spark/spark_sparse-convnextv2-tiny_16xb256-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/spark/spark_sparse-resnet50_8xb512-amp-coslr-1600e_in1k.py
+mmpretrain/.mim/configs/spark/spark_sparse-resnet50_8xb512-amp-coslr-800e_in1k.py
+mmpretrain/.mim/configs/spark/benchmarks/convnextv2-tiny_8xb256-coslr-300e_in1k.py
+mmpretrain/.mim/configs/spark/benchmarks/resnet50_8xb256-coslr-300e_in1k.py
 mmpretrain/.mim/configs/swav/metafile.yml
 mmpretrain/.mim/configs/swav/swav_resnet50_8xb32-mcrop-coslr-200e_in1k-224px-96px.py
 mmpretrain/.mim/configs/swav/benchmarks/resnet50_8xb512-linear-coslr-90e_in1k.py
 mmpretrain/.mim/configs/swin_transformer/metafile.yml
 mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k-384px.py
 mmpretrain/.mim/configs/swin_transformer/swin-base_16xb64_in1k.py
 mmpretrain/.mim/configs/swin_transformer/swin-large_16xb64_in1k-384px.py
@@ -827,14 +887,15 @@
 mmpretrain/.mim/configs/vig/vig-small_8xb128_in1k.py
 mmpretrain/.mim/configs/vig/vig-tiny_8xb128_in1k.py
 mmpretrain/.mim/configs/vision_transformer/metafile.yml
 mmpretrain/.mim/configs/vision_transformer/vit-base-p16_32xb128-mae_in1k.py
 mmpretrain/.mim/configs/vision_transformer/vit-base-p16_4xb544-ipu_in1k.py
 mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k-384px.py
 mmpretrain/.mim/configs/vision_transformer/vit-base-p16_64xb64_in1k.py
+mmpretrain/.mim/configs/vision_transformer/vit-base-p16_8xb64-lora_in1k-384px.py
 mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k-384px.py
 mmpretrain/.mim/configs/vision_transformer/vit-base-p32_64xb64_in1k.py
 mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k-384px.py
 mmpretrain/.mim/configs/vision_transformer/vit-large-p16_64xb64_in1k.py
 mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k-384px.py
 mmpretrain/.mim/configs/vision_transformer/vit-large-p32_64xb64_in1k.py
 mmpretrain/.mim/configs/wrn/metafile.yml
@@ -878,44 +939,52 @@
 mmpretrain/.mim/tools/test.py
 mmpretrain/.mim/tools/train.py
 mmpretrain/.mim/tools/analysis_tools/analyze_logs.py
 mmpretrain/.mim/tools/analysis_tools/analyze_results.py
 mmpretrain/.mim/tools/analysis_tools/confusion_matrix.py
 mmpretrain/.mim/tools/analysis_tools/eval_metric.py
 mmpretrain/.mim/tools/analysis_tools/get_flops.py
+mmpretrain/.mim/tools/analysis_tools/shape_bias.py
+mmpretrain/.mim/tools/analysis_tools/utils.py
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_test.sh
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_c4.sh
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_dist_train_fpn.sh
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_test.sh
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_c4.sh
 mmpretrain/.mim/tools/benchmarks/mmdetection/mim_slurm_train_fpn.sh
 mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_test.sh
 mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_dist_train.sh
 mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_test.sh
 mmpretrain/.mim/tools/benchmarks/mmsegmentation/mim_slurm_train.sh
+mmpretrain/.mim/tools/dataset_converters/convert_flickr30k_ann.py
 mmpretrain/.mim/tools/dataset_converters/convert_imagenet_subsets.py
 mmpretrain/.mim/tools/dataset_converters/convert_inaturalist.py
+mmpretrain/.mim/tools/dataset_converters/odl_cub_preprocess.sh
+mmpretrain/.mim/tools/dataset_converters/odl_imagenet1k_preprocess.sh
 mmpretrain/.mim/tools/misc/print_config.py
 mmpretrain/.mim/tools/misc/verify_dataset.py
 mmpretrain/.mim/tools/model_converters/clip_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/convnext_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/davit_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/deit3_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/edgenext_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/efficientnet_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/efficientnetv2_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/eva02_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/eva_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/glip_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/hornet2mmpretrain.py
 mmpretrain/.mim/tools/model_converters/levit2mmpretrain.py
+mmpretrain/.mim/tools/model_converters/llava-delta2mmpre.py
+mmpretrain/.mim/tools/model_converters/merge_lora_weight.py
 mmpretrain/.mim/tools/model_converters/mixmim_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/mlpmixer_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/mobilenetv2_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/ofa.py
+mmpretrain/.mim/tools/model_converters/otter2mmpre.py
 mmpretrain/.mim/tools/model_converters/publish_model.py
 mmpretrain/.mim/tools/model_converters/reparameterize_model.py
 mmpretrain/.mim/tools/model_converters/replknet_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/repvgg_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/revvit_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/shufflenetv2_to_mmpretrain.py
 mmpretrain/.mim/tools/model_converters/tinyvit_to_mmpretrain.py
@@ -954,39 +1023,48 @@
 mmpretrain/datasets/coco_vqa.py
 mmpretrain/datasets/cub.py
 mmpretrain/datasets/custom.py
 mmpretrain/datasets/dataset_wrappers.py
 mmpretrain/datasets/dtd.py
 mmpretrain/datasets/fgvcaircraft.py
 mmpretrain/datasets/flamingo.py
+mmpretrain/datasets/flickr30k_caption.py
+mmpretrain/datasets/flickr30k_retrieval.py
 mmpretrain/datasets/flowers102.py
 mmpretrain/datasets/food101.py
+mmpretrain/datasets/gqa_dataset.py
 mmpretrain/datasets/imagenet.py
 mmpretrain/datasets/inshop.py
 mmpretrain/datasets/mnist.py
 mmpretrain/datasets/multi_label.py
 mmpretrain/datasets/multi_task.py
 mmpretrain/datasets/nlvr2.py
+mmpretrain/datasets/nocaps.py
+mmpretrain/datasets/ocr_vqa.py
 mmpretrain/datasets/oxfordiiitpet.py
 mmpretrain/datasets/places205.py
 mmpretrain/datasets/refcoco.py
 mmpretrain/datasets/scienceqa.py
 mmpretrain/datasets/stanfordcars.py
 mmpretrain/datasets/sun397.py
+mmpretrain/datasets/textvqa.py
 mmpretrain/datasets/utils.py
 mmpretrain/datasets/vg_vqa.py
 mmpretrain/datasets/visual_genome.py
+mmpretrain/datasets/vizwiz.py
 mmpretrain/datasets/voc.py
+mmpretrain/datasets/vsr.py
 mmpretrain/datasets/samplers/__init__.py
 mmpretrain/datasets/samplers/repeat_aug.py
 mmpretrain/datasets/samplers/sequential.py
 mmpretrain/datasets/transforms/__init__.py
 mmpretrain/datasets/transforms/auto_augment.py
 mmpretrain/datasets/transforms/formatting.py
 mmpretrain/datasets/transforms/processing.py
+mmpretrain/datasets/transforms/utils.py
 mmpretrain/datasets/transforms/wrappers.py
 mmpretrain/engine/__init__.py
 mmpretrain/engine/hooks/__init__.py
 mmpretrain/engine/hooks/class_num_check_hook.py
 mmpretrain/engine/hooks/densecl_hook.py
 mmpretrain/engine/hooks/ema_hook.py
 mmpretrain/engine/hooks/margin_head_hooks.py
@@ -1000,22 +1078,27 @@
 mmpretrain/engine/optimizers/__init__.py
 mmpretrain/engine/optimizers/adan_t.py
 mmpretrain/engine/optimizers/lamb.py
 mmpretrain/engine/optimizers/lars.py
 mmpretrain/engine/optimizers/layer_decay_optim_wrapper_constructor.py
 mmpretrain/engine/runners/__init__.py
 mmpretrain/engine/runners/retrieval_loop.py
+mmpretrain/engine/schedulers/__init__.py
+mmpretrain/engine/schedulers/weight_decay_scheduler.py
 mmpretrain/evaluation/__init__.py
 mmpretrain/evaluation/functional/__init__.py
 mmpretrain/evaluation/metrics/__init__.py
 mmpretrain/evaluation/metrics/caption.py
+mmpretrain/evaluation/metrics/gqa.py
 mmpretrain/evaluation/metrics/multi_label.py
 mmpretrain/evaluation/metrics/multi_task.py
+mmpretrain/evaluation/metrics/nocaps.py
 mmpretrain/evaluation/metrics/retrieval.py
 mmpretrain/evaluation/metrics/scienceqa.py
+mmpretrain/evaluation/metrics/shape_bias_label.py
 mmpretrain/evaluation/metrics/single_label.py
 mmpretrain/evaluation/metrics/visual_grounding_eval.py
 mmpretrain/evaluation/metrics/voc_multi_label.py
 mmpretrain/evaluation/metrics/vqa.py
 mmpretrain/models/__init__.py
 mmpretrain/models/builder.py
 mmpretrain/models/backbones/__init__.py
@@ -1030,14 +1113,15 @@
 mmpretrain/models/backbones/deit.py
 mmpretrain/models/backbones/deit3.py
 mmpretrain/models/backbones/densenet.py
 mmpretrain/models/backbones/edgenext.py
 mmpretrain/models/backbones/efficientformer.py
 mmpretrain/models/backbones/efficientnet.py
 mmpretrain/models/backbones/efficientnet_v2.py
+mmpretrain/models/backbones/hivit.py
 mmpretrain/models/backbones/hornet.py
 mmpretrain/models/backbones/hrnet.py
 mmpretrain/models/backbones/inception_v3.py
 mmpretrain/models/backbones/lenet.py
 mmpretrain/models/backbones/levit.py
 mmpretrain/models/backbones/mixmim.py
 mmpretrain/models/backbones/mlp_mixer.py
@@ -1058,14 +1142,16 @@
 mmpretrain/models/backbones/resnext.py
 mmpretrain/models/backbones/revvit.py
 mmpretrain/models/backbones/riformer.py
 mmpretrain/models/backbones/seresnet.py
 mmpretrain/models/backbones/seresnext.py
 mmpretrain/models/backbones/shufflenet_v1.py
 mmpretrain/models/backbones/shufflenet_v2.py
+mmpretrain/models/backbones/sparse_convnext.py
+mmpretrain/models/backbones/sparse_resnet.py
 mmpretrain/models/backbones/swin_transformer.py
 mmpretrain/models/backbones/swin_transformer_v2.py
 mmpretrain/models/backbones/t2t_vit.py
 mmpretrain/models/backbones/timm_backbone.py
 mmpretrain/models/backbones/tinyvit.py
 mmpretrain/models/backbones/tnt.py
 mmpretrain/models/backbones/twins.py
@@ -1089,28 +1175,30 @@
 mmpretrain/models/heads/conformer_head.py
 mmpretrain/models/heads/contrastive_head.py
 mmpretrain/models/heads/deit_head.py
 mmpretrain/models/heads/efficientformer_head.py
 mmpretrain/models/heads/grounding_head.py
 mmpretrain/models/heads/itc_head.py
 mmpretrain/models/heads/itm_head.py
+mmpretrain/models/heads/itpn_clip_head.py
 mmpretrain/models/heads/latent_heads.py
 mmpretrain/models/heads/levit_head.py
 mmpretrain/models/heads/linear_head.py
 mmpretrain/models/heads/mae_head.py
 mmpretrain/models/heads/margin_head.py
 mmpretrain/models/heads/mim_head.py
 mmpretrain/models/heads/mixmim_head.py
 mmpretrain/models/heads/mocov3_head.py
 mmpretrain/models/heads/multi_label_cls_head.py
 mmpretrain/models/heads/multi_label_csra_head.py
 mmpretrain/models/heads/multi_label_linear_head.py
 mmpretrain/models/heads/multi_task_head.py
 mmpretrain/models/heads/seq_gen_head.py
 mmpretrain/models/heads/simmim_head.py
+mmpretrain/models/heads/spark_head.py
 mmpretrain/models/heads/stacked_head.py
 mmpretrain/models/heads/swav_head.py
 mmpretrain/models/heads/vig_head.py
 mmpretrain/models/heads/vision_transformer_head.py
 mmpretrain/models/heads/vqa_head.py
 mmpretrain/models/losses/__init__.py
 mmpretrain/models/losses/asymmetric_loss.py
@@ -1143,52 +1231,65 @@
 mmpretrain/models/multimodal/chinese_clip/chinese_clip.py
 mmpretrain/models/multimodal/chinese_clip/utils.py
 mmpretrain/models/multimodal/flamingo/__init__.py
 mmpretrain/models/multimodal/flamingo/adapter.py
 mmpretrain/models/multimodal/flamingo/flamingo.py
 mmpretrain/models/multimodal/flamingo/modules.py
 mmpretrain/models/multimodal/flamingo/utils.py
+mmpretrain/models/multimodal/llava/__init__.py
+mmpretrain/models/multimodal/llava/llava.py
+mmpretrain/models/multimodal/llava/modules.py
+mmpretrain/models/multimodal/minigpt4/__init__.py
+mmpretrain/models/multimodal/minigpt4/minigpt4.py
 mmpretrain/models/multimodal/ofa/__init__.py
 mmpretrain/models/multimodal/ofa/ofa.py
 mmpretrain/models/multimodal/ofa/ofa_modules.py
+mmpretrain/models/multimodal/otter/__init__.py
+mmpretrain/models/multimodal/otter/otter.py
 mmpretrain/models/necks/__init__.py
 mmpretrain/models/necks/beitv2_neck.py
 mmpretrain/models/necks/cae_neck.py
 mmpretrain/models/necks/densecl_neck.py
 mmpretrain/models/necks/gap.py
 mmpretrain/models/necks/gem.py
 mmpretrain/models/necks/hr_fuse.py
+mmpretrain/models/necks/itpn_neck.py
 mmpretrain/models/necks/linear_neck.py
 mmpretrain/models/necks/mae_neck.py
 mmpretrain/models/necks/milan_neck.py
 mmpretrain/models/necks/mixmim_neck.py
 mmpretrain/models/necks/mocov2_neck.py
 mmpretrain/models/necks/nonlinear_neck.py
 mmpretrain/models/necks/simmim_neck.py
+mmpretrain/models/necks/spark_neck.py
 mmpretrain/models/necks/swav_neck.py
+mmpretrain/models/peft/__init__.py
+mmpretrain/models/peft/lora.py
 mmpretrain/models/retrievers/__init__.py
 mmpretrain/models/retrievers/base.py
 mmpretrain/models/retrievers/image2image.py
 mmpretrain/models/selfsup/__init__.py
 mmpretrain/models/selfsup/barlowtwins.py
 mmpretrain/models/selfsup/base.py
 mmpretrain/models/selfsup/beit.py
 mmpretrain/models/selfsup/byol.py
 mmpretrain/models/selfsup/cae.py
 mmpretrain/models/selfsup/densecl.py
 mmpretrain/models/selfsup/eva.py
+mmpretrain/models/selfsup/itpn.py
 mmpretrain/models/selfsup/mae.py
 mmpretrain/models/selfsup/maskfeat.py
 mmpretrain/models/selfsup/milan.py
 mmpretrain/models/selfsup/mixmim.py
 mmpretrain/models/selfsup/moco.py
 mmpretrain/models/selfsup/mocov3.py
 mmpretrain/models/selfsup/simclr.py
 mmpretrain/models/selfsup/simmim.py
 mmpretrain/models/selfsup/simsiam.py
+mmpretrain/models/selfsup/spark.py
 mmpretrain/models/selfsup/swav.py
 mmpretrain/models/tta/__init__.py
 mmpretrain/models/tta/score_tta.py
 mmpretrain/models/utils/__init__.py
 mmpretrain/models/utils/attention.py
 mmpretrain/models/utils/batch_shuffle.py
 mmpretrain/models/utils/box_utils.py
@@ -1202,14 +1303,15 @@
 mmpretrain/models/utils/inverted_residual.py
 mmpretrain/models/utils/layer_scale.py
 mmpretrain/models/utils/make_divisible.py
 mmpretrain/models/utils/norm.py
 mmpretrain/models/utils/position_encoding.py
 mmpretrain/models/utils/res_layer_extra_norm.py
 mmpretrain/models/utils/se_layer.py
+mmpretrain/models/utils/sparse_modules.py
 mmpretrain/models/utils/swiglu_ffn.py
 mmpretrain/models/utils/tokenizer.py
 mmpretrain/models/utils/vector_quantizer.py
 mmpretrain/models/utils/batch_augments/__init__.py
 mmpretrain/models/utils/batch_augments/cutmix.py
 mmpretrain/models/utils/batch_augments/mixup.py
 mmpretrain/models/utils/batch_augments/resizemix.py
```

### Comparing `mmpretrain-1.0.0rc8/setup.cfg` & `mmpretrain-1.0.1/setup.cfg`

 * *Files 16% similar despite different names*

```diff
@@ -22,12 +22,13 @@
 quiet-level = 3
 ignore-words-list = patten,confectionary,nd,ty,formating,dows
 
 [flake8]
 extend-ignore = E251
 per-file-ignores = 
 	*/__init__.py: F401
+	mmpretrain/configs/*: F401,F403,F405
 
 [egg_info]
 tag_build = 
 tag_date = 0
```

### Comparing `mmpretrain-1.0.0rc8/setup.py` & `mmpretrain-1.0.1/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -113,15 +113,15 @@
     elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:
         # installed by `pip install .`
         # or create source distribution by `python setup.py sdist`
         mode = 'copy'
     else:
         return
 
-    filenames = ['tools', 'configs', 'model-index.yml']
+    filenames = ['tools', 'configs', 'model-index.yml', 'dataset-index.yml']
     repo_path = osp.dirname(__file__)
     mim_path = osp.join(repo_path, 'mmpretrain', '.mim')
     os.makedirs(mim_path, exist_ok=True)
 
     for filename in filenames:
         if osp.exists(filename):
             src_path = osp.join(repo_path, filename)
```

