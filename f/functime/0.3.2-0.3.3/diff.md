# Comparing `tmp/functime-0.3.2-py3-none-any.whl.zip` & `tmp/functime-0.3.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 58146 bytes, number of entries: 42
--rw-r--r--  2.0 unx       66 b- defN 23-Jul-27 15:46 functime/__init__.py
--rw-r--r--  2.0 unx      117 b- defN 23-Jul-27 15:46 functime/__main__.py
--rw-r--r--  2.0 unx     5908 b- defN 23-Jul-27 15:46 functime/backtesting.py
--rw-r--r--  2.0 unx     1838 b- defN 23-Jul-27 15:46 functime/conformal.py
--rw-r--r--  2.0 unx     1379 b- defN 23-Jul-27 15:46 functime/conversion.py
--rw-r--r--  2.0 unx     5549 b- defN 23-Jul-27 15:46 functime/cross_validation.py
--rw-r--r--  2.0 unx      123 b- defN 23-Jul-27 15:46 functime/embeddings.py
--rw-r--r--  2.0 unx     1495 b- defN 23-Jul-27 15:46 functime/offsets.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-27 15:46 functime/plotting.py
--rw-r--r--  2.0 unx    20394 b- defN 23-Jul-27 15:46 functime/preprocessing.py
--rw-r--r--  2.0 unx     1976 b- defN 23-Jul-27 15:46 functime/ranges.py
--rw-r--r--  2.0 unx      217 b- defN 23-Jul-27 15:46 functime/base/__init__.py
--rw-r--r--  2.0 unx     8253 b- defN 23-Jul-27 15:46 functime/base/forecaster.py
--rw-r--r--  2.0 unx     1630 b- defN 23-Jul-27 15:46 functime/base/metric.py
--rw-r--r--  2.0 unx     2771 b- defN 23-Jul-27 15:46 functime/base/model.py
--rw-r--r--  2.0 unx     2155 b- defN 23-Jul-27 15:46 functime/base/transformer.py
--rw-r--r--  2.0 unx      285 b- defN 23-Jul-27 15:46 functime/feature_extraction/__init__.py
--rw-r--r--  2.0 unx     4294 b- defN 23-Jul-27 15:46 functime/feature_extraction/calendar.py
--rw-r--r--  2.0 unx      745 b- defN 23-Jul-27 15:46 functime/forecasting/__init__.py
--rw-r--r--  2.0 unx    12667 b- defN 23-Jul-27 15:46 functime/forecasting/_ar.py
--rw-r--r--  2.0 unx     4909 b- defN 23-Jul-27 15:46 functime/forecasting/_evaluate.py
--rw-r--r--  2.0 unx     2231 b- defN 23-Jul-27 15:46 functime/forecasting/_reduction.py
--rw-r--r--  2.0 unx     7569 b- defN 23-Jul-27 15:46 functime/forecasting/_regressors.py
--rw-r--r--  2.0 unx     8513 b- defN 23-Jul-27 15:46 functime/forecasting/automl.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Jul-27 15:46 functime/forecasting/catboost.py
--rw-r--r--  2.0 unx     3717 b- defN 23-Jul-27 15:46 functime/forecasting/censored.py
--rw-r--r--  2.0 unx     1010 b- defN 23-Jul-27 15:46 functime/forecasting/knn.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-27 15:46 functime/forecasting/lance.py
--rw-r--r--  2.0 unx     4208 b- defN 23-Jul-27 15:46 functime/forecasting/lightgbm.py
--rw-r--r--  2.0 unx     3958 b- defN 23-Jul-27 15:46 functime/forecasting/linear.py
--rw-r--r--  2.0 unx     1657 b- defN 23-Jul-27 15:46 functime/forecasting/naive.py
--rw-r--r--  2.0 unx     2412 b- defN 23-Jul-27 15:46 functime/forecasting/xgboost.py
--rw-r--r--  2.0 unx      312 b- defN 23-Jul-27 15:46 functime/metrics/__init__.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Jul-27 15:46 functime/metrics/multi_objective.py
--rw-r--r--  2.0 unx     7432 b- defN 23-Jul-27 15:46 functime/metrics/point.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-27 15:46 functime/metrics/probabilistic.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     7412 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       52 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3522 b- defN 23-Jul-27 15:47 functime-0.3.2.dist-info/RECORD
-42 files, 175104 bytes uncompressed, 52532 bytes compressed:  70.0%
+Zip file size: 62080 bytes, number of entries: 42
+-rw-r--r--  2.0 unx       66 b- defN 23-Jul-31 15:46 functime/__init__.py
+-rw-r--r--  2.0 unx     6143 b- defN 23-Jul-31 15:46 functime/backtesting.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Jul-31 15:46 functime/conformal.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-Jul-31 15:46 functime/conversion.py
+-rw-r--r--  2.0 unx     5549 b- defN 23-Jul-31 15:46 functime/cross_validation.py
+-rw-r--r--  2.0 unx      123 b- defN 23-Jul-31 15:46 functime/embeddings.py
+-rw-r--r--  2.0 unx     1495 b- defN 23-Jul-31 15:46 functime/offsets.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-31 15:46 functime/plotting.py
+-rw-r--r--  2.0 unx    20573 b- defN 23-Jul-31 15:46 functime/preprocessing.py
+-rw-r--r--  2.0 unx     1996 b- defN 23-Jul-31 15:46 functime/ranges.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-31 15:46 functime/base/__init__.py
+-rw-r--r--  2.0 unx     8265 b- defN 23-Jul-31 15:46 functime/base/forecaster.py
+-rw-r--r--  2.0 unx     1746 b- defN 23-Jul-31 15:46 functime/base/metric.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Jul-31 15:46 functime/base/model.py
+-rw-r--r--  2.0 unx     2155 b- defN 23-Jul-31 15:46 functime/base/transformer.py
+-rw-r--r--  2.0 unx      349 b- defN 23-Jul-31 15:46 functime/feature_extraction/__init__.py
+-rw-r--r--  2.0 unx     4294 b- defN 23-Jul-31 15:46 functime/feature_extraction/calendar.py
+-rw-r--r--  2.0 unx     2162 b- defN 23-Jul-31 15:46 functime/feature_extraction/fourier.py
+-rw-r--r--  2.0 unx      783 b- defN 23-Jul-31 15:46 functime/forecasting/__init__.py
+-rw-r--r--  2.0 unx    12609 b- defN 23-Jul-31 15:46 functime/forecasting/_ar.py
+-rw-r--r--  2.0 unx     4893 b- defN 23-Jul-31 15:46 functime/forecasting/_evaluate.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Jul-31 15:46 functime/forecasting/_reduction.py
+-rw-r--r--  2.0 unx     6424 b- defN 23-Jul-31 15:46 functime/forecasting/_regressors.py
+-rw-r--r--  2.0 unx     8513 b- defN 23-Jul-31 15:46 functime/forecasting/automl.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Jul-31 15:46 functime/forecasting/catboost.py
+-rw-r--r--  2.0 unx     3722 b- defN 23-Jul-31 15:46 functime/forecasting/censored.py
+-rw-r--r--  2.0 unx    10936 b- defN 23-Jul-31 15:46 functime/forecasting/elite.py
+-rw-r--r--  2.0 unx     1010 b- defN 23-Jul-31 15:46 functime/forecasting/knn.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-31 15:46 functime/forecasting/lance.py
+-rw-r--r--  2.0 unx     4208 b- defN 23-Jul-31 15:46 functime/forecasting/lightgbm.py
+-rw-r--r--  2.0 unx     3958 b- defN 23-Jul-31 15:46 functime/forecasting/linear.py
+-rw-r--r--  2.0 unx     1943 b- defN 23-Jul-31 15:46 functime/forecasting/naive.py
+-rw-r--r--  2.0 unx     2412 b- defN 23-Jul-31 15:46 functime/forecasting/xgboost.py
+-rw-r--r--  2.0 unx      312 b- defN 23-Jul-31 15:46 functime/metrics/__init__.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Jul-31 15:46 functime/metrics/multi_objective.py
+-rw-r--r--  2.0 unx     7432 b- defN 23-Jul-31 15:46 functime/metrics/point.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-31 15:46 functime/metrics/probabilistic.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7412 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3532 b- defN 23-Jul-31 15:46 functime-0.3.3.dist-info/RECORD
+42 files, 188856 bytes uncompressed, 56454 bytes compressed:  70.1%
```

## zipnote {}

```diff
@@ -1,13 +1,10 @@
 Filename: functime/__init__.py
 Comment: 
 
-Filename: functime/__main__.py
-Comment: 
-
 Filename: functime/backtesting.py
 Comment: 
 
 Filename: functime/conformal.py
 Comment: 
 
 Filename: functime/conversion.py
@@ -48,14 +45,17 @@
 
 Filename: functime/feature_extraction/__init__.py
 Comment: 
 
 Filename: functime/feature_extraction/calendar.py
 Comment: 
 
+Filename: functime/feature_extraction/fourier.py
+Comment: 
+
 Filename: functime/forecasting/__init__.py
 Comment: 
 
 Filename: functime/forecasting/_ar.py
 Comment: 
 
 Filename: functime/forecasting/_evaluate.py
@@ -72,14 +72,17 @@
 
 Filename: functime/forecasting/catboost.py
 Comment: 
 
 Filename: functime/forecasting/censored.py
 Comment: 
 
+Filename: functime/forecasting/elite.py
+Comment: 
+
 Filename: functime/forecasting/knn.py
 Comment: 
 
 Filename: functime/forecasting/lance.py
 Comment: 
 
 Filename: functime/forecasting/lightgbm.py
@@ -102,26 +105,23 @@
 
 Filename: functime/metrics/point.py
 Comment: 
 
 Filename: functime/metrics/probabilistic.py
 Comment: 
 
-Filename: functime-0.3.2.dist-info/LICENSE
-Comment: 
-
-Filename: functime-0.3.2.dist-info/METADATA
+Filename: functime-0.3.3.dist-info/LICENSE
 Comment: 
 
-Filename: functime-0.3.2.dist-info/WHEEL
+Filename: functime-0.3.3.dist-info/METADATA
 Comment: 
 
-Filename: functime-0.3.2.dist-info/entry_points.txt
+Filename: functime-0.3.3.dist-info/WHEEL
 Comment: 
 
-Filename: functime-0.3.2.dist-info/top_level.txt
+Filename: functime-0.3.3.dist-info/top_level.txt
 Comment: 
 
-Filename: functime-0.3.2.dist-info/RECORD
+Filename: functime-0.3.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## functime/backtesting.py

```diff
@@ -106,15 +106,15 @@
     forecaster: Forecaster,
     y: pl.DataFrame,
     cv: Callable[[pl.DataFrame], Mapping[int, pl.DataFrame]],
     X: Optional[pl.DataFrame] = None,
     residualize: bool = True,
 ) -> Tuple[pl.DataFrame, pl.DataFrame]:
     pl.enable_string_cache(True)
-    entity_col = y.columns[0]
+    entity_col, time_col = y.columns[:2]
     y_splits = cv(y)
     X_splits = X if X is None else cv(X)
     y_preds = []
     y_resids = []
     for i in range(len(y_splits)):
         y_train, y_test = y_splits[i]
         fh = int(
@@ -127,25 +127,30 @@
         # Forecast
         forecaster = forecaster.fit(y=y_train, X=X_train)
         y_pred = forecaster.predict(fh=fh, X=X_test)
         # Coerce split column names back into original names
         y_pred = y_pred.select(y_pred.columns[:3]).with_columns(
             pl.lit(i).alias("split")
         )
+        # Coerce time column to y_test timestamps
+        y_test = y_test.sort([entity_col, time_col]).collect()
+        y_pred = y_pred.sort([entity_col, time_col]).with_columns(
+            y_test.get_column(time_col)
+        )
         # Append results
         y_preds.append(y_pred)
         if residualize:
             # Residuals
             y_resid = _residualize_autoreg(
                 y_train=y_train,
                 X_train=X_train,
-                strategy=forecaster.state["strategy"],
+                strategy=forecaster.state.strategy,
                 lags=forecaster.lags,
                 max_horizons=forecaster.max_horizons,
-                artifacts=forecaster.state["artifacts"],
+                artifacts=forecaster.state.artifacts,
             )
             y_resid = y_resid.with_columns(pl.lit(i).alias("split"))
             y_resids.append(y_resid)
 
     y_preds = pl.concat(y_preds)
     full_forecaster = forecaster.fit(y=y, X=X)
     if residualize:
```

## functime/conversion.py

```diff
@@ -32,14 +32,51 @@
             series = df.get_column(col)
             x = series.to_numpy(zero_copy_only=True)
             X[:, i] = x
         X = da.from_zarr(X).compute()
     return X
 
 
+def X_to_numpy(X: pl.DataFrame) -> np.ndarray:
+    X_arr = (
+        X.lazy()
+        .select(pl.col(X.columns[2:]).cast(pl.Float32))
+        .select(
+            pl.when(pl.all().is_infinite() | pl.all().is_nan())
+            .then(None)
+            .otherwise(pl.all())
+            .keep_name()
+        )
+        # TODO: Support custom groupby imputation
+        .fill_null(strategy="mean")  # Do not fill backward (data leak)
+        .collect(streaming=True)
+        .pipe(df_to_ndarray)
+    )
+    return X_arr
+
+
+def y_to_numpy(y: pl.DataFrame) -> np.ndarray:
+    y_arr = (
+        y.lazy()
+        .select(pl.col(y.columns[-1]).cast(pl.Float32))
+        .select(
+            pl.when(pl.all().is_infinite() | pl.all().is_nan())
+            .then(None)
+            .otherwise(pl.all())
+            .keep_name()
+        )
+        # TODO: Support custom groupby imputation
+        .fill_null(strategy="mean")  # Do not fill backward (data leak)
+        .collect(streaming=True)
+        .get_column(y.columns[-1])
+        .to_numpy(zero_copy_only=True)
+    )
+    return y_arr
+
+
 if __name__ == "__main__":
 
     from timeit import default_timer
 
     df = pl.DataFrame({f"x{i}": pl.arange(0, 1_000_000, eager=True) for i in range(48)})
     start = default_timer()
     X = df_to_ndarray(df)
```

## functime/preprocessing.py

```diff
@@ -1,8 +1,7 @@
-from itertools import product
 from typing import List, Mapping, Union
 
 import polars as pl
 import polars.selectors as cs
 from scipy.stats import boxcox_normmax
 from typing_extensions import Literal
 
@@ -11,24 +10,38 @@
 from functime.offsets import _strip_freq_alias
 
 
 def PL_NUMERIC_COLS(*exclude):
     return cs.numeric() - cs.by_name(exclude)
 
 
-def reindex(X: pl.DataFrame) -> pl.DataFrame:
-    entity_col, time_col = X.columns[:2]
-    dtypes = X.dtypes[:2]
-    entities = sorted(set(X.get_column(entity_col)))
-    timestamps = sorted(set(X.get_column(time_col)))
-    X = pl.DataFrame(
-        product(entities, timestamps),
-        schema={entity_col: dtypes[0], time_col: dtypes[1]},
-    ).join(X, how="left", on=[entity_col, time_col])
-    return X
+@transformer
+def reindex(drop_duplicates: bool = False):
+    """Reindexes the entity and time columns to have every possible combination of (entity, time).
+
+    Parameters
+    ---------
+    drop_duplicates : bool
+        Defaults to False. If True, duplicates are dropped before reindexing.
+    """
+
+    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        entity_col, time_col = X.columns[:2]
+        if drop_duplicates:
+            entities = X.select(pl.col(entity_col).unique())
+            timestamps = X.select(pl.col(time_col).unique())
+        else:
+            entities = X.select(entity_col)
+            timestamps = X.select(time_col)
+        idx = entities.join(timestamps, how="cross")
+        X_new = idx.join(X, how="left", on=[entity_col, time_col])
+        artifacts = {"X_new": X_new}
+        return artifacts
+
+    return transform
 
 
 @transformer
 def coerce_dtypes(schema: Mapping[str, pl.DataType]):
     """Coerces the column datatypes of a DataFrame using the provided schema.
 
     Parameters
@@ -50,28 +63,22 @@
     """Coerces time column into arange per entity.
 
     Assumes even-spaced time-series and homogenous start dates.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
         entity_col, time_col = X.columns[:2]
-        range_expr = pl.arange(0, pl.col(time_col).count()).alias(time_col)
-        other_cols = pl.all().exclude(time_col)
-        X_new = (
-            X.groupby(entity_col)
-            .agg([range_expr, other_cols])
-            .explode(pl.all().exclude(entity_col))
-            .select(
-                [
-                    entity_col,
-                    pl.col(time_col).cast(pl.Int32),
-                    pl.all().exclude([entity_col, time_col]),
-                ]
-            )
+        time_range_expr = (
+            pl.arange(0, pl.col(time_col).count())
+            .over(entity_col)
+            .alias(time_col)
+            .cast(pl.Int32)
         )
+        other_cols = pl.all().exclude([entity_col, time_col])
+        X_new = X.select([entity_col, time_range_expr, other_cols])
         if eager:
             X_new = X_new.collect(streaming=True)
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
@@ -115,14 +122,53 @@
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
+def trim(direction: Literal["both", "left", "right"] = "both"):
+    """Trims time-series in panel to have the same start or end dates as the shortest time-series.
+
+    Parameters
+    ----------
+    direction : Literal["both", "left", "right"]
+        Defaults to "both". If "left" trims from start date of the shortest time series);
+        if "right" trims up to the end date of the shortest time-series; or otherwise
+        "both" trims between start and end dates of the shortest time-series
+    """
+
+    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        entity_col, time_col = X.columns[:2]
+        maxmin = (
+            X.groupby(entity_col)
+            .agg(pl.col(time_col).min())
+            .select(pl.col(time_col).max())
+        )
+        minmax = (
+            X.groupby(entity_col)
+            .agg(pl.col(time_col).max())
+            .select(pl.col(time_col).min())
+        )
+        start, end = pl.collect_all([minmax, maxmin])
+        start, end = start.item(), end.item()
+        if direction == "both":
+            expr = (pl.col(time_col) >= start) & (pl.col(time_col) <= end)
+        elif direction == "left":
+            expr = pl.col(time_col) >= start
+        else:
+            expr = pl.col(time_col) <= start
+        X_new = X.filter(expr)
+        artifacts = {"X_new": X_new}
+        return artifacts
+
+    return transform
+
+
+@transformer
 def lag(lags: List[int]):
     """Applies lag transformation to a LazyFrame.
 
     Parameters
     ----------
     lags : List[int]
         A list of lag values to apply.
@@ -436,55 +482,39 @@
     order : int
         The order to difference.
     sp : int
         Seasonal periodicity.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
-        def _diff(X):
-            X_new = (
-                X.groupby(entity_col, maintain_order=True)
-                .agg([pl.col(time_col), cs.float() - cs.float().shift(sp)])
-                .explode(pl.all().exclude(entity_col))
-            )
-            return X_new
 
         idx_cols = X.columns[:2]
         entity_col = idx_cols[0]
         time_col = idx_cols[1]
-        X = X.with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
 
         X_first, X_last = pl.collect_all(
             [
                 X.groupby(entity_col).head(1),
                 X.groupby(entity_col).tail(1),
             ]
         )
         for _ in range(order):
-            X = _diff(X)
+            X = X.select([entity_col, time_col, cs.float().diff(n=sp).over(entity_col)])
 
         # Drop null
         artifacts = {
             "X_new": X.drop_nulls(),
             "X_first": X_first.lazy(),
             "X_last": X_last.lazy(),
         }
         return artifacts
 
     def invert(
         state: ModelState, X: pl.LazyFrame, from_last: bool = False
     ) -> pl.LazyFrame:
-        def _inverse_diff(X: pl.LazyFrame):
-            X_new = (
-                X.groupby(entity_col, maintain_order=True)
-                .agg([pl.col(time_col), cs.float().cumsum()])
-                .explode(pl.all().exclude(entity_col))
-            )
-            return X_new
-
         artifacts = state.artifacts
         entity_col = X.columns[0]
         time_col = X.columns[1]
         idx_cols = entity_col, time_col
 
         X_cutoff = artifacts["X_last"] if from_last else artifacts["X_first"]
         X_new = (
@@ -497,15 +527,17 @@
                 ],
                 how="diagonal",
             )
             .drop_nulls()
             .sort(idx_cols)
         )
         for _ in range(order):
-            X_new = _inverse_diff(X_new)  # noqa: F821
+            X_new = X_new.select(
+                [entity_col, time_col, cs.float().cumsum().over(entity_col)]
+            )
 
         return X.select(idx_cols).join(X_new, on=idx_cols, how="left")
 
     return transform, invert
 
 
 @transformer
@@ -564,36 +596,7 @@
                 )
                 for col in cols
             ]
         )
         return X_new
 
     return transform, invert
-
-
-@transformer
-def trim(direction: Literal["both", "left", "right"] = "both"):
-    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
-        entity_col, time_col = X.columns[:2]
-        maxmin = (
-            X.groupby(entity_col)
-            .agg(pl.col(time_col).min())
-            .select(pl.col(time_col).max())
-        )
-        minmax = (
-            X.groupby(entity_col)
-            .agg(pl.col(time_col).max())
-            .select(pl.col(time_col).min())
-        )
-        start, end = pl.collect_all([minmax, maxmin])
-        start, end = start.item(), end.item()
-        if direction == "both":
-            expr = (pl.col(time_col) >= start) & (pl.col(time_col) <= end)
-        elif direction == "left":
-            expr = pl.col(time_col) >= start
-        else:
-            expr = pl.col(time_col) <= start
-        X_new = X.filter(expr)
-        artifacts = {"X_new": X_new}
-        return artifacts
-
-    return transform
```

## functime/ranges.py

```diff
@@ -9,15 +9,16 @@
     time_col: str,
     cutoffs: pl.DataFrame,
     fh: int,
     freq: Optional[str] = None,
     time_unit: Optional[str] = None,
 ) -> pl.DataFrame:
     """Return pl.DataFrame with columns entity_col, time_col.
-    DataFrame has shape (n_entities, 2) and dtypes (str, list[date]) or (str, list[datetime]).
+
+    DataFrame has shape (n_entities, 2) and dtypes (str, list[date]), (str, list[datetime]), or (str, list[int]).
     """
     entity_col = cutoffs.columns[0]
     if isinstance(freq, str):
         if freq.endswith("i"):
             future_ranges = cutoffs.select(
                 [
                     pl.col(entity_col),
```

## functime/base/forecaster.py

```diff
@@ -56,15 +56,14 @@
         **kwargs,
     ):
         self.freq = freq
         self.lags = lags
         self.max_horizons = max_horizons
         self.strategy = strategy
         self.target_transform = target_transform
-        self._time_col_dtype = None
         self.kwargs = kwargs
         super().__init__()
 
     def __call__(
         self,
         y: DF_TYPE,
         fh: int,
@@ -108,17 +107,17 @@
         return self
 
     def predict(self, fh: int, X: Optional[DF_TYPE] = None) -> pl.DataFrame:
 
         from functime.forecasting._ar import predict_autoreg
 
         state = self.state
-        entity = state.entity
-        time = state.time
-        target = state.target
+        entity_col = state.entity
+        time_col = state.time
+        target_col = state.target
         # Cutoffs cannot be lazy
         cutoffs: pl.DataFrame = state.artifacts["__cutoffs"]
         future_ranges = make_future_ranges(
             time_col=state.time,
             cutoffs=cutoffs,
             fh=fh,
             freq=self.freq,
@@ -130,36 +129,36 @@
             has_entity = X.columns[0] == state.entity
             has_time = X.columns[1] == state.time
 
             if has_entity:
                 X = self._enforce_string_cache(X.lazy().collect()).lazy()
 
             if has_entity and not has_time:
-                X = future_ranges.lazy().join(X, on=entity, how="left")
+                X = future_ranges.lazy().join(X, on=entity_col, how="left")
             elif has_time and not has_entity:
-                X = future_ranges.lazy().join(X, on=time, how="left")
+                X = future_ranges.lazy().join(X, on=time_col, how="left")
 
             # NOTE: Unlike `y_lag` we DO NOT reshape exogenous features
             # into list columns. This is because .arr[i] with List[cat] does
             # not seem to support null values
             # Raises: ComputeError: cannot construct Categorical
             # from these categories, at least on of them is out of bounds
-            X = X.select(pl.all().exclude(time)).lazy()
+            X = X.select(pl.all().exclude(time_col)).lazy()
         y_pred_vals = predict_autoreg(self.state, fh=fh, X=X)
-        y_pred_vals = y_pred_vals.sort(by=entity).select(
-            pl.col(y_pred_vals.columns[-1]).alias(target)
+        y_pred_vals = y_pred_vals.sort(by=entity_col).select(
+            pl.col(y_pred_vals.columns[-1]).alias(target_col)
         )
         y_pred = pl.concat(
-            [future_ranges.sort(by=entity), y_pred_vals], how="horizontal"
-        ).explode(pl.all().exclude(entity))
+            [future_ranges.sort(by=entity_col), y_pred_vals], how="horizontal"
+        ).explode(pl.all().exclude(entity_col))
 
         if self.target_transform is not None:
             schema = self.state.target_schema
             y_pred = (
-                y_pred.with_columns(pl.col(time).cast(schema[time]))
+                y_pred.with_columns(pl.col(time_col).cast(schema[time_col]))
                 .pipe(self.target_transform.invert)
                 .collect(streaming=True)
             )
 
         y_pred = y_pred.pipe(self._reset_string_cache)
         return y_pred
```

## functime/base/metric.py

```diff
@@ -5,14 +5,18 @@
 
 from functime.base.model import (
     _enforce_string_cache,
     _reset_string_cache,
     _set_string_cache,
 )
 
+METRIC_TYPE = Callable[
+    [Union[pl.LazyFrame, pl.DataFrame], Union[pl.LazyFrame, pl.DataFrame]], pl.DataFrame
+]
+
 
 # Simple wrapper to collect y_true, y_pred if lazy
 def metric(score: Callable):
     @wraps(score)
     def _score(
         y_true: Union[pl.LazyFrame, pl.DataFrame],
         y_pred: Union[pl.LazyFrame, pl.DataFrame],
```

## functime/feature_extraction/__init__.py

```diff
@@ -1,13 +1,15 @@
 from .calendar import (
     add_calendar_effects,
     add_holiday_effects,
     make_future_calendar_effects,
     make_future_holiday_effects,
 )
+from .fourier import add_fourier_terms
 
 __all__ = [
     "add_calendar_effects",
     "add_holiday_effects",
+    "add_fourier_terms",
     "make_future_calendar_effects",
     "make_future_holiday_effects",
 ]
```

## functime/forecasting/__init__.py

```diff
@@ -4,14 +4,15 @@
     auto_lasso,
     auto_lightgbm,
     auto_linear_model,
     auto_ridge,
 )
 from .catboost import catboost
 from .censored import censored_model, zero_inflated_model
+from .elite import elite
 from .knn import knn
 from .lance import ann
 from .lightgbm import flaml_lightgbm, lightgbm
 from .linear import elastic_net, lasso, linear_model, ridge
 from .xgboost import xgboost
 
 __all__ = [
@@ -21,14 +22,15 @@
     "auto_lasso",
     "auto_lightgbm",
     "auto_linear_model",
     "auto_ridge",
     "catboost",
     "censored_model",
     "elastic_net",
+    "elite",
     "flaml_lightgbm",
     "knn",
     "lasso",
     "lightgbm",
     "linear_model",
     "ridge",
     "xgboost",
```

## functime/forecasting/_ar.py

```diff
@@ -148,19 +148,15 @@
     # Test each lag
     best_lags = None
     best_score = np.inf
     best_params = None
     scores_path = []
     lags_path = list(range(min_lags, max_lags + 1))
     scores_path = []
-    for lags in (
-        pbar := tqdm(
-            lags_path, desc=f"🚀 Evaluating models with n={min(lags_path)} lags"
-        )
-    ):
+    for lags in (pbar := tqdm(lags_path, desc=f"Evaluating n={min(lags_path)} lags")):
         score, params = evaluate(
             **{
                 "lags": lags,
                 "n_splits": n_splits,
                 "time_budget": time_budget,
                 "points_to_evaluate": points_to_evaluate,
                 "num_samples": num_samples,
@@ -178,15 +174,15 @@
         )
         scores_path.append(score)
         if score < best_score:
             best_score = score
             best_lags = lags
             best_params = params
         pbar.set_description(
-            f"🚀 [Best round: lags={best_lags}, score={best_score:.2f}] Evaluating models with n={lags + 1} lags"
+            f"[Best round: lags={best_lags}, score={best_score:.2f}] Evaluating models with n={lags + 1} lags"
         )
 
     # Refit
     best_params = best_params or {}
     best_params = {
         "freq": freq,
         **best_params,
```

## functime/forecasting/_evaluate.py

```diff
@@ -38,16 +38,17 @@
         forecaster.fit(y=y_train, X=X_train)
         y_pred = forecaster.predict(fh=test_size, X=X_test)
         # NOTE: Defensive match of entity, time indices
         # Need to do this for example if the train set is "1i", but the
         # test set starts from 1,2,3,...,fh
         # Assumes y_test and y_pred align up
         y_test = y_test.sort([entity_col, time_col])
-        y_pred = y_pred.sort([entity_col, time_col])
-        y_test = y_test.with_columns(**{time_col: y_pred.get_column(time_col)})
+        y_pred = y_pred.sort([entity_col, time_col]).with_columns(
+            y_test.get_column(time_col)
+        )
         score = mae(y_true=y_test, y_pred=y_pred).get_column("mae").mean()
         res = {"score": score}
     except ValueError as exc:
         # AttributeError: 'NoneType' object has no attribute 'last_result'
         # Root cause: y_preds are inf hence mae = inf
         logging.warning(
             "%s fit-predict failed with lags %s and parameters %s",
```

## functime/forecasting/_regressors.py

```diff
@@ -5,55 +5,18 @@
 from typing import Callable, Optional, Union
 
 import numpy as np
 import polars as pl
 import sklearn
 from typing_extensions import Literal
 
-from functime.conversion import df_to_ndarray
+from functime.conversion import X_to_numpy, y_to_numpy
 from functime.preprocessing import PL_NUMERIC_COLS
 
 
-def _X_to_numpy(X: pl.DataFrame) -> np.ndarray:
-    X_arr = (
-        X.lazy()
-        .select(pl.col(X.columns[2:]).cast(pl.Float32))
-        .select(
-            pl.when(pl.all().is_infinite() | pl.all().is_nan())
-            .then(None)
-            .otherwise(pl.all())
-            .keep_name()
-        )
-        # TODO: Support custom groupby imputation
-        .fill_null(strategy="mean")  # Do not fill backward (data leak)
-        .collect(streaming=True)
-        .pipe(df_to_ndarray)
-    )
-    return X_arr
-
-
-def _y_to_numpy(y: pl.DataFrame) -> np.ndarray:
-    y_arr = (
-        y.lazy()
-        .select(pl.col(y.columns[-1]).cast(pl.Float32))
-        .select(
-            pl.when(pl.all().is_infinite() | pl.all().is_nan())
-            .then(None)
-            .otherwise(pl.all())
-            .keep_name()
-        )
-        # TODO: Support custom groupby imputation
-        .fill_null(strategy="mean")  # Do not fill backward (data leak)
-        .collect(streaming=True)
-        .get_column(y.columns[-1])
-        .to_numpy(zero_copy_only=True)
-    )
-    return y_arr
-
-
 class GradientBoostedTreeRegressor:
     def __init__(
         self,
         regress,
         weight_transform: Optional[Callable] = None,
         fit_dtype: Literal["numpy", "arrow"] = None,
         predict_dtype: Union[Literal["numpy", "arrow"], Callable] = None,
@@ -78,31 +41,31 @@
         sample_weight = None
         if weight_transform is not None:
             sample_weight = y.pipe(weight_transform)
 
         X = self._preproc_X(X)
 
         if self.fit_dtype == "numpy":
-            X_coerced = _X_to_numpy(X)
-            y_coerced = _y_to_numpy(X)
+            X_coerced = X_to_numpy(X)
+            y_coerced = y_to_numpy(X)
         elif self.fit_dtype == "arrow":
             X_coerced = X.to_arrow()
             y_coerced = y.to_arrow()
         else:
             raise ValueError(f"`fit_dtype` not supported: {self.fit_dtype}")
 
         self.regressor = self.regress(
             X=X_coerced, y=y_coerced, sample_weight=sample_weight
         )
         return self
 
     def predict(self, X: pl.DataFrame) -> np.ndarray:
         X = self._preproc_X(X)
         if self.predict_dtype == "numpy":
-            X_coerced = _X_to_numpy(X)
+            X_coerced = X_to_numpy(X)
         elif self.predict_dtype == "arrow":
             X_coerced = X.to_arrow
         elif isinstance(self.predict_dtype, Callable):
             X_coerced = self.predict_dtype(X)
         y_pred = self.regressor.predict(X_coerced)
         return y_pred
 
@@ -125,22 +88,22 @@
         return X_new
 
     def fit(self, X: pl.DataFrame, y: pl.DataFrame):
         X_new = self._preproc_X(X).lazy()
         # Regress
         with sklearn.config_context(assume_finite=True):
             # NOTE: We can assume finite due to preproc
-            self.regressor = self.regressor.fit(X=_X_to_numpy(X_new), y=_y_to_numpy(y))
+            self.regressor = self.regressor.fit(X=X_to_numpy(X_new), y=y_to_numpy(y))
         return self
 
     def predict(self, X: pl.DataFrame) -> np.ndarray:
         X_new = self._preproc_X(X).lazy()
         with sklearn.config_context(assume_finite=True):
             # NOTE: We can assume finite due to preproc
-            y_pred = self.regressor.predict(_X_to_numpy(X_new))
+            y_pred = self.regressor.predict(X_to_numpy(X_new))
         return y_pred
 
 
 class CensoredRegressor:
     def __init__(
         self,
         threshold: Union[int, float],
@@ -161,36 +124,32 @@
         target_col = y.columns[-1]
         threshold = self.threshold
         X_y_above = X.join(y, on=idx_cols).filter(pl.col(target_col) > threshold)
         y_above = X_y_above.select([*idx_cols, target_col])
         X_above = X_y_above.select(pl.all().exclude(target_col))
         if threshold == 0:
             self.regressors = (
-                self.regress(_X_to_numpy(X_above), _y_to_numpy(y_above)),
+                self.regress(X_to_numpy(X_above), y_to_numpy(y_above)),
                 None,
             )
         else:
             X_y_below = X.join(y, on=idx_cols).filter(pl.col(target_col) <= threshold)
             y_below = X_y_below.select([*idx_cols, target_col])
             X_below = X_y_below.select(pl.all().exclude(target_col))
-            fitted_model_above = self.regress(
-                _X_to_numpy(X_above), _y_to_numpy(y_above)
-            )
-            fitted_model_below = self.regress(
-                _X_to_numpy(X_below), _y_to_numpy(y_below)
-            )
+            fitted_model_above = self.regress(X_to_numpy(X_above), y_to_numpy(y_above))
+            fitted_model_below = self.regress(X_to_numpy(X_below), y_to_numpy(y_below))
             self.regressors = fitted_model_above, fitted_model_below
         return self
 
     def predict(self, X: pl.DataFrame) -> np.ndarray:
-        weights = self.predict_proba(_X_to_numpy(X))
+        weights = self.predict_proba(X_to_numpy(X))
         regress_above, regress_below = self.regressors
-        y_pred = weights[:, 1] * regress_above.predict(_X_to_numpy(X))
+        y_pred = weights[:, 1] * regress_above.predict(X_to_numpy(X))
         if abs(self.threshold) > 0:
-            y_pred += weights[:, 0] * regress_below.predict(_X_to_numpy(X))
+            y_pred += weights[:, 0] * regress_below.predict(X_to_numpy(X))
         return y_pred, weights[:, 1]
 
 
 class FLAMLRegressor:
     """FLAML AutoML regressor.
 
     API reference: https://microsoft.github.io/FLAML/docs/reference/automl/automl/#automl-objects
```

## functime/forecasting/censored.py

```diff
@@ -1,17 +1,18 @@
 from typing import Callable, Optional, Union
 
 import numpy as np
 import polars as pl
 
 from functime.base import Forecaster
 from functime.base.forecaster import FORECAST_STRATEGIES
+from functime.conversion import X_to_numpy, y_to_numpy
 from functime.forecasting._ar import fit_autoreg
 from functime.forecasting._reduction import make_reduction
-from functime.forecasting._regressors import CensoredRegressor, _X_to_numpy, _y_to_numpy
+from functime.forecasting._regressors import CensoredRegressor
 
 
 def default_regress(X: np.ndarray, y: np.ndarray):
     from sklearn.ensemble import HistGradientBoostingRegressor
 
     regressor = HistGradientBoostingRegressor()
     regressor.fit(X=X, y=y)
@@ -67,17 +68,15 @@
         )
         X_final, y_final = pl.collect_all(
             [
                 X_y_final.select(pl.all().exclude(target_col)),
                 X_y_final.select([*X_y_final.columns[:2], target_col]),
             ]
         )
-        fitted_classifier = self.classify(
-            X=_X_to_numpy(X_final), y=_y_to_numpy(y_final)
-        )
+        fitted_classifier = self.classify(X=X_to_numpy(X_final), y=y_to_numpy(y_final))
         # 2. Fit forecast model on non-zeros
         censored_regressor = CensoredRegressor(
             threshold=self.threshold,
             regress=self.regress,
             predict_proba=fitted_classifier.predict_proba,
         )
         forecast_artifacts = fit_autoreg(
```

## functime/forecasting/naive.py

```diff
@@ -3,32 +3,44 @@
 import polars as pl
 
 from functime.base import Forecaster
 from functime.ranges import make_future_ranges
 
 
 class naive(Forecaster):
-    def __init__(self, freq: str, fh: int):
-        self.fh = fh
+    """Naive forecaster.
+
+    Parameters
+    ----------
+    freq : str
+        Offset alias supported by Polars
+    max_fh : int
+        Max forecast horizon. `fh` in predict cannot exceed this value.
+    """
+
+    def __init__(self, freq: str, max_fh: int):
+        self.max_fh = max_fh
         super().__init__(freq=freq, lags=1)
 
     def _fit(self, y: pl.DataFrame, X: Optional[pl.DataFrame] = None):
-        fh = self.fh
+        max_fh = self.max_fh
         idx_cols = y.columns[:2]
         entity_col = idx_cols[0]
         target_col = y.columns[2]
         y_past = (
             y
             # Sort by entity and time
             .sort(idx_cols)
             # Group by entity then takes the last row
             .groupby(entity_col).tail(1)
         )
 
-        y_pred = pl.concat([y_past] * fh).groupby(entity_col).agg(pl.col(target_col))
+        y_pred = (
+            pl.concat([y_past] * max_fh).groupby(entity_col).agg(pl.col(target_col))
+        )
         artifacts = {"y_pred": y_pred}
         return artifacts
 
     def predict(self, fh: int, X: Optional[pl.DataFrame] = None) -> pl.DataFrame:
         state = self.state
         entity = state.entity
         target = state.target
@@ -39,14 +51,14 @@
             cutoffs=cutoffs,
             fh=fh,
             freq=self.freq,
         )
         y_pred_vals = state.artifacts["y_pred"]
         y_pred_vals = (
             y_pred_vals.sort(by=entity)
-            .select(pl.col(y_pred_vals.columns[-1]).alias(target))
-            .collect()
+            .select(pl.col(y_pred_vals.columns[-1]).alias(target).list.head(fh))
+            .collect(streaming=True)
         )
         y_pred = pl.concat(
             [future_ranges.sort(by=entity), y_pred_vals], how="horizontal"
         ).explode(pl.all().exclude(entity))
         return y_pred.pipe(self._reset_string_cache)
```

## Comparing `functime-0.3.2.dist-info/LICENSE` & `functime-0.3.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `functime-0.3.2.dist-info/METADATA` & `functime-0.3.3.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: functime
-Version: 0.3.2
+Version: 0.3.3
 Summary: The easiest way to run and scale time-series machine learning in the Cloud.
 Author-email: functime Team <team@functime.ai>, Chris Lo <chris@functime.ai>, Daryl Lim <daryl@functime.ai>
 Project-URL: Homepage, https://github.com/descendant-ai/functime
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
```

## Comparing `functime-0.3.2.dist-info/RECORD` & `functime-0.3.3.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 functime/__init__.py,sha256=IpTQKBIY-7DJ1VnnWAQcziICkAKKoLMM-rJkzlO_x2o,66
-functime/__main__.py,sha256=HpYJUAIeN5HXQhaM5_kDwo441jDPe9GJ8dLjF9HE9VQ,117
-functime/backtesting.py,sha256=S85juPZ_X3x52rIcXAxmalXGWQeRw3KCVXqVcR6aKns,5908
+functime/backtesting.py,sha256=rEn299g57Mwa4-JUdgJBfvK3KwZs10dufmdNeK8aE8U,6143
 functime/conformal.py,sha256=SLLPOEEzOdzVdFzCpNfmETmr4fHKqIhtJC-Xce6mHyo,1838
-functime/conversion.py,sha256=ulS0QWDUWgA3KgJ_2Gn0R-eMRWADN9sVfSOghtzjUIE,1379
+functime/conversion.py,sha256=VUn6bbsXTaRAR1n9fDQpgLf4KanvbCIEMctIw5FG5tg,2456
 functime/cross_validation.py,sha256=uTkcotiLATM_qA0zfgWAIPaD4fJW9LSqiBABfAbeqvY,5549
 functime/embeddings.py,sha256=rupMM8Fs6-nExoWYHS-PWFHiEHXsrNBvP4o-PPzACos,123
 functime/offsets.py,sha256=GUCVLFs0xBSj_gsSaBz19ibBr_nquJVUxtuosObBucU,1495
 functime/plotting.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime/preprocessing.py,sha256=hU2flqwzR0bxCcObpJhPWHzflApEQ2uQI5W8NglC0jg,20394
-functime/ranges.py,sha256=b3biGP7s2OpoYopEGExELes79D_LdloDW4QH9j1slxI,1976
+functime/preprocessing.py,sha256=l6EdFHIp3DFjUYZACJbEk0_Tf0LGwEN1ZoiMz80VmeE,20573
+functime/ranges.py,sha256=aZ3CFVRhoAVqw2EAnK532Tara3n66Ka7c_Qz1FJJEQQ,1996
 functime/base/__init__.py,sha256=YKzO_7RyxYbn3-jTa4gipQxrwnzUQN2_d2edjDehLmw,217
-functime/base/forecaster.py,sha256=kzK7x_VzS0t-YIBZEpuQkdZs4CCVGBfp9NXxmFQXSfw,8253
-functime/base/metric.py,sha256=Ous83EZ4ADpcyvHnTGNaxekqKEkwQB7qbYWuzKiGAAI,1630
+functime/base/forecaster.py,sha256=5JwdxqxE0UnukC8o1K8xhSRfyKkh7J6vQ_iV_qlf7mw,8265
+functime/base/metric.py,sha256=__ByYz8ojkLJMjbTB1fTg2KDC5wEl-h_s1AxDSIsjkM,1746
 functime/base/model.py,sha256=mL8GFL7qNNb4rufm3ZbLzxyDPoDuoHgQhpZ0t20AHv4,2771
 functime/base/transformer.py,sha256=qSuKYisdWL1qFufJlr50zNg-NryMX5q7MP61I1DgPHw,2155
-functime/feature_extraction/__init__.py,sha256=GS4FtQUUuzJZAJJ4g7fgamtbZhAecz8iwaQsejPTVhs,285
+functime/feature_extraction/__init__.py,sha256=mSHvtROzMYq19_-emVmIEP87qDzQjC1RtsmzVRgGRX4,349
 functime/feature_extraction/calendar.py,sha256=z6WsFPma0KV0zAq93sZ3vlofaG_LMht47ihry8y4GEg,4294
-functime/forecasting/__init__.py,sha256=XWBiKZpNADvDUe-aaMtdppZt_8LvEYth7xzELJuy6Do,745
-functime/forecasting/_ar.py,sha256=MIV_JBQ5HlgL6uV7W4GKGDl6jx8gwZqmSaPcwJ-Xadw,12667
-functime/forecasting/_evaluate.py,sha256=mVXNCX-JBXq2FNJFAGn_rF22zVidlQFG6cIA0dtyxaQ,4909
+functime/feature_extraction/fourier.py,sha256=2mQ61zhynGmBxSyyOqcjTQC_wYe0-6Vu8_0neUcWzNU,2162
+functime/forecasting/__init__.py,sha256=IRwMvzG0WkYRXwKm45NIpRvhrdX6duEMHrCOQ6woP74,783
+functime/forecasting/_ar.py,sha256=RwdvXWNXD888i0n2RSUqsqFC4W647kWd30i57v9TK_s,12609
+functime/forecasting/_evaluate.py,sha256=V8XmiXA0loE-7DUkixJiC-I8DSxGkFjqjx5Dqg_4-Hs,4893
 functime/forecasting/_reduction.py,sha256=FLYt6FXJCIcg5j60BiJNX8HvE8LHnkBdoc0BvYyDEtA,2231
-functime/forecasting/_regressors.py,sha256=Zw3THK02QvgnP0VTyIEJ0elFlbz6ic7uRS9rymXR2bs,7569
+functime/forecasting/_regressors.py,sha256=_UihMNrrCShfmABv5HKhx9JOSR_b-r-voacKXv-6pLI,6424
 functime/forecasting/automl.py,sha256=Wan9SoRzPdi7M8RkkniuHclxw1PO34XbC2EOPiwESW8,8513
 functime/forecasting/catboost.py,sha256=nF8I4AxJH7qDIu2BUGE_l7w-guNt8iYuNKH9oy-N3tU,2175
-functime/forecasting/censored.py,sha256=-iOSbD1byLF_ixsDKCLlqgY4YttRDyFbvRfoNGkXgok,3717
+functime/forecasting/censored.py,sha256=fKTO9I0c9k_TKuPwxk4sYdo9793Lge0R0QVncFNCwUY,3722
+functime/forecasting/elite.py,sha256=g-G7eo0oipbxEU38tUvOcUuwAUFM5wqOls6mVeaT1e4,10936
 functime/forecasting/knn.py,sha256=NA2-dCTHtVMUonuQMCU8i-bZuScB2H6mRVCXB_uUytQ,1010
 functime/forecasting/lance.py,sha256=LiEhbLrd6DpMoFhYjbjyPXWhHxNEdM_v_XdpPcgpwiU,3703
 functime/forecasting/lightgbm.py,sha256=1TPdFXhtXfp6cl7fQGzmHg1G0OUpAoSXZkvbiP1amKE,4208
 functime/forecasting/linear.py,sha256=adMbCOpqNZZDLICTULY8Ya6dDTCJacmeY0-76SC7odI,3958
-functime/forecasting/naive.py,sha256=Hv9U1x-jrCbPjGWDgIA-hSOl40at9ffPOj5TjuyOr8M,1657
+functime/forecasting/naive.py,sha256=Sla8qTn18j9rTctzMJKxXIZrYbHBIQSGfkqMXGTIby4,1943
 functime/forecasting/xgboost.py,sha256=kguM3IvqtJKSD3tysxgCiOe-NwApBew5tOO9_zxt4-I,2412
 functime/metrics/__init__.py,sha256=4DE4A-UtFY8AfTAEmVGqrbs2tXQwtO7Ogq-BZJp4zr8,312
 functime/metrics/multi_objective.py,sha256=EP9sGQ4HSCRN1SQazoJfntcovMP6RAVf7hZMXgcA2gE,3826
 functime/metrics/point.py,sha256=4rgQ0RJ8UR0xRxCOxxO2wuSHZFhj26XgLK4VBm3GHtQ,7432
 functime/metrics/probabilistic.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime-0.3.2.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-functime-0.3.2.dist-info/METADATA,sha256=0j3iB7dvO5WAWh3Uz7opRKbLvaVSOcIrRY29N6lQPm4,7412
-functime-0.3.2.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-functime-0.3.2.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
-functime-0.3.2.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
-functime-0.3.2.dist-info/RECORD,,
+functime-0.3.3.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+functime-0.3.3.dist-info/METADATA,sha256=InSiHm3EBC1RMw8wb_UiItS2trfbJr6TFonvWSu0HRc,7412
+functime-0.3.3.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+functime-0.3.3.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
+functime-0.3.3.dist-info/RECORD,,
```

