# Comparing `tmp/e2eqavn-0.1.7-py2.py3-none-any.whl.zip` & `tmp/e2eqavn-0.1.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,42 @@
-Zip file size: 38774 bytes, number of entries: 40
--rw-rw-r--  2.0 unx      498 b- defN 23-Jul-03 15:47 e2eqavn/__init__.py
--rw-rw-r--  2.0 unx    10774 b- defN 23-Jul-01 11:37 e2eqavn/cli.py
+Zip file size: 39514 bytes, number of entries: 40
+-rw-rw-r--  2.0 unx      498 b- defN 23-Jul-31 08:08 e2eqavn/__init__.py
+-rw-rw-r--  2.0 unx    12134 b- defN 23-Jul-26 13:49 e2eqavn/cli.py
 -rw-rw-r--  2.0 unx     2716 b- defN 23-Jul-01 11:37 e2eqavn/keywords.py
 -rw-rw-r--  2.0 unx     3639 b- defN 23-Jul-01 11:37 e2eqavn/datasets/MRCDataset.py
 -rw-rw-r--  2.0 unx     1386 b- defN 23-Jul-01 11:32 e2eqavn/datasets/TripletDataset.py
 -rw-rw-r--  2.0 unx      124 b- defN 23-Jul-01 11:32 e2eqavn/datasets/__init__.py
 -rw-rw-r--  2.0 unx     3814 b- defN 23-Jul-01 11:32 e2eqavn/datasets/data_collator.py
 -rw-rw-r--  2.0 unx       85 b- defN 23-Jul-01 11:32 e2eqavn/documents/__init__.py
--rw-rw-r--  2.0 unx    12681 b- defN 23-Jul-01 11:32 e2eqavn/documents/corpus.py
+-rw-rw-r--  2.0 unx    12807 b- defN 23-Jul-30 15:42 e2eqavn/documents/corpus.py
 -rw-rw-r--  2.0 unx      251 b- defN 23-Jul-01 11:32 e2eqavn/documents/document_store.py
 -rw-rw-r--  2.0 unx       93 b- defN 23-Jul-01 11:32 e2eqavn/evaluate/__init__.py
 -rw-rw-r--  2.0 unx      283 b- defN 23-Jul-01 11:32 e2eqavn/evaluate/bm25_evaluate_retrieval.py
--rw-rw-r--  2.0 unx     3408 b- defN 23-Jul-01 11:32 e2eqavn/evaluate/information_retrieval_evaluator_custom.py
+-rw-rw-r--  2.0 unx     3417 b- defN 23-Jul-17 15:00 e2eqavn/evaluate/information_retrieval_evaluator_custom.py
 -rw-rw-r--  2.0 unx     1958 b- defN 23-Jul-01 11:32 e2eqavn/evaluate/mrc_evaluator.py
 -rw-rw-r--  2.0 unx      110 b- defN 23-Jul-01 11:32 e2eqavn/mrc/__init__.py
 -rw-rw-r--  2.0 unx      887 b- defN 23-Jul-01 11:32 e2eqavn/mrc/base.py
--rw-rw-r--  2.0 unx    11913 b- defN 23-Jul-01 11:37 e2eqavn/mrc/mrc_model.py
+-rw-rw-r--  2.0 unx    12496 b- defN 23-Jul-23 07:05 e2eqavn/mrc/mrc_model.py
 -rw-rw-r--  2.0 unx       61 b- defN 23-Jul-01 11:32 e2eqavn/pipeline/__init__.py
--rw-rw-r--  2.0 unx     2083 b- defN 23-Jul-01 11:32 e2eqavn/pipeline/e2e_question_answering.py
+-rw-rw-r--  2.0 unx     2083 b- defN 23-Jul-17 15:06 e2eqavn/pipeline/e2e_question_answering.py
 -rw-rw-r--  2.0 unx     2090 b- defN 23-Jul-01 11:32 e2eqavn/pipeline/pipeline.py
 -rw-rw-r--  2.0 unx      145 b- defN 23-Jul-01 11:32 e2eqavn/processor/__init__.py
--rw-rw-r--  2.0 unx     4463 b- defN 23-Jul-01 11:32 e2eqavn/processor/bm25.py
+-rw-rw-r--  2.0 unx     4720 b- defN 23-Jul-30 15:48 e2eqavn/processor/bm25.py
 -rw-rw-r--  2.0 unx     2248 b- defN 23-Jul-01 11:32 e2eqavn/processor/chunk.py
--rw-rw-r--  2.0 unx     7252 b- defN 23-Jul-01 11:37 e2eqavn/processor/qa_ext_processor.py
+-rw-rw-r--  2.0 unx     7481 b- defN 23-Jul-16 15:43 e2eqavn/processor/qa_ext_processor.py
 -rw-rw-r--  2.0 unx     6500 b- defN 23-Jul-03 15:38 e2eqavn/processor/retrieval_sampling.py
 -rw-rw-r--  2.0 unx      138 b- defN 23-Jul-01 11:32 e2eqavn/retrieval/__init__.py
 -rw-rw-r--  2.0 unx      644 b- defN 23-Jul-01 11:32 e2eqavn/retrieval/base.py
 -rw-rw-r--  2.0 unx     3322 b- defN 23-Jul-01 11:32 e2eqavn/retrieval/bm25_retrieval.py
--rw-rw-r--  2.0 unx    10419 b- defN 23-Jul-01 11:32 e2eqavn/retrieval/sbert_retrieval.py
+-rw-rw-r--  2.0 unx    10913 b- defN 23-Jul-05 15:47 e2eqavn/retrieval/sbert_retrieval.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jul-01 11:32 e2eqavn/utils/__init__.py
--rw-rw-r--  2.0 unx    12985 b- defN 23-Jul-01 11:32 e2eqavn/utils/calculate.py
--rw-rw-r--  2.0 unx      603 b- defN 23-Jul-01 11:32 e2eqavn/utils/io.py
+-rw-rw-r--  2.0 unx    13034 b- defN 23-Jul-18 15:02 e2eqavn/utils/calculate.py
+-rw-rw-r--  2.0 unx      906 b- defN 23-Jul-26 14:33 e2eqavn/utils/io.py
 -rw-rw-r--  2.0 unx      998 b- defN 23-Jul-01 11:32 e2eqavn/utils/preprocess.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jul-01 11:32 test/__init__.py
 -rw-rw-r--  2.0 unx      334 b- defN 23-Jul-01 11:32 test/test_chunking.py
--rw-rw-r--  2.0 unx     4990 b- defN 23-Jul-03 15:47 e2eqavn-0.1.7.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Jul-03 15:47 e2eqavn-0.1.7.dist-info/WHEEL
--rw-rw-r--  2.0 unx       52 b- defN 23-Jul-03 15:47 e2eqavn-0.1.7.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       13 b- defN 23-Jul-03 15:47 e2eqavn-0.1.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3379 b- defN 23-Jul-03 15:47 e2eqavn-0.1.7.dist-info/RECORD
-40 files, 117449 bytes uncompressed, 33358 bytes compressed:  71.6%
+-rw-rw-r--  2.0 unx     4981 b- defN 23-Jul-31 08:09 e2eqavn-0.1.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jul-31 08:09 e2eqavn-0.1.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       52 b- defN 23-Jul-31 08:09 e2eqavn-0.1.9.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jul-31 08:09 e2eqavn-0.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3379 b- defN 23-Jul-31 08:09 e2eqavn-0.1.9.dist-info/RECORD
+40 files, 120850 bytes uncompressed, 34098 bytes compressed:  71.8%
```

## zipnote {}

```diff
@@ -99,23 +99,23 @@
 
 Filename: test/__init__.py
 Comment: 
 
 Filename: test/test_chunking.py
 Comment: 
 
-Filename: e2eqavn-0.1.7.dist-info/METADATA
+Filename: e2eqavn-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: e2eqavn-0.1.7.dist-info/WHEEL
+Filename: e2eqavn-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: e2eqavn-0.1.7.dist-info/entry_points.txt
+Filename: e2eqavn-0.1.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: e2eqavn-0.1.7.dist-info/top_level.txt
+Filename: e2eqavn-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: e2eqavn-0.1.7.dist-info/RECORD
+Filename: e2eqavn-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## e2eqavn/__init__.py

```diff
@@ -16,9 +16,9 @@
 
 stream_handler.setFormatter(formatted)
 logger.addHandler(stream_handler)
 logger.setLevel(logging.INFO)
 logger.propagate = False
 
 __author__ = 'khanhdm'
-__version__ = '0.1.7'
+__version__ = '0.1.9'
```

## e2eqavn/cli.py

```diff
@@ -36,18 +36,24 @@
     '--config', '-c',
     required=True,
     default='config/config.yaml',
     help='Path config model'
 )
 def train(config: Union[str, Text]):
     config_pipeline = load_yaml_file(config)
-    train_corpus = Corpus.parser_uit_squad(
-        config_pipeline[DATA][PATH_TRAIN],
-        **config_pipeline.get(CONFIG_DATA, {})
-    )
+    try:
+        train_corpus = Corpus.parser_uit_squad(
+            config_pipeline[DATA][PATH_TRAIN],
+            **config_pipeline.get(CONFIG_DATA, {})
+        )
+    except:
+        train_corpus = Corpus.init_corpus(
+            config_pipeline[DATA][PATH_TRAIN],
+            **config_pipeline.get(CONFIG_DATA, {})
+        )
     retrieval_config = config_pipeline.get(RETRIEVAL, None)
     reader_config = config_pipeline.get(READER, None)
     if retrieval_config.get(IS_TRAIN, False):
         retrieval_sample = RetrievalGeneration.generate_sampling(train_corpus, **retrieval_config[PARAMETERS])
         train_dataset = TripletDataset.load_from_retrieval_sampling(retrieval_sample=retrieval_sample)
         dev_evaluator = make_vnsquad_retrieval_evaluator(
             path_data_json=config_pipeline[DATA][PATH_EVALUATOR]
@@ -59,18 +65,25 @@
         retrieval_learner.train(
             train_dataset=train_dataset,
             dev_evaluator=dev_evaluator,
             **retrieval_config[MODEL]
         )
 
     if reader_config.get(IS_TRAIN, False):
-        eval_corpus = Corpus.parser_uit_squad(
-            config_pipeline[DATA][PATH_EVALUATOR],
-            **config_pipeline.get(CONFIG_DATA, {})
-        )
+        try:
+            eval_corpus = Corpus.parser_uit_squad(
+                config_pipeline[DATA][PATH_EVALUATOR],
+                **config_pipeline.get(CONFIG_DATA, {})
+            )
+        except:
+            eval_corpus = Corpus.init_corpus(
+                config_pipeline[DATA][PATH_EVALUATOR],
+                **config_pipeline.get(CONFIG_DATA, {})
+            )
+
         mrc_dataset = MRCDataset.init_mrc_dataset(
             corpus_train=train_corpus,
             corpus_eval=eval_corpus,
             model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_testing'),
             max_length=reader_config[MODEL].get(MAX_LENGTH, 368),
             **reader_config.get(DATA_ARGUMENT, {})
         )
@@ -100,57 +113,84 @@
     help='Top k retrieval by sentence-bert algorithm'
 )
 @click.option(
     '--logging_result_pipeline',
     default=True,
     help='Logging result predict to file'
 )
+@click.option(
+    '--path_save_log',
+    default='log',
+    help='Path folder for save pipeline result'
+)
 @click.argument('mode', default='retrieval')
 def evaluate(config: Union[str, Text], mode,
              top_k_bm25: int,
              logging_result_pipeline: bool,
+             path_save_log: str,
              top_k_sbert: int
              ):
     config_pipeline = load_yaml_file(config)
     retrieval_config = config_pipeline.get(RETRIEVAL, None)
     reader_config = config_pipeline.get(READER, None)
     pipeline = E2EQuestionAnsweringPipeline()
-    eval_corpus = Corpus.parser_uit_squad(
-        config_pipeline[DATA][PATH_EVALUATOR],
-        **config_pipeline.get(CONFIG_DATA, {})
-    )
-    if mode == 'pipeline':
+
+    try:
+        eval_corpus = Corpus.parser_uit_squad(
+            config_pipeline[DATA][PATH_EVALUATOR],
+            **config_pipeline.get(CONFIG_DATA, {})
+        )
+    except:
+        eval_corpus = Corpus.init_corpus(
+            config_pipeline[DATA][PATH_EVALUATOR],
+            **config_pipeline.get(CONFIG_DATA, {})
+        )
+    if mode in ['retrieval', 'pipeline', 'bm25']:
+        logger.info("Start loading BM25")
         bm25_retrieval = BM25Retrieval(corpus=eval_corpus)
         pipeline.add_component(
             component=bm25_retrieval,
             name_component='bm25_retrieval'
         )
 
+    context_copurs = {doc.document_id: doc.document_context for doc in eval_corpus.list_document}
+    queries = {}
+    relevant_docs = {}
+    for doc in eval_corpus.list_document:
+        if len(doc.list_pair_question_answers) == 0:
+            continue
+        for question_answer in doc.list_pair_question_answers:
+            ques_id = hashlib.sha1(str(question_answer.question).encode('utf-8')).hexdigest()
+            queries[ques_id] = question_answer.question
+            if ques_id not in relevant_docs:
+                relevant_docs[ques_id] = set()
+            relevant_docs[ques_id].add(doc.document_id)
+
     if mode in ['retrieval', 'pipeline'] and retrieval_config:
-        corpus, queries, relevant_docs = make_input_for_retrieval_evaluator(
-            path_data_json=config_pipeline[DATA][PATH_EVALUATOR]
-        )
+        logger.info("Start loading Sbert")
         retrieval_model = SBertRetrieval.from_pretrained(retrieval_config[MODEL][MODEL_NAME_OR_PATH])
         retrieval_model.update_embedding(eval_corpus)
         pipeline.add_component(
             component=retrieval_model,
             name_component='sbert_retrieval'
         )
-        if mode == 'retrieval':
-            logger.info("Start evaluate retrieval")
-            information_evaluator = InformationRetrievalEvaluatorCustom(
-                queries=queries,
-                corpus=corpus,
-                relevant_docs=relevant_docs
-            )
-            information_evaluator.compute_metrices_retrieval(
-                pipeline=pipeline
-            )
+
+    if mode in ['retrieval', 'bm25']:
+        logger.info("Start evaluate retrieval")
+        information_evaluator = InformationRetrievalEvaluatorCustom(
+            queries=queries,
+            corpus=context_copurs,
+            relevant_docs=relevant_docs
+        )
+        information_evaluator.compute_metrices_retrieval(
+            pipeline=pipeline
+        )
 
     if mode in ['reader', 'pipeline'] and reader_config:
+        logger.info("Start loading Reader")
         mrc_dataset = MRCDataset.init_mrc_dataset(
             corpus_eval=eval_corpus,
             model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_dev'),
             max_length=reader_config[MODEL].get(MAX_LENGTH, 368)
         )
         reader_model = MRCReader.from_pretrained(
             model_name_or_path=reader_config[MODEL].get(MODEL_NAME_OR_PATH, 'khanhbk20/mrc_dev')
@@ -213,15 +253,15 @@
                             'answer_end_idx': doc_reader.get('answer_end_idx', 0)
                         } for doc_retrieval, doc_reader in
                         zip(pred_answers['documents'][idx], pred_answers['reader_logging'][idx])
                     ]
                 }
                 )
         if logging_result_pipeline:
-            write_json_file(results_logging, 'logging.json')
+            write_json_file(results_logging, os.path.join(path_save_log, 'logging.json'))
         logger.info(f"Evaluate E2E pipeline: {metric_fn.compute(predictions=predictions, references=ground_truth)}")
 
 
 @click.command()
 @click.option(
     '--config', '-c',
     required=True,
```

## e2eqavn/documents/corpus.py

```diff
@@ -137,18 +137,20 @@
         if not kwargs.get(MODE_CHUNKING, False):
             document_id = hashlib.sha1(str(document_context).encode('utf-8')).hexdigest()
             dict_question_answers = defaultdict(list)
             if len(context[qas_key]) > 0:
                 for question in context[qas_key]:
                     if not is_vnsquad_eval:
                         for answer in question[answers_key]:
+                            if answer_key not in answer:
+                                continue
                             dict_question_answers[question[question_key]].append(
                                 {
-                                    answer_key: answer[answer_key],
-                                    answer_start: answer[answer_start]
+                                    answer_key: normalize('NFC', answer[answer_key]),
+                                    answer_start: answer.get(answer_start, None)
                                 }
                             )
                     else:
                         dict_question_answers[question[question_key]] = []
             list_document.append(
                 Document.init_document(
                     document_id=document_id,
```

## e2eqavn/evaluate/information_retrieval_evaluator_custom.py

```diff
@@ -26,15 +26,15 @@
                          precision_recall_at_k, map_at_k, show_progress_bar, batch_size, name, write_csv,
                          score_functions, main_score_function)
         self.queries = list(queries.values())
 
     def compute_metrices_retrieval(self, pipeline,
                                    **kwargs) -> Dict[str, float]:
         top_k_bm25 = kwargs.get(TOP_K_BM25, 30)
-        get_scoring_method = kwargs.get(GET_SCORE, EMBEDDING_SCORE)
+        get_scoring_method = kwargs.get(GET_SCORE, COMBINE_BM25_EMBED_SCORE)
         if kwargs.get(TOP_K_SBERT, None):
             top_k_sbert = kwargs.get(TOP_K_SBERT)
         else:
             top_k_sbert = max(max(self.mrr_at_k), max(self.ndcg_at_k), max(self.accuracy_at_k),
                               max(self.precision_recall_at_k),
                               max(self.map_at_k))
         list_question = self.queries
```

## e2eqavn/mrc/mrc_model.py

```diff
@@ -89,36 +89,40 @@
                 start_positions = start_positions.squeeze(-1)
             if len(end_positions.size()) > 1:
                 end_positions = end_positions.squeeze(-1)
 
             ignored_index = start_logits.size(1)
             start_positions = start_positions.clamp(0, ignored_index)
             end_positions = end_positions.clamp(0, ignored_index)
-            list_negative = torch.where(is_negative_sample == 0)[0]
-            list_positive = torch.where(is_negative_sample == 1)[0]
             loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)
-            if list_positive.size(0) > 0:
-                start_loss = loss_fct(start_logits[list_positive, :], start_positions[list_positive])
-                end_loss = loss_fct(end_logits[list_positive, :], end_positions[list_positive])
-                total_loss += (start_loss + end_loss) / 2
-
-            if list_negative.size(0) > 0:
-                total_loss += 1 / 2 * self.lambda_weight * (
-                        loss_fct(start_logits[list_negative, :], start_positions[list_negative]) +
-                        loss_fct(end_logits[list_negative, :], end_positions[list_negative])
-                )
-                total_loss += self.lambda_weight * (
-                    torch.sum(torch.clamp(
-                        torch.max(start_logits, dim=-1)[0] - 0.9, min=0
-                    ))
-                    +
-                    torch.sum(torch.clamp(
-                        torch.max(end_logits, dim=-1)[0] - 0.9, min=0
-                    ))
-                )
+            start_loss = loss_fct(start_logits, start_positions)
+            end_loss = loss_fct(end_logits, end_positions)
+            total_loss = (start_loss + end_loss) / 2
+            # list_negative = torch.where(is_negative_sample == 0)[0]
+            # list_positive = torch.where(is_negative_sample == 1)[0]
+            # loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)
+            # if list_positive.size(0) > 0:
+            #     start_loss = loss_fct(start_logits[list_positive, :], start_positions[list_positive])
+            #     end_loss = loss_fct(end_logits[list_positive, :], end_positions[list_positive])
+            #     total_loss += (start_loss + end_loss) / 2
+            #
+            # if list_negative.size(0) > 0:
+            #     total_loss += 1 / 2 * self.lambda_weight * (
+            #             loss_fct(start_logits[list_negative, :], start_positions[list_negative]) +
+            #             loss_fct(end_logits[list_negative, :], end_positions[list_negative])
+            #     )
+            #     total_loss += self.lambda_weight * (
+            #         torch.sum(torch.clamp(
+            #             torch.max(start_logits, dim=-1)[0] - 0.9, min=0
+            #         ))
+            #         +
+            #         torch.sum(torch.clamp(
+            #             torch.max(end_logits, dim=-1)[0] - 0.9, min=0
+            #         ))
+            #     )
 
         if not return_dict:
             output = (start_logits, end_logits) + outputs[2:]
             return ((total_loss,) + output) if total_loss is not None else output
 
         return QuestionAnsweringModelOutput(
             loss=total_loss,
@@ -192,70 +196,73 @@
             args=training_args,
             train_dataset=mrc_dataset.train_dataset,
             eval_dataset=mrc_dataset.evaluator_dataset,
             data_collator=self.data_collator,
             compute_metrics=self.compute_metrics
         )
 
-    def extract_answer(self, input_features, outputs):
+    def extract_answer(self, input_features, outputs, retrieval_score: List):
         results = []
-        for input_feature, start_logit, end_logit in zip(input_features, outputs.start_logits, outputs.end_logits):
+        flag = all(value == 0 for value in retrieval_score)
+        for idx, (input_feature, start_logit, end_logit) in enumerate(zip(input_features, outputs.start_logits, outputs.end_logits)):
             input_ids = input_feature[INPUT_IDS]
             words_length = input_feature[WORDS_LENGTH]
             answer_start_idx = sum(words_length[: torch.argmax(start_logit)])
             answer_end_idx = sum(words_length[: torch.argmax(end_logit) + 1])
             if answer_start_idx <= answer_end_idx:
                 answer = self.tokenizer.convert_tokens_to_string(
                     self.tokenizer.convert_ids_to_tokens(input_ids[answer_start_idx:answer_end_idx])
                 )
             else:
                 answer = " "
             score_start = torch.max(torch.softmax(start_logit, dim=-1)).cpu().detach().numpy().tolist()
             score_end = torch.max(torch.softmax(end_logit, dim=-1)).cpu().detach().numpy().tolist()
+            if flag:
+                score_reader = score_start * score_end
+            else:
+                score_reader = score_start * score_end * retrieval_score[idx]
             results.append({
                 "answer": answer,
                 "score_start": score_start,
                 "score_end": score_end,
-                "score": score_end * score_start,
+                "score":  score_reader,
                 'answer_start_idx': answer_start_idx,
                 'answer_end_idx': answer_end_idx
             })
         return sorted(results, key=lambda x: x['score'], reverse=True)
 
     def predict(self, queries: List[str], documents: List[List[Document]], **kwargs):
         logger.info(f'Number documents: {len(documents)}')
         assert len(queries) == len(documents), "Number question must equal number document"
         results, results_raw = [], []
         for question, list_document in tqdm(zip(queries, documents), total=len(documents)):
             tmp_pred, tmp_pred_raw = self.qa_inference(
                 question=question,
-                documents=[
-                    doc.document_context for doc in list_document
-                ],
+                documents=list_document,
                 **kwargs
             )
             results.append(tmp_pred)
             results_raw.append(tmp_pred_raw)
         return results, results_raw
 
-    def qa_inference(self, question: str, documents: List[str], **kwargs):
+    def qa_inference(self, question: str, documents: List[Document], **kwargs):
         questions = [question] * len(documents)
         top_k_qa = kwargs.get(TOP_K_QA, 1)
         input_features_raw = make_input_feature_qa(
             questions=questions,
-            documents=documents,
+            documents=[doc.document_context for doc in documents],
             tokenizer=self.tokenizer,
             max_length=368
         )
         input_features = self.data_collator(input_features_raw)
         for key, value in input_features.items():
             if isinstance(value, Tensor):
                 input_features[key] = value.to(self.device)
         outs = self.model(**input_features)
-        results = self.extract_answer(input_features_raw, outs)
+        results = self.extract_answer(input_features_raw, outs, retrieval_score=[doc.score for doc in documents])
         return results[:top_k_qa], results
 
     def train(self):
         wandb_api_key = os.getenv("WANDB_API")
         wandb.login(key=wandb_api_key)
         self.trainer.train()
         self.compute_metrics.log_predict = []  # refresh log
```

## e2eqavn/pipeline/e2e_question_answering.py

```diff
@@ -39,15 +39,15 @@
             component=component,
             name_component=name_component,
             input_component=self.input_root
         )
         self.input_root = name_component
 
     def run(self, queries: Union[str, List[str]],
-            top_k_bm25: int = 50,
+            top_k_bm25: int = 30,
             top_k_sbert: int = 10,
             top_k_qa: int = 1,
             **kwargs):
         if isinstance(queries, str):
             queries = [queries]
         queries = [process_text(query) for query in queries]
```

## e2eqavn/processor/bm25.py

```diff
@@ -10,15 +10,15 @@
         self.corpus = corpus
         self.n_docs = len(self.corpus)
         self.avg_document_length = 0
         self.doc_freqs = []
         self.idf = {}
         self.doc_len = []
         if isinstance(corpus[0], str):
-            corpus = [doc.lower().split(" ") for doc in corpus]
+            corpus = [doc.lower().split() for doc in corpus]
 
         nd = self._initialize(corpus)
         self._calc_idf(nd)
 
     def _initialize(self, corpus: List[str]):
         nd = {}
         num_doc = 0
@@ -89,15 +89,15 @@
 
         eps = self.epsilon * self.average_idf
         for word in negative_idfs:
             self.idf[word] = eps
 
     def get_scores(self, query: Union[List, str]):
         if isinstance(query, str):
-            query = query.lower().split(" ")
+            query = query.lower().split()
         score = np.zeros(self.n_docs)
         doc_len = np.array(self.doc_len)
         for q in query:
             q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])
             if self.penalty_oov:
                 if self.idf.get(q) is not None:
                     score += self.idf.get(q) * (
@@ -109,14 +109,20 @@
                     score -= temp
             else:
                 score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /
                                                    (q_freq + self.k1 * (
                                                                1 - self.b + self.b * doc_len / self.avg_document_length)))
         return score
 
-    def get_top_k(self, query: Union[List[str], str], top_k: int):
+    def get_top_k(self, query: Union[List[str], str], top_k: int, normalize: bool = False):
         if isinstance(query, str):
-            query = query.lower().split(" ")
+            query = query.lower().split()
         scores = self.get_scores(query)
+        print(max(scores), min(scores))
+        print(scores)
         top_k_idxs = np.argsort(scores)[-top_k:]
-        return {idx: scores[idx] for idx in top_k_idxs}
+        if not normalize:
+            return {idx: scores[idx] for idx in top_k_idxs}
+        else:
+            sum_score = sum([scores[idx] for idx in top_k_idxs])
+            return {idx: scores[idx]/sum_score for idx in top_k_idxs}
```

## e2eqavn/processor/qa_ext_processor.py

```diff
@@ -56,19 +56,24 @@
 
     def strip_context(self, text):
         text = text.replace('\n', ' ')
         text = re.sub(r'\s+', ' ', text)
         text = text.strip()
         return text
 
+    def find_answer_start_raw(self, context: str, answer: str):
+        return context.find(answer)
+
     def process_example(self, example: dict):
         question = example[self.question_key]
         context = example[self.context_key]
         answer = example[self.answer_key]
         answer_start_raw = example[self.answer_start_key]
+        if answer_start_raw is None:
+            answer_start_raw = self.find_answer_start_raw(context=context, answer=answer)
         flag = False
         for step in [-1, 0, 1]:
             if context[answer_start_raw + step: answer_start_raw + step + len(answer)] == answer:
                 answer_start_raw += step
                 flag = True
                 break
         if not flag and context.count(answer) == 1:
@@ -119,15 +124,15 @@
             }
 
         return example
 
     def make_example(self, corpus: Corpus, **kwargs):
         if kwargs.get(MAKE_NEGATIVE_MRC, False):
             logger.info("Turn on mode make negative sample for mrc")
-            logger.info(f"Start sampling negative by BM25 with {kwargs.get(THRESHOLD_SAMPLING, 0.2) *100} % corpus")
+            logger.info(f"Start sampling negative by BM25 with {kwargs.get(THRESHOLD_SAMPLING, 0.2) * 100} % corpus")
             bm25_scoring = BM25Scoring(corpus=[doc.document_context for doc in corpus.list_document])
         list_documents = corpus.list_document
         examples = []
         for index, document in tqdm(enumerate(corpus.list_document), total=len(corpus.list_document)):
             if len(document.list_pair_question_answers) == 0:
                 continue
             document_context = document.document_context
```

## e2eqavn/retrieval/sbert_retrieval.py

```diff
@@ -116,19 +116,21 @@
 
 
 class SBertRetrieval(BaseRetrieval, ABC):
     def __init__(self, model: SentenceBertLearner, device,
                  corpus: Corpus = None,
                  corpus_embedding: Union[np.array, torch.Tensor] = None,
                  convert_to_numpy: bool = False,
+                 pretrained_model_name: str = None,
                  convert_to_tensor: bool = False):
         self.list_documents: List[Document] = None
         self.model = model
         self.device = device
         self.corpus = corpus
+        self.pretrained_model_name = pretrained_model_name
         self.corpus_embedding = corpus_embedding
         self.convert_to_tensor = convert_to_tensor
         self.convert_to_numpy = convert_to_numpy
         if not convert_to_numpy and not convert_to_tensor:
             self.convert_to_numpy = True
         elif next(self.model.get_device()) == torch.device('cuda'):
             self.convert_to_tensor = True
@@ -167,40 +169,45 @@
         """
         Update embedding for corpus
         :param corpus: Corpus document context
         :param batch_size: number document in 1 batch
         :return:
         """
         path_corpus_embedding = kwargs.get('path_corpus_embedding', 'model/retrieval/corpus_embedding.pth')
+        folder = path_corpus_embedding.rsplit('/', 1)[0]
         self.list_documents = deepcopy(corpus.list_document)
         flag = True
         if os.path.isfile(path_corpus_embedding):
             logger.info(f"Loading corpus embedding at {path_corpus_embedding}")
             tmp_corpus_embedding = torch.load(path_corpus_embedding, map_location=self.device)
-            if len(tmp_corpus_embedding) != len(self.list_documents):
+            config = load_json_data(os.path.join(folder, 'config.json'))
+            if len(tmp_corpus_embedding) != len(self.list_documents) or \
+                    self.pretrained_model_name != config['pretrained_model_name']:
                 flag = False
-            self.corpus_embedding = tmp_corpus_embedding
+            else:
+                self.corpus_embedding = tmp_corpus_embedding
 
         if not os.path.isfile(path_corpus_embedding) or not flag:
             logger.info(f"Start encoding corpus with {len(corpus.list_document)} document")
             document_context = corpus.list_document_context
             self.corpus_embedding = self.model.encode_context(
                 sentences=document_context,
                 convert_to_numpy=False,
                 convert_to_tensor=True,
                 batch_size=batch_size,
                 show_progress_bar=True,
                 device=self.device,
                 **kwargs
             )
-            folder = path_corpus_embedding.rsplit('/', 1)[0]
             if not os.path.exists(folder):
                 os.makedirs(folder, exist_ok=True)
             torch.save(self.corpus_embedding, path_corpus_embedding)
             logger.info(f"Save corpus embedding at {path_corpus_embedding}")
+            config = {'pretrained_model_name': self.pretrained_model_name}
+            write_json_file(data=config, path_file=os.path.join(folder, 'config.json'))
 
     def query_by_embedding(self, query: List[str], top_k: int, **kwargs):
         """
         :param top_k: k index document will return
         :param query: question
         :return: List document id
         """
@@ -226,8 +233,8 @@
             scores, sub_index_select = torch.topk(similarity_scores, top_k, dim=1, largest=True, sorted=False, )
         return scores, sub_index_select
 
     @classmethod
     def from_pretrained(cls, model_name_or_path: str, **kwargs):
         device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
         model = SentenceBertLearner.from_pretrained(model_name_or_path)
-        return cls(model=model, device=device)
+        return cls(model=model, device=device, pretrained_model_name=model_name_or_path)
```

## e2eqavn/utils/calculate.py

```diff
@@ -287,16 +287,19 @@
             relevant_docs[question_id].add(context_id)
     return corpus, queries, relevant_docs
 
 
 def make_input_for_retrieval_evaluator(path_data_json, **kwargs):
     data = load_json_data(path_data_json)
     temp = []
-    for context in data['data']:
-        temp.extend(context['paragraphs'])
+    try:
+        for context in data['data']:
+            temp.extend(context['paragraphs'])
+    except:
+        temp = data
     return prepare_input_for_retrieval_evaluator(temp, **kwargs)
 
 
 def make_vnsquad_retrieval_evaluator(path_data_json: str, **kwargs):
     corpus, queries, relevant_docs = make_input_for_retrieval_evaluator(path_data_json)
     return InformationRetrievalEvaluator(
         queries=queries,
```

## e2eqavn/utils/io.py

```diff
@@ -1,22 +1,32 @@
 import json
 import logging
+import os.path
+
 import yaml
 from typing import *
 
 logger = logging.getLogger(__name__)
 
 
 def load_json_data(path_file: str):
     with open(path_file, 'r', encoding='utf-8') as file:
         data = json.load(file)
     return data
 
 
 def write_json_file(data: Union[Dict, List[Dict]], path_file: str):
+    paths = path_file.rsplit('/', 1)
+    if len(paths) == 1:
+        path_file = paths[0]
+    else:
+        path_folder, file = paths
+        if os.path.exists(path_folder) is False:
+            os.makedirs(path_folder, exist_ok=True)
+        path_file = os.path.join(path_folder, file)
     with open(path_file, 'w') as file:
         json.dump(data, file, ensure_ascii=False, indent=4)
     logger.info(f"Save data in {path_file}")
 
 
 def load_yaml_file(path_file: str):
     with open(path_file) as file:
```

## Comparing `e2eqavn-0.1.7.dist-info/METADATA` & `e2eqavn-0.1.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: e2eqavn
-Version: 0.1.7
+Version: 0.1.9
 Summary: e2eqavn is end to end pipeline for question answering
 Author: khanhdm
 Author-email: khanhc1k36@gmail.com
 Requires-Python: >3.6.0
 Description-Content-Type: text/markdown
 Requires-Dist: numpy
 Requires-Dist: PyYAML
@@ -17,15 +17,15 @@
 Requires-Dist: nltk
 Requires-Dist: python-dotenv
 Requires-Dist: wandb
 Requires-Dist: kaggle
 Requires-Dist: matplotlib
 Requires-Dist: seaborn
 Requires-Dist: accelerate
-Requires-Dist: protobuf (==3.20)
+Requires-Dist: protobuf
 Requires-Dist: colorlog
 Requires-Dist: fastapi
 Requires-Dist: uvicorn
 Requires-Dist: pymongo[srv]
 
 # 1. Introduction
```

## Comparing `e2eqavn-0.1.7.dist-info/RECORD` & `e2eqavn-0.1.9.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-e2eqavn/__init__.py,sha256=54lwLY5cq5gKPvQLikORk_bjRCGEVJrPOYuE3lxjieI,498
-e2eqavn/cli.py,sha256=YV4JVKLr_kQLre7rbWUh9E76e9KWYif8m9PEu6Rgmus,10774
+e2eqavn/__init__.py,sha256=ccXAAcc5TDEy5-9rl3J9NpMawU9UUXKybYBcOuf8GfQ,498
+e2eqavn/cli.py,sha256=S3oVqrp6uPf_71udtKFXtfzhUdnMAUEOk2HpX-nQ9iI,12134
 e2eqavn/keywords.py,sha256=RUpkhbJ14i4XbIojmiSaXpjN9lI4bgPnpIY_ZnivTrc,2716
 e2eqavn/datasets/MRCDataset.py,sha256=EDQ656zUVz-PFZoBbA8D1WpyrMSdyTUzDHWbfheHdsk,3639
 e2eqavn/datasets/TripletDataset.py,sha256=UalsiPb-sVq79gE923C_Gu8mqih_KJ-NCAI2MEV-VCY,1386
 e2eqavn/datasets/__init__.py,sha256=PVd6KkEbmwFhGlSdv8lCnLcyIT6MrIOU2zeLEDnNBQM,124
 e2eqavn/datasets/data_collator.py,sha256=9anyZh7fMzgr0R3tmqO63d-kBUE2UgnrBbU0jPuTEJc,3814
 e2eqavn/documents/__init__.py,sha256=KhJ-7DiMYhKRaNBXpMFooJ4EioOOXKxFqjTVB__1JG8,85
-e2eqavn/documents/corpus.py,sha256=mKMcqgpmH8kYylhWI-fXVQ8Zp9hl-_o3V7-BvIkGfN8,12681
+e2eqavn/documents/corpus.py,sha256=HL23tmDOu2_djOj3wB7lg3ZpXhFzH7aWAgv8LUSB7OY,12807
 e2eqavn/documents/document_store.py,sha256=SI0eQivEdTGucoRkEzOiHVXcL4IeP-3R9I8JC7iUvk0,251
 e2eqavn/evaluate/__init__.py,sha256=FoaGcO_g8rveNGpk_6_n4RFw4P_dpRxmMGk0B3T1DMI,93
 e2eqavn/evaluate/bm25_evaluate_retrieval.py,sha256=eyvV7MFGQcd2f5BXph8kn5B3wqdL2kzXOyXWzNJOrYc,283
-e2eqavn/evaluate/information_retrieval_evaluator_custom.py,sha256=qStzIgsOGq8Tu3nt6W2lu6ajZiXxNWaq2EQOv_pnSoo,3408
+e2eqavn/evaluate/information_retrieval_evaluator_custom.py,sha256=jHN_fmxIGLg8WMO5HuMybod7Z7RA9EujwvSq8JJw8w8,3417
 e2eqavn/evaluate/mrc_evaluator.py,sha256=dTn_8L2nDLR8frBxZWBuY_bdcDZ7aXHSy7nRvrnf1V0,1958
 e2eqavn/mrc/__init__.py,sha256=7wccJ-QQrMqjYIPIPmOMcQzWhjFgne7QbPZlxp5njPU,110
 e2eqavn/mrc/base.py,sha256=MOQRuPmEGCK_nnZYgFXEirPa5UHApspBV0e66Q8gWes,887
-e2eqavn/mrc/mrc_model.py,sha256=652sb2UDaItiGRP1uw-0ABY1EjiiViTMaR7bu6q07Gk,11913
+e2eqavn/mrc/mrc_model.py,sha256=hBoSVwKabUR1fhnjaO3xkMQC-EmX_M8sFQAu0YLu4UI,12496
 e2eqavn/pipeline/__init__.py,sha256=nHdcilMUCQJATXwseGf_ejz-wFKlGSMs0sM7HneaZjQ,61
-e2eqavn/pipeline/e2e_question_answering.py,sha256=vre0tEAI9klK9Musj8VRWf2DTrnQkpIa43ku-lx7GQo,2083
+e2eqavn/pipeline/e2e_question_answering.py,sha256=pdcS-FF4C6stzyI0HxQwrTwkARRV5O99OjsJQJutdJY,2083
 e2eqavn/pipeline/pipeline.py,sha256=kRQmaA9GDAsU4zfB3TOY92jxVF5r3N-JWg2xqvCu1o4,2090
 e2eqavn/processor/__init__.py,sha256=7Y91pODl0Ba5-z0UHyw4uGkWO8M4r5S4vtzDx30Qlhs,145
-e2eqavn/processor/bm25.py,sha256=Lp2s4FTPHFHRMbVVY6B78ulip9yiORec4OFY32_DpJ8,4463
+e2eqavn/processor/bm25.py,sha256=W5J5H3BBxm_WuKCbvHFg6nO8njVBtQMBNO76hvvKCSQ,4720
 e2eqavn/processor/chunk.py,sha256=JSpowb5eqngBsFPmJEr_X___cGMSVwbzkiznJYAjTUo,2248
-e2eqavn/processor/qa_ext_processor.py,sha256=yN1OoCnFIt8EisRupSL9Z-w0T0-4VZREBDz9u_v4YkM,7252
+e2eqavn/processor/qa_ext_processor.py,sha256=T-gzrw3NAltCUO8qYS8sLbcy6DodKU8RVeCfvEAiLsQ,7481
 e2eqavn/processor/retrieval_sampling.py,sha256=gpOmtOc96VPxh83UUN2T7mYY8wl2ko7C3FO7_M9ZLBY,6500
 e2eqavn/retrieval/__init__.py,sha256=SiDAppt0_X9DRa9dhiCxmle7RpKQOW0dtYpA84y0W18,138
 e2eqavn/retrieval/base.py,sha256=XGkgyS5es4DFJyS5HwZKzfWBWaKekJowk6UvLuqMVhs,644
 e2eqavn/retrieval/bm25_retrieval.py,sha256=EyxojDjg8saKj-xEozm9PhrGyu4iV63P98a-UrDjS48,3322
-e2eqavn/retrieval/sbert_retrieval.py,sha256=KhdDq2CBF_E9nhWpQExyNtcjQOtxvOaZybj1ZAV7idQ,10419
+e2eqavn/retrieval/sbert_retrieval.py,sha256=Oi3r5GFoYVglmCcUHu16H65vu8Y9CJQxbd6Wx5bOYuo,10913
 e2eqavn/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-e2eqavn/utils/calculate.py,sha256=LOaXVbVNIkhbH9UyKD004Evb_lbDY0SMnVSC7CnMgcI,12985
-e2eqavn/utils/io.py,sha256=StKqF8nRhuApqlI0vPlme8Y50Do4epId8KeW4oJ5uTI,603
+e2eqavn/utils/calculate.py,sha256=jBpSIClikhRibnEtg2RIc88r_qjGtbSxZIj5Ut9WtbE,13034
+e2eqavn/utils/io.py,sha256=fO0vuSN7Kb-N0sZP2H2UUPchUVtXQDyYODWJ-ubEycw,906
 e2eqavn/utils/preprocess.py,sha256=1yhNSGZ5FRzIglW0IRiZyxS1b3Rr5CNzLN78lQsRQjQ,998
 test/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/test_chunking.py,sha256=tW3Cg_Ll8mtfYjmPmNy5MgElUSe8eIKhcWAND20SRDA,334
-e2eqavn-0.1.7.dist-info/METADATA,sha256=qOt6UWzeH6JF0pnBmOwTKTG4lTDFzYBVRixLpdYgPEE,4990
-e2eqavn-0.1.7.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
-e2eqavn-0.1.7.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
-e2eqavn-0.1.7.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
-e2eqavn-0.1.7.dist-info/RECORD,,
+e2eqavn-0.1.9.dist-info/METADATA,sha256=sLlwYcQLwqz4WuQnd9VnQRgaZInh6vGEkObxwwZT7pg,4981
+e2eqavn-0.1.9.dist-info/WHEEL,sha256=bb2Ot9scclHKMOLDEHY6B2sicWOgugjFKaJsT7vwMQo,110
+e2eqavn-0.1.9.dist-info/entry_points.txt,sha256=pPUmMeBtyeitA4rbQwBo_glm5HRGPLREjN9eeAGu_Bk,52
+e2eqavn-0.1.9.dist-info/top_level.txt,sha256=Qu5Dlk8CtzRo5i3_O722D1zBatiVaNkiPAbSDCOA2pA,13
+e2eqavn-0.1.9.dist-info/RECORD,,
```

